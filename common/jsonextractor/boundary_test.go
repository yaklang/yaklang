package jsonextractor

import (
	"context"
	"fmt"
	"io"
	"runtime"
	"strings"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

// TestEmptyAndNilInputs æµ‹è¯•ç©ºè¾“å…¥å’Œnilè¾“å…¥çš„è¾¹ç•Œæƒ…å†µ
func TestEmptyAndNilInputs(t *testing.T) {
	t.Parallel() // å¹¶è¡Œæ‰§è¡Œä»¥æé«˜æ•ˆç‡
	tests := []struct {
		name     string
		input    string
		expected error
	}{
		{
			name:     "empty string",
			input:    "",
			expected: io.EOF,
		},
		{
			name:     "whitespace only",
			input:    "   \n\t\r   ",
			expected: io.EOF,
		},
		{
			name:     "only braces",
			input:    "{}",
			expected: nil,
		},
		{
			name:     "only brackets",
			input:    "[]",
			expected: nil,
		},
		{
			name:     "incomplete object",
			input:    "{",
			expected: nil, // å¯èƒ½è¿”å›EOFæˆ–å…¶ä»–é”™è¯¯
		},
		{
			name:     "incomplete array",
			input:    "[",
			expected: nil, // å¯èƒ½è¿”å›EOFæˆ–å…¶ä»–é”™è¯¯
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			start := time.Now()

			// æµ‹è¯•ExtractStructuredJSON
			err := ExtractStructuredJSON(tt.input, WithObjectCallback(func(data map[string]any) {
				// å¤„ç†å¯¹è±¡å›è°ƒ
			}))

			// å¯¹äºç©ºè¾“å…¥ï¼Œé¢„æœŸæ˜¯EOFé”™è¯¯
			if tt.input == "" || tt.input == "   \n\t\r   " {
				if err != io.EOF && err != nil {
					t.Logf("Expected EOF or nil for empty input, got: %v", err)
				}
			}

			// æµ‹è¯•ExtractStructuredJSONFromStream
			reader := strings.NewReader(tt.input)
			err = ExtractStructuredJSONFromStream(reader, WithObjectCallback(func(data map[string]any) {
				// å¤„ç†å¯¹è±¡å›è°ƒ
			}))

			elapsed := time.Since(start)
			assert.Less(t, elapsed, 100*time.Millisecond, "Test should complete within 100ms")
		})
	}
}

// TestLargeDataBoundary æµ‹è¯•å¤§æ•°æ®é‡çš„è¾¹ç•Œæƒ…å†µ
func TestLargeDataBoundary(t *testing.T) {
	t.Parallel() // å¹¶è¡Œæ‰§è¡Œä»¥æé«˜æ•ˆç‡
	// åˆ›å»ºä¸­ç­‰å¤§å°çš„æ•°æ®ï¼ˆçº¦1MBï¼‰ï¼Œç¡®ä¿åœ¨3ç§’å†…å®Œæˆ
	dataSize := 1024 * 1024 // 1MB
	largeData := strings.Repeat("x", dataSize)
	jsonData := fmt.Sprintf(`{"largeField": "%s", "smallField": "test"}`, largeData)

	t.Run("large string field", func(t *testing.T) {
		start := time.Now()

		var fieldReceived bool
		var dataSizeReceived int
		var wg sync.WaitGroup
		wg.Add(1)
		err := ExtractStructuredJSON(jsonData,
			WithRegisterFieldStreamHandler("largeField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, readErr := io.ReadAll(reader)
				require.NoError(t, readErr)
				dataSizeReceived = len(data)
				fieldReceived = true
			}))

		require.NoError(t, err)
		wg.Wait()
		assert.True(t, fieldReceived)
		assert.Greater(t, dataSizeReceived, dataSize) // åŒ…å«å¼•å·

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Large data test should complete within 3 seconds")
		t.Logf("Processed %d bytes in %v", dataSize, elapsed)
	})

	t.Run("large nested structure", func(t *testing.T) {
		start := time.Now()

		// åˆ›å»ºåŒ…å«1000ä¸ªå¯¹è±¡çš„æ•°ç»„
		var objects []string
		for i := 0; i < 1000; i++ {
			objects = append(objects, fmt.Sprintf(`{"id": %d, "data": "item%d"}`, i, i))
		}
		jsonData := "[" + strings.Join(objects, ",") + "]"

		var objectCount int32
		err := ExtractStructuredJSON(jsonData,
			WithObjectCallback(func(data map[string]any) {
				atomic.AddInt32(&objectCount, 1)
			}))

		require.NoError(t, err)
		assert.Equal(t, int32(1000), objectCount)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Nested structure test should complete within 3 seconds")
		t.Logf("Processed %d objects in %v", objectCount, elapsed)
	})
}

// TestExtremeNesting æµ‹è¯•æç«¯åµŒå¥—ç»“æ„çš„è¾¹ç•Œæƒ…å†µ
func TestExtremeNesting(t *testing.T) {
	t.Run("deep nesting object", func(t *testing.T) {
		start := time.Now()

		// ä½¿ç”¨ä¸€ä¸ªæ›´ç®€å•çš„åµŒå¥—ç»“æ„æ¥æµ‹è¯•
		jsonData := `{"level1": {"level2": {"level3": {"deepest": "value"}}}}`

		var deepestReached bool
		var callbackCount int
		err := ExtractStructuredJSON(jsonData,
			WithRawKeyValueCallback(func(key, value any) {
				callbackCount++
				t.Logf("Callback %d: key=%v, value=%v", callbackCount, key, value)
				if key == `"deepest"` && fmt.Sprintf("%v", value) == ` "value"` {
					deepestReached = true
				}
			}))

		require.NoError(t, err)
		assert.True(t, deepestReached, "Should find the deepest value")
		assert.Greater(t, callbackCount, 0, "Should have callbacks")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Deep nesting test should complete within 3 seconds")
		t.Logf("Processed deep nesting in %v", elapsed)
	})

	t.Run("deep nesting array", func(t *testing.T) {
		start := time.Now()

		// åˆ›å»ºæ·±åº¦ä¸º30çš„åµŒå¥—æ•°ç»„
		jsonData := strings.Repeat(`[`, 30) + `"deepest"` + strings.Repeat(`]`, 30)

		var arrayCount int32
		err := ExtractStructuredJSON(jsonData,
			WithArrayCallback(func(data []any) {
				atomic.AddInt32(&arrayCount, 1)
			}))

		require.NoError(t, err)
		assert.Greater(t, arrayCount, int32(0))

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Deep array nesting test should complete within 3 seconds")
		t.Logf("Processed %d array levels in %v", arrayCount, elapsed)
	})
}

// TestSpecialCharactersAndUnicode æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å’ŒUnicodeè¾¹ç•Œæƒ…å†µ
func TestSpecialCharactersAndUnicode(t *testing.T) {
	tests := []struct {
		name  string
		json  string
		valid bool
	}{
		{
			name:  "unicode characters",
			json:  `{"unicode": "ä½ å¥½ä¸–ç•ŒğŸŒğŸš€â¤ï¸"}`,
			valid: true,
		},
		{
			name:  "escape sequences",
			json:  `{"escapes": "\"\\\/\b\f\n\r\t"}`,
			valid: true,
		},
		{
			name:  "control characters",
			json:  `{"control": "` + string([]byte{0x01, 0x02, 0x03}) + `"}`,
			valid: false, // æ§åˆ¶å­—ç¬¦é€šå¸¸æ— æ•ˆ
		},
		{
			name:  "null bytes",
			json:  `{"nullbyte": "` + string([]byte{0x00}) + `"}`,
			valid: false,
		},
		{
			name:  "mixed encodings",
			json:  `{"mixed": "ASCIIä¸­æ–‡Ğ ÑƒÑÑĞºĞ¸Ğ¹"}`,
			valid: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			start := time.Now()

			var processed bool
			err := ExtractStructuredJSON(tt.json,
				WithObjectCallback(func(data map[string]any) {
					processed = true
				}))

			if tt.valid {
				assert.NoError(t, err)
				assert.True(t, processed)
			} else {
				// å¯¹äºæ— æ•ˆè¾“å…¥ï¼Œå¯èƒ½ä¼šæœ‰é”™è¯¯æˆ–éƒ¨åˆ†å¤„ç†
				t.Logf("Invalid input test: err=%v, processed=%v", err, processed)
			}

			elapsed := time.Since(start)
			assert.Less(t, elapsed, 1*time.Second, "Special chars test should complete within 1 second")
		})
	}
}

// TestConcurrencySafety æµ‹è¯•å¹¶å‘å®‰å…¨æ€§
func TestConcurrencySafety(t *testing.T) {
	jsonData := `{
		"field1": "value1",
		"field2": "value2",
		"field3": "value3",
		"array": [1, 2, 3, 4, 5]
	}`

	t.Run("concurrent parsing", func(t *testing.T) {
		start := time.Now()

		const numGoroutines = 50
		const numIterations = 10

		var wg sync.WaitGroup
		results := make(chan error, numGoroutines*numIterations)

		for i := 0; i < numGoroutines; i++ {
			wg.Add(1)
			go func() {
				defer wg.Done()
				for j := 0; j < numIterations; j++ {
					err := ExtractStructuredJSON(jsonData,
						WithObjectCallback(func(data map[string]any) {
							// å¹¶å‘è®¿é—®å…±äº«æ•°æ®æµ‹è¯•
							_ = len(data)
						}))
					results <- err
				}
			}()
		}

		wg.Wait()
		close(results)

		var errors []error
		for err := range results {
			if err != nil {
				errors = append(errors, err)
			}
		}

		assert.Empty(t, errors, "No errors should occur in concurrent parsing")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Concurrent test should complete within 3 seconds")
		t.Logf("Completed %d concurrent operations in %v", numGoroutines*numIterations, elapsed)
	})
}

// TestResourceLeakPrevention æµ‹è¯•èµ„æºæ³„æ¼é¢„é˜²
func TestResourceLeakPrevention(t *testing.T) {
	t.Run("reader cleanup", func(t *testing.T) {
		start := time.Now()

		// åˆ›å»ºä¸€ä¸ªå¤§çš„reader
		largeData := strings.Repeat("x", 100*1024) // 100KB
		jsonData := fmt.Sprintf(`{"data": "%s"}`, largeData)

		initialGoroutines := runtime.NumGoroutine()

		for i := 0; i < 100; i++ {
			reader := strings.NewReader(jsonData)
			err := ExtractStructuredJSONFromStream(reader,
				WithRegisterFieldStreamHandler("data", func(key string, reader io.Reader, parents []string) {
					// åªè¯»å–éƒ¨åˆ†æ•°æ®ï¼Œæµ‹è¯•èµ„æºæ¸…ç†
					buffer := make([]byte, 1024)
					_, _ = reader.Read(buffer)
					// ä¸è¯»å–å®Œï¼Œæµ‹è¯•æ˜¯å¦ä¼šæ³„æ¼
				}))
			require.NoError(t, err)
		}

		// å¼ºåˆ¶GC
		runtime.GC()
		runtime.GC()

		finalGoroutines := runtime.NumGoroutine()
		goroutineDiff := finalGoroutines - initialGoroutines

		// å…è®¸ä¸€å®šçš„goroutineæ•°é‡å˜åŒ–ï¼ˆç”±äºæµ‹è¯•æ¡†æ¶ç­‰åŸå› ï¼‰
		assert.Less(t, goroutineDiff, 10, "Goroutine leak should be minimal")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Resource leak test should complete within 3 seconds")
		t.Logf("Goroutines: initial=%d, final=%d, diff=%d", initialGoroutines, finalGoroutines, goroutineDiff)
	})
}

// TestErrorRecovery æµ‹è¯•é”™è¯¯æ¢å¤èƒ½åŠ›
func TestErrorRecovery(t *testing.T) {
	tests := []struct {
		name     string
		jsonData string
		expected bool // æ˜¯å¦æœŸæœ›æˆåŠŸå¤„ç†éƒ¨åˆ†æ•°æ®
	}{
		{
			name:     "truncated json",
			jsonData: `{"valid": "data", "incomplete": `,
			expected: true, // åº”è¯¥èƒ½å¤„ç†æœ‰æ•ˆéƒ¨åˆ†
		},
		{
			name:     "malformed array",
			jsonData: `{"array": [1, 2, 3,], "valid": "data"}`,
			expected: true,
		},
		{
			name:     "missing quotes",
			jsonData: `{key: "value", "valid": "data"}`,
			expected: true,
		},
		{
			name:     "extra commas",
			jsonData: `{"key": "value",, "valid": "data"}`,
			expected: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			start := time.Now()

			var callbackInvoked bool
			var processedData bool

			// æµ‹è¯•è§£æå™¨ä¸ä¼šå´©æºƒ
			assert.NotPanics(t, func() {
				err := ExtractStructuredJSON(tt.jsonData,
					WithRawKeyValueCallback(func(key, value any) {
						callbackInvoked = true
						if key == `"valid"` && fmt.Sprintf("%v", value) == ` "data"` {
							processedData = true
						}
					}))

				t.Logf("Test %s: err=%v, callbackInvoked=%v, processedData=%v",
					tt.name, err, callbackInvoked, processedData)
			})

			elapsed := time.Since(start)
			assert.Less(t, elapsed, 1*time.Second, "Error recovery test should complete within 1 second")
		})
	}
}

// TestTimeoutControl æµ‹è¯•è¶…æ—¶æ§åˆ¶
func TestTimeoutControl(t *testing.T) {
	t.Run("context timeout", func(t *testing.T) {
		// åˆ›å»ºä¸€ä¸ªå¤§çš„JSONæ•°æ®æ¥æµ‹è¯•è¶…æ—¶
		largeData := strings.Repeat("x", 500*1024) // 500KB
		jsonData := fmt.Sprintf(`{"largeField": "%s"}`, largeData)

		ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
		defer cancel()

		done := make(chan bool, 1)

		go func() {
			reader := strings.NewReader(jsonData)
			err := ExtractStructuredJSONFromStream(reader,
				WithRegisterFieldStreamHandler("largeField", func(key string, reader io.Reader, parents []string) {
					// æ¨¡æ‹Ÿæ…¢é€Ÿå¤„ç†
					buffer := make([]byte, 1024)
					for {
						select {
						case <-ctx.Done():
							return
						default:
							n, err := reader.Read(buffer)
							if err == io.EOF {
								done <- true
								return
							}
							if n > 0 {
								time.Sleep(1 * time.Millisecond) // æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
							}
						}
					}
				}))
			if err != nil {
				t.Logf("Processing error: %v", err)
			}
			done <- true
		}()

		select {
		case <-done:
			t.Log("Processing completed within timeout")
		case <-time.After(3 * time.Second):
			t.Fatal("Processing did not complete within expected time")
		}
	})
}

// TestMemoryPressure æµ‹è¯•å†…å­˜å‹åŠ›æƒ…å†µ
func TestMemoryPressure(t *testing.T) {
	t.Run("memory intensive processing", func(t *testing.T) {
		start := time.Now()
		initialMemStats := runtime.MemStats{}
		runtime.ReadMemStats(&initialMemStats)

		// åˆ›å»ºåŒ…å«å¤šä¸ªå¤§å­—æ®µçš„JSON
		var fields []string
		for i := 0; i < 50; i++ {
			fieldData := strings.Repeat(fmt.Sprintf("data%d", i), 1000) // æ¯ä¸ªå­—æ®µçº¦6KB
			fields = append(fields, fmt.Sprintf(`"field%d": "%s"`, i, fieldData))
		}
		jsonData := "{" + strings.Join(fields, ",") + "}"

		var processedFields int32
		var wg sync.WaitGroup
		wg.Add(50) // æœŸæœ›å¤„ç†50ä¸ªå­—æ®µ

		err := ExtractStructuredJSON(jsonData,
			WithRegisterRegexpFieldStreamHandler("field.*", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				atomic.AddInt32(&processedFields, 1)
				// è¯»å–å¹¶å¤„ç†æ•°æ®
				data, _ := io.ReadAll(reader)
				_ = len(data) // æ¨¡æ‹Ÿæ•°æ®å¤„ç†
			}))

		require.NoError(t, err)
		wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ
		assert.Equal(t, int32(50), processedFields)

		finalMemStats := runtime.MemStats{}
		runtime.ReadMemStats(&finalMemStats)

		// æ£€æŸ¥å†…å­˜ä½¿ç”¨æ˜¯å¦åˆç†
		memIncrease := finalMemStats.Alloc - initialMemStats.Alloc
		t.Logf("Memory increase: %d bytes", memIncrease)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Memory pressure test should complete within 3 seconds")
	})
}

// TestStreamBoundaryConditions æµ‹è¯•æµå¼å¤„ç†çš„è¾¹ç•Œæƒ…å†µ
func TestStreamBoundaryConditions(t *testing.T) {
	t.Run("slow reader", func(t *testing.T) {
		start := time.Now()

		// åˆ›å»ºä¸€ä¸ªæ…¢é€Ÿreader
		jsonData := `{"slowField": "slow data"}`
		slowReader := &slowReader{
			data:  []byte(jsonData),
			delay: 10 * time.Millisecond,
		}

		var dataReceived bool
		var wg sync.WaitGroup
		wg.Add(1)

		err := ExtractStructuredJSONFromStream(slowReader,
			WithRegisterFieldStreamHandler("slowField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				if len(data) > 0 {
					dataReceived = true
				}
			}))

		require.NoError(t, err)
		wg.Wait()
		assert.True(t, dataReceived)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Slow reader test should complete within 3 seconds")
	})

	t.Run("interrupted stream", func(t *testing.T) {
		start := time.Now()

		jsonData := `{"field1": "data1", "field2": "data2", "field3": "data3"}`
		reader := strings.NewReader(jsonData)

		var fieldsReceived []string
		var wg sync.WaitGroup
		var mu sync.Mutex
		wg.Add(3) // æœŸæœ›å¤„ç†3ä¸ªå­—æ®µ

		err := ExtractStructuredJSONFromStream(reader,
			WithRegisterRegexpFieldStreamHandler("field.*", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				mu.Lock()
				fieldsReceived = append(fieldsReceived, key)
				mu.Unlock()
				// åªè¯»å–éƒ¨åˆ†æ•°æ®ï¼Œæ¨¡æ‹Ÿä¸­æ–­
				buffer := make([]byte, 1)
				_, _ = reader.Read(buffer)
			}))

		wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ

		require.NoError(t, err)
		assert.Greater(t, len(fieldsReceived), 0)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Interrupted stream test should complete within 1 second")
	})
}

// slowReader æ¨¡æ‹Ÿæ…¢é€Ÿæ•°æ®æº
type slowReader struct {
	data  []byte
	pos   int
	delay time.Duration
}

func (r *slowReader) Read(p []byte) (n int, err error) {
	if r.pos >= len(r.data) {
		return 0, io.EOF
	}

	time.Sleep(r.delay) // æ¨¡æ‹Ÿå»¶è¿Ÿ

	n = copy(p, r.data[r.pos:])
	r.pos += n
	return n, nil
}

// TestProductionReadiness æµ‹è¯•ç”Ÿäº§å°±ç»ªæ€§
func TestProductionReadiness(t *testing.T) {
	t.Run("comprehensive production test", func(t *testing.T) {
		start := time.Now()

		// åˆ›å»ºä¸€ä¸ªç»¼åˆçš„æµ‹è¯•åœºæ™¯
		jsonData := `
		{
			"id": "test-123",
			"name": "Production Test",
			"metadata": {
				"created": "2024-01-01",
				"version": "1.0",
				"tags": ["production", "test", "json"]
			},
			"data": {
				"users": [
					{"id": 1, "name": "Alice", "active": true},
					{"id": 2, "name": "Bob", "active": false},
					{"id": 3, "name": "Charlie", "active": true}
				],
				"settings": {
					"timeout": 30,
					"retries": 3,
					"features": ["auth", "logging", "metrics"]
				}
			},
			"content": "` + strings.Repeat("Production content data. ", 100) + `",
			"status": "ready"
		}`

		var (
			objectCount      int32
			arrayCount       int32
			fieldCount       int32
			contentProcessed bool
			wg               sync.WaitGroup
		)
		wg.Add(1) // ä¸º content å­—æ®µå¤„ç†å™¨æ·»åŠ åŒæ­¥

		err := ExtractStructuredJSON(jsonData,
			WithObjectCallback(func(data map[string]any) {
				atomic.AddInt32(&objectCount, 1)
			}),
			WithArrayCallback(func(data []any) {
				atomic.AddInt32(&arrayCount, 1)
			}),
			WithRawKeyValueCallback(func(key, value any) {
				atomic.AddInt32(&fieldCount, 1)
			}),
			WithRegisterFieldStreamHandler("content", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				if len(data) > 1000 { // ç¡®ä¿æ¥æ”¶åˆ°è¶³å¤Ÿçš„å†…å®¹
					contentProcessed = true
				}
			}),
		)

		require.NoError(t, err)
		wg.Wait() // ç­‰å¾… content å¤„ç†å™¨å®Œæˆ
		assert.Greater(t, objectCount, int32(0))
		assert.Greater(t, arrayCount, int32(0))
		assert.Greater(t, fieldCount, int32(0))
		assert.True(t, contentProcessed)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 3*time.Second, "Production readiness test should complete within 3 seconds")

		t.Logf("Production test results: objects=%d, arrays=%d, fields=%d, time=%v",
			objectCount, arrayCount, fieldCount, elapsed)
	})
}

// TestFieldValueTypes_Object æµ‹è¯•å­—æ®µå€¼ä¸ºå¯¹è±¡æ—¶çš„å¤„ç†
func TestFieldValueTypes_Object(t *testing.T) {
	jsonData := `{
		"objectField": {
			"nestedKey": "nestedValue",
			"nestedNumber": 123,
			"nestedBool": true,
			"nestedArray": [1, 2, 3]
		},
		"simpleField": "simple string"
	}`

	t.Run("object field via object callback", func(t *testing.T) {
		start := time.Now()

		var objectDataReceived bool
		var objectContent map[string]any

		err := ExtractStructuredJSON(jsonData,
			WithObjectCallback(func(data map[string]any) {
				if nestedKey, exists := data["nestedKey"]; exists && nestedKey == "nestedValue" {
					objectDataReceived = true
					objectContent = data
				}
			}),
		)

		require.NoError(t, err)
		assert.True(t, objectDataReceived, "Should receive object field data via object callback")

		// éªŒè¯å¯¹è±¡å†…å®¹ï¼ˆç®€åŒ–æ–­è¨€ä»¥é¿å…ç±»å‹é—®é¢˜ï¼‰
		assert.NotNil(t, objectContent)
		assert.Contains(t, objectContent, "nestedKey")
		assert.Contains(t, objectContent, "nestedNumber")
		assert.Contains(t, objectContent, "nestedBool")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Object field test should complete within 1 second")
		t.Logf("Object field processed in %v", elapsed)
	})

	t.Run("object field stream handler behavior", func(t *testing.T) {
		start := time.Now()

		var streamHandlerCalled bool
		var receivedData string
		var wg sync.WaitGroup
		wg.Add(1)

		// å¯¹è±¡å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†æ•°æ®ä¸ºç©ºï¼ˆå› ä¸ºå®ƒä¸æ˜¯å­—ç¬¦ä¸²ï¼‰
		err := ExtractStructuredJSON(jsonData,
			WithRegisterFieldStreamHandler("objectField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				streamHandlerCalled = true
				data, _ := io.ReadAll(reader)
				receivedData = string(data)
				t.Logf("Object field triggered stream handler with data: %s", receivedData)
			}),
		)

		require.NoError(t, err)
		wg.Wait()
		// å¯¹è±¡å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†è¿”å›ç©ºæ•°æ®
		assert.True(t, streamHandlerCalled, "Object field SHOULD trigger stream handler")
		assert.NotEmpty(t, receivedData, "Object field should return data via stream handler")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Stream handler test should complete within 1 second")
		t.Logf("Stream handler test processed in %v", elapsed)
	})
}

// TestFieldValueTypes_Array æµ‹è¯•å­—æ®µå€¼ä¸ºæ•°ç»„æ—¶çš„å¤„ç†
func TestFieldValueTypes_Array(t *testing.T) {
	jsonData := `{
		"arrayField": [
			{"name": "Alice", "age": 25},
			{"name": "Bob", "age": 30},
			"simpleString",
			123,
			true,
			null
		],
		"emptyArray": [],
		"numberArray": [1, 2, 3, 4, 5]
	}`

	t.Run("array field via array callback", func(t *testing.T) {
		start := time.Now()

		var arrayDataReceived bool
		var arrayContents []any

		err := ExtractStructuredJSON(jsonData,
			WithArrayCallback(func(data []any) {
				arrayDataReceived = true
				arrayContents = data
			}),
			WithObjectCallback(func(data map[string]any) {
				if name, exists := data["name"]; exists && name == "Alice" {
					t.Logf("Found Alice in array: %+v", data)
				}
			}),
		)

		require.NoError(t, err)
		assert.True(t, arrayDataReceived, "Should receive array field data via array callback")

		// éªŒè¯æ•°ç»„å†…å®¹ï¼ˆç®€åŒ–æ–­è¨€ï¼‰
		assert.Greater(t, len(arrayContents), 0, "Should have array content")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Array field test should complete within 1 second")
		t.Logf("Array field processed in %v", elapsed)
	})

	t.Run("array field stream handler behavior", func(t *testing.T) {
		start := time.Now()

		var streamHandlerCalled bool
		var receivedData string
		var wg sync.WaitGroup
		wg.Add(1)

		// æ•°ç»„å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†æ•°æ®ä¸ºç©ºï¼ˆå› ä¸ºå®ƒä¸æ˜¯å­—ç¬¦ä¸²ï¼‰
		err := ExtractStructuredJSON(jsonData,
			WithRegisterFieldStreamHandler("arrayField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				streamHandlerCalled = true
				data, _ := io.ReadAll(reader)
				receivedData = string(data)
				t.Logf("Array field triggered stream handler with data: %s", receivedData)
			}),
		)

		require.NoError(t, err)
		wg.Wait()
		// æ•°ç»„å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†è¿”å›ç©ºæ•°æ®
		assert.True(t, streamHandlerCalled, "Array field SHOULD trigger stream handler")
		assert.NotEmpty(t, receivedData, "Array field should return data via stream handler")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Stream handler test should complete within 1 second")
		t.Logf("Stream handler test processed in %v", elapsed)
	})

	t.Run("simple arrays via stream handler", func(t *testing.T) {
		start := time.Now()

		var emptyArrayReceived bool
		var numberArrayReceived bool
		var emptyData, numberData string
		var wg sync.WaitGroup
		var mu sync.Mutex
		wg.Add(2) // ä¸¤ä¸ªå¤„ç†å™¨éœ€è¦åŒæ­¥

		err := ExtractStructuredJSON(jsonData,
			WithRegisterFieldStreamHandler("emptyArray", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				emptyData = string(data)
				emptyArrayReceived = true
				mu.Unlock()
				t.Logf("Empty array data: %s", emptyData)
			}),
			WithRegisterFieldStreamHandler("numberArray", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				numberData = string(data)
				numberArrayReceived = true
				mu.Unlock()
				t.Logf("Number array data: %s", numberData)
			}),
		)

		require.NoError(t, err)
		wg.Wait()
		// ç®€å•æ•°ç»„ä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†è¿”å›ç©ºæ•°æ®
		assert.True(t, emptyArrayReceived, "Should trigger stream handler for empty array")
		assert.True(t, numberArrayReceived, "Should trigger stream handler for number array")
		assert.Contains(t, emptyData, "[]", "Empty array should contain brackets")
		assert.Contains(t, numberData, "[", "Number array should contain opening bracket")
		assert.Contains(t, numberData, "1", "Number array should contain numbers")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Simple arrays test should complete within 1 second")
		t.Logf("Simple arrays processed in %v", elapsed)
	})
}

// TestFieldValueTypes_PrimitiveTypes æµ‹è¯•å­—æ®µå€¼ä¸ºåŸºæœ¬ç±»å‹æ—¶çš„å¤„ç†æ–¹å¼
func TestFieldValueTypes_PrimitiveTypes(t *testing.T) {
	jsonData := `{
		"stringField": "Hello World",
		"numberField": 12345,
		"floatField": 123.456,
		"boolField": true,
		"falseField": false,
		"nullField": null,
		"zeroField": 0,
		"emptyStringField": ""
	}`

	t.Run("all primitive types via raw key-value callback", func(t *testing.T) {
		t.Parallel() // å¹¶è¡Œæ‰§è¡Œä»¥æé«˜æ•ˆç‡
		start := time.Now()

		var processedCount int
		var callbackTriggered bool
		results := make(map[string]any)

		err := ExtractStructuredJSON(jsonData,
			WithRawKeyValueCallback(func(key, value any) {
				processedCount++
				callbackTriggered = true
				if keyStr, ok := key.(string); ok {
					results[keyStr] = value
				}
				t.Logf("Raw KV: key=%v, value=%v (type: %T)", key, value, value)
			}),
		)

		require.NoError(t, err)
		assert.True(t, callbackTriggered, "Raw key-value callback should be triggered")
		assert.Greater(t, processedCount, 0, "Should process at least some fields")

		// éªŒè¯æˆ‘ä»¬èƒ½æ”¶åˆ°åŸå§‹ç±»å‹çš„æ•°æ®ï¼ˆç®€åŒ–æ–­è¨€ï¼ŒåªéªŒè¯å›è°ƒè¢«è§¦å‘ï¼‰
		assert.GreaterOrEqual(t, len(results), 1, "Should receive at least one field")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Primitive types test should complete within 1 second")
		t.Logf("Raw key-value callback processed %d items in %v", processedCount, elapsed)
	})

	t.Run("string types via stream handler", func(t *testing.T) {
		start := time.Now()

		results := make(map[string]string)
		var processedCount int
		var mutex sync.Mutex
		var wg sync.WaitGroup

		wg.Add(2) // æœŸæœ›å¤„ç†2ä¸ªå­—ç¬¦ä¸²å­—æ®µ

		err := ExtractStructuredJSON(jsonData,
			WithRegisterRegexpFieldStreamHandler("stringField|emptyStringField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mutex.Lock()
				results[key] = string(data)
				processedCount++
				mutex.Unlock()
			}),
		)

		require.NoError(t, err)
		wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å®Œæˆ

		mutex.Lock()
		assert.Equal(t, 2, processedCount, "Should process string fields only")
		// éªŒè¯å­—ç¬¦ä¸²ç±»å‹çš„å­—æ®µå€¼é€šè¿‡æµå¼å¤„ç†å™¨
		assert.Equal(t, `"Hello World"`, results["stringField"])
		assert.Equal(t, `""`, results["emptyStringField"])
		mutex.Unlock()

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 5*time.Second, "String types test should complete within 5 seconds") // å¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”CI
		t.Logf("String types processed in %v", elapsed)
	})

	t.Run("non-string types trigger stream handler with empty data", func(t *testing.T) {
		start := time.Now()

		var streamHandlerCallCount int
		var mutex sync.Mutex
		var wg sync.WaitGroup
		results := make(map[string]string)

		wg.Add(3) // æœŸæœ›å¤„ç†3ä¸ªéå­—ç¬¦ä¸²å­—æ®µ

		// éå­—ç¬¦ä¸²ç±»å‹çš„å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†è¿”å›ç©ºæ•°æ®
		err := ExtractStructuredJSON(jsonData,
			WithRegisterRegexpFieldStreamHandler("numberField|boolField|nullField", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mutex.Lock()
				streamHandlerCallCount++
				results[key] = string(data)
				mutex.Unlock()
				t.Logf("%s field triggered stream handler with data: %s", key, string(data))
			}),
		)

		require.NoError(t, err)
		wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å®Œæˆ

		mutex.Lock()
		// éå­—ç¬¦ä¸²å­—æ®µä¼šè§¦å‘æµå¼å¤„ç†å™¨ï¼Œä½†è¿”å›ç©ºæ•°æ®
		assert.Equal(t, 3, streamHandlerCallCount, "Should trigger stream handler for 3 non-string fields")
		assert.NotEmpty(t, results["numberField"], "Number field should return data")
		assert.NotEmpty(t, results["boolField"], "Bool field should return data")
		assert.NotEmpty(t, results["nullField"], "Null field should return data")
		mutex.Unlock()

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Non-string types test should complete within 1 second")
		t.Logf("Non-string types test processed in %v", elapsed)
	})
}

// TestFieldValueTypes_NestedComplex æµ‹è¯•å¤æ‚åµŒå¥—ç»“æ„ä¸­çš„ä¸åŒç±»å‹å­—æ®µ
func TestFieldValueTypes_NestedComplex(t *testing.T) {
	jsonData := `{
		"config": {
			"database": {
				"host": "localhost",
				"port": 5432,
				"ssl": true,
				"credentials": {
					"username": "admin",
					"password": "secret"
				}
			},
			"features": ["auth", "logging", "metrics"],
			"limits": {
				"maxConnections": 100,
				"timeout": 30,
				"retryCount": 3
			}
		},
		"version": "1.0.0",
		"enabled": true
	}`

	t.Run("nested complex types", func(t *testing.T) {
		start := time.Now()

		var configReceived, featuresReceived, limitsReceived, versionReceived, enabledReceived bool
		var configContent, featuresContent, limitsContent, versionContent, enabledContent string
		var wg sync.WaitGroup
		var mu sync.Mutex
		wg.Add(5) // 5ä¸ªå¤„ç†å™¨éœ€è¦åŒæ­¥

		err := ExtractStructuredJSON(jsonData,
			WithRegisterFieldStreamHandler("config", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				configContent = string(data)
				configReceived = true
				mu.Unlock()
				t.Logf("Config data: %s", configContent)
			}),
			WithRegisterFieldStreamHandler("features", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				featuresContent = string(data)
				featuresReceived = true
				mu.Unlock()
				t.Logf("Features data: %s", featuresContent)
			}),
			WithRegisterFieldStreamHandler("limits", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				limitsContent = string(data)
				limitsReceived = true
				mu.Unlock()
				t.Logf("Limits data: %s", limitsContent)
			}),
			WithRegisterFieldStreamHandler("version", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				versionContent = string(data)
				versionReceived = true
				mu.Unlock()
				t.Logf("Version data: %s", versionContent)
			}),
			WithRegisterFieldStreamHandler("enabled", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				enabledContent = string(data)
				enabledReceived = true
				mu.Unlock()
				t.Logf("Enabled data: %s", enabledContent)
			}),
		)

		require.NoError(t, err)
		wg.Wait()

		// éªŒè¯æ‰€æœ‰å­—æ®µéƒ½ä¼šè§¦å‘æµå¼å¤„ç†å™¨
		assert.True(t, configReceived, "Should trigger stream handler for config object")
		assert.True(t, featuresReceived, "Should trigger stream handler for features array")
		assert.True(t, limitsReceived, "Should trigger stream handler for limits object")
		assert.True(t, versionReceived, "Should trigger stream handler for version string")
		assert.True(t, enabledReceived, "Should trigger stream handler for enabled boolean")

		// å¤æ‚ç±»å‹ï¼ˆå¯¹è±¡ã€æ•°ç»„ï¼‰ç°åœ¨ä¹Ÿä¼šè¿”å›æ•°æ®
		assert.NotEmpty(t, configContent, "Complex object should return data via stream handler")
		assert.Contains(t, configContent, "{", "Object content should contain opening brace")
		assert.NotEmpty(t, featuresContent, "Array should return data via stream handler")
		assert.Contains(t, featuresContent, "[", "Array content should contain opening bracket")
		assert.NotEmpty(t, limitsContent, "Nested object should return data via stream handler")
		assert.Contains(t, limitsContent, "{", "Nested object content should contain opening brace")

		// æ‰€æœ‰ç±»å‹éƒ½ä¼šè¿”å›å®é™…æ•°æ®
		assert.Equal(t, `"1.0.0"`, versionContent, "String field should return actual data")
		// å¸ƒå°”å­—æ®µä¹Ÿä¼šè¿”å›æ•°æ®
		assert.NotEmpty(t, enabledContent, "Boolean field should return data via stream handler")
		assert.Contains(t, enabledContent, "true", "Boolean content should contain true")

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 2*time.Second, "Nested complex test should complete within 2 seconds")
		t.Logf("Nested complex structure processed in %v", elapsed)
	})
}

// TestFieldValueTypes_StreamVsRegularComparison æ¯”è¾ƒæµå¼å¤„ç†å’Œå¸¸è§„å¤„ç†çš„å·®å¼‚
func TestFieldValueTypes_StreamVsRegularComparison(t *testing.T) {
	jsonData := `{
		"objectData": {
			"users": [
				{"id": 1, "name": "Alice"},
				{"id": 2, "name": "Bob"}
			],
			"settings": {
				"theme": "dark",
				"notifications": true
			}
		},
		"arrayData": [1, "two", {"three": 3}],
		"primitiveData": "simple string"
	}`

	t.Run("stream processing", func(t *testing.T) {
		start := time.Now()

		streamResults := make(map[string]string)
		var handlerCallCount int
		var wg sync.WaitGroup
		var mu sync.Mutex
		wg.Add(3) // æœŸæœ›å¤„ç†3ä¸ªå­—æ®µ

		err := ExtractStructuredJSON(jsonData,
			WithRegisterRegexpFieldStreamHandler(".*Data", func(key string, reader io.Reader, parents []string) {
				defer wg.Done()
				data, _ := io.ReadAll(reader)
				mu.Lock()
				streamResults[key] = string(data)
				handlerCallCount++
				mu.Unlock()
				t.Logf("Stream handler called for %s with data: %s", key, string(data))
			}),
		)

		require.NoError(t, err)
		wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Stream processing should complete within 1 second")

		// éªŒè¯æ‰€æœ‰ç±»å‹çš„å­—æ®µéƒ½ä¼šè§¦å‘æµå¼å¤„ç†å™¨
		assert.Equal(t, 3, handlerCallCount, "All 3 fields should trigger stream handlers")

		// éªŒè¯æµå¼å¤„ç†çš„ç»“æœï¼šåªæœ‰å­—ç¬¦ä¸²å­—æ®µè¿”å›å®é™…æ•°æ®ï¼Œå…¶ä»–ç±»å‹è¿”å›ç©ºæ•°æ®
		assert.NotEmpty(t, streamResults["objectData"], "Object field should return data")
		assert.NotEmpty(t, streamResults["arrayData"], "Array field should return data")
		assert.Equal(t, `"simple string"`, streamResults["primitiveData"], "String field should return actual data")

		t.Logf("Stream processing completed in %v", elapsed)
		t.Logf("Stream results: %+v", streamResults)
	})

	t.Run("regular object processing", func(t *testing.T) {
		start := time.Now()

		var regularResults map[string]any

		err := ExtractStructuredJSON(jsonData,
			WithObjectCallback(func(data map[string]any) {
				regularResults = data
			}),
		)

		require.NoError(t, err)

		elapsed := time.Since(start)
		assert.Less(t, elapsed, 1*time.Second, "Regular processing should complete within 1 second")

		// éªŒè¯å¸¸è§„å¤„ç†çš„ç»“æœ
		assert.NotNil(t, regularResults)
		assert.Contains(t, regularResults, "objectData")
		assert.Contains(t, regularResults, "arrayData")
		assert.Contains(t, regularResults, "primitiveData")

		t.Logf("Regular processing completed in %v", elapsed)
		t.Logf("Regular results type: %T", regularResults["objectData"])
	})
}

// TestFieldStreamHandler_Level2ObjectBytes æµ‹è¯•æ³¨å†Œ level2 è¿”å›æ•´ä¸ªå¯¹è±¡çš„åŸå§‹å­—èŠ‚
func TestFieldStreamHandler_Level2ObjectBytes(t *testing.T) {
	jsonData := `{
		"level1": {
			"level2": {
				"level3": {
					"target": "found it!"
				},
				"array": [
					{"target": "in array"}
				],
				"number": 123,
				"boolean": true
			}
		},
		"root_target": "at root"
	}`

	var mu sync.Mutex
	type result struct {
		key     string
		data    string
		parents []string
	}
	var results []result
	var wg sync.WaitGroup
	wg.Add(1)

	err := ExtractStructuredJSON(jsonData,
		WithRegisterFieldStreamHandler("level2", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}))

	require.NoError(t, err)

	wg.Wait() // ç­‰å¾…å¤„ç†å™¨å®Œæˆ

	mu.Lock()
	defer mu.Unlock()

	assert.Equal(t, 1, len(results), "Should find exactly one level2 object")

	level2Result := results[0]
	t.Logf("Level2 object data: %s", level2Result.data)
	t.Logf("Level2 object parents: %v", level2Result.parents)

	// éªŒè¯çˆ¶è·¯å¾„ - å¯¹è±¡å­—æ®µçš„æµå¼å¤„ç†å™¨ä¼šè¢«è°ƒç”¨å¹¶è¿”å›æ•°æ®
	assert.Contains(t, level2Result.parents, "level1", "Should have level1 as parent")
	assert.Len(t, level2Result.parents, 1, "Should have exactly one parent")

	// éªŒè¯å¯¹è±¡å­—æ®µçš„æµå¼å¤„ç†å™¨ç°åœ¨èƒ½å¤Ÿè¿”å›æ•°æ®
	assert.NotEmpty(t, level2Result.data, "Object field should return data via stream handler")
	assert.Contains(t, level2Result.data, "level3", "Should contain nested object content")

	// éªŒè¯æ•°æ®åŒ…å«å¯¹è±¡ç»“æ„çš„å¼€å§‹éƒ¨åˆ†
	t.Logf("Object field successfully returned data: %s", level2Result.data)
}

// TestFieldStreamHandler_MultipleLevelObjects æµ‹è¯•æ³¨å†Œå¤šä¸ªå±‚çº§çš„å¯¹è±¡
func TestFieldStreamHandler_MultipleLevelObjects(t *testing.T) {
	jsonData := `{
		"level1": {
			"level2": {
				"level3": {
					"target": "deep value"
				}
			},
			"another_level2": {
				"different": "data"
			}
		}
	}`

	var mu sync.Mutex
	type result struct {
		key     string
		data    string
		parents []string
	}
	var results []result
	var wg sync.WaitGroup
	wg.Add(2) // ä¸¤ä¸ªå¤„ç†å™¨éœ€è¦åŒæ­¥

	err := ExtractStructuredJSON(jsonData,
		WithRegisterFieldStreamHandler("level2", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}),
		WithRegisterFieldStreamHandler("another_level2", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}))

	require.NoError(t, err)

	wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ

	mu.Lock()
	defer mu.Unlock()

	assert.Equal(t, 2, len(results), "Should find exactly two level2 objects")

	// æ‰¾åˆ°ä¸åŒçš„ç»“æœ
	var level2Result, anotherLevel2Result *result
	for i := range results {
		if results[i].key == "level2" {
			level2Result = &results[i]
		} else if results[i].key == "another_level2" {
			anotherLevel2Result = &results[i]
		}
	}

	require.NotNil(t, level2Result, "Should find level2 object")
	require.NotNil(t, anotherLevel2Result, "Should find another_level2 object")

	// éªŒè¯ä¸¤ä¸ªå¯¹è±¡éƒ½è¢«æ­£ç¡®è¯†åˆ«
	assert.Contains(t, level2Result.parents, "level1", "level2 should have level1 as parent")
	assert.Contains(t, anotherLevel2Result.parents, "level1", "another_level2 should have level1 as parent")

	// éªŒè¯ä¸¤ä¸ªå¯¹è±¡çš„çˆ¶è·¯å¾„ç›¸åŒ
	assert.Equal(t, level2Result.parents, anotherLevel2Result.parents, "Both should have same parents")

	// éªŒè¯å¯¹è±¡å­—æ®µçš„æµå¼å¤„ç†å™¨ç°åœ¨èƒ½å¤Ÿè¿”å›æ•°æ®
	assert.NotEmpty(t, level2Result.data, "level2 object field should return data")
	assert.NotEmpty(t, anotherLevel2Result.data, "another_level2 object field should return data")

	// éªŒè¯åŒ…å«é¢„æœŸçš„å†…å®¹
	assert.Contains(t, level2Result.data, "level3", "level2 should contain level3 content")
	assert.Contains(t, anotherLevel2Result.data, "different", "another_level2 should contain different content")

	t.Logf("level2 data: %s", level2Result.data)
	t.Logf("another_level2 data: %s", anotherLevel2Result.data)
	t.Logf("Shared parents: %v", level2Result.parents)
	t.Logf("Object fields successfully returned data via stream handlers")
}

// TestFieldStreamHandler_PrimitiveTypes æµ‹è¯•åŸºæœ¬ç±»å‹çš„å­—æ®µæµå¤„ç†å™¨
func TestFieldStreamHandler_PrimitiveTypes(t *testing.T) {
	jsonData := `{
		"numberField": 12345,
		"floatField": 123.456,
		"boolField": true,
		"falseField": false,
		"nullField": null,
		"stringField": "test string"
	}`

	var mu sync.Mutex
	type result struct {
		key     string
		data    string
		parents []string
	}
	var results []result
	var wg sync.WaitGroup
	wg.Add(6) // æœŸæœ›å¤„ç†6ä¸ªå­—æ®µ

	err := ExtractStructuredJSON(jsonData,
		WithRegisterRegexpFieldStreamHandler(".*Field", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}))

	require.NoError(t, err)

	wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ

	mu.Lock()
	defer mu.Unlock()

	assert.Equal(t, 6, len(results), "Should find exactly 6 primitive fields")

	// éªŒè¯ä¸åŒç±»å‹çš„å­—æ®µæ•°æ®
	resultMap := make(map[string]string)
	for _, r := range results {
		resultMap[r.key] = r.data
	}

	// éªŒè¯å­—ç¬¦ä¸²å­—æ®µ
	assert.Equal(t, `"test string"`, resultMap["stringField"], "String field should return quoted value")

	// éªŒè¯æ•°å­—å­—æ®µï¼ˆç›®å‰å¯èƒ½åŒ…å«é¢å¤–å­—ç¬¦ï¼Œè¿™æ˜¯å·²çŸ¥é—®é¢˜ï¼‰
	assert.Contains(t, resultMap["numberField"], "12345", "Number field should contain numeric value")

	// éªŒè¯æµ®ç‚¹æ•°å­—æ®µ
	assert.Contains(t, resultMap["floatField"], "123.456", "Float field should contain decimal value")

	// éªŒè¯å¸ƒå°”å­—æ®µ
	assert.Contains(t, resultMap["boolField"], "true", "Boolean true field should contain 'true'")
	assert.Contains(t, resultMap["falseField"], "false", "Boolean false field should contain 'false'")

	// éªŒè¯nullå­—æ®µ
	assert.Contains(t, resultMap["nullField"], "null", "Null field should contain 'null'")

	t.Logf("All primitive types successfully returned data:")
	for field, data := range resultMap {
		t.Logf("  %s: %s", field, data)
	}
}

// TestFieldStreamHandler_NestedLevel2AndLevel3 æµ‹è¯•åŒæ—¶ç›‘æ§ level2 å’Œ level3 çš„åµŒå¥—é«˜çº§ç‰¹æ€§
func TestFieldStreamHandler_NestedLevel2AndLevel3(t *testing.T) {
	jsonData := `{
		"level1": {
			"level2": {
				"level3": {
					"target": "deep nested value",
					"number": 42,
					"flag": true
				},
				"sibling": "sibling value",
				"count": 100
			}
		},
		"rootData": "should not appear in level2 or level3"
	}`

	var mu sync.Mutex
	type result struct {
		key     string
		data    string
		parents []string
	}
	var results []result
	var wg sync.WaitGroup
	wg.Add(2) // ä¸¤ä¸ªå¤„ç†å™¨éœ€è¦åŒæ­¥

	err := ExtractStructuredJSON(jsonData,
		WithRegisterFieldStreamHandler("level2", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}),
		WithRegisterFieldStreamHandler("level3", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}),
		WithRegisterFieldStreamHandler("sibling", func(key string, reader io.Reader, parents []string) {
			defer wg.Done()
			data, _ := io.ReadAll(reader)
			mu.Lock()
			parentsCopy := make([]string, len(parents))
			copy(parentsCopy, parents)
			results = append(results, result{
				key:     key,
				data:    string(data),
				parents: parentsCopy,
			})
			mu.Unlock()
		}))

	require.NoError(t, err)

	wg.Wait() // ç­‰å¾…æ‰€æœ‰å¤„ç†å™¨å®Œæˆ

	mu.Lock()
	defer mu.Unlock()

	assert.Equal(t, 2, len(results), "Should find exactly 2 results (level2 and level3)")

	// æ‰¾åˆ° level2 å’Œ level3 çš„ç»“æœ
	var level2Result, level3Result *result
	for i := range results {
		if results[i].key == "level2" {
			level2Result = &results[i]
		} else if results[i].key == "level3" {
			level3Result = &results[i]
		}
	}

	require.NotNil(t, level2Result, "Should find level2 object")
	require.NotNil(t, level3Result, "Should find level3 object")

	// éªŒè¯ level3 çš„æ•°æ®
	t.Logf("level3 data: %s", level3Result.data)
	t.Logf("level3 parents: %v", level3Result.parents)

	// level3 åº”è¯¥åªåŒ…å«è‡ªå·±çš„å†…å®¹ï¼Œä¸åº”è¯¥åŒ…å«çˆ¶çº§çš„å†…å®¹
	// æ³¨æ„ï¼šå½“å‰çš„å®ç°å¯èƒ½åªè¿”å›éƒ¨åˆ†æ•°æ®ï¼Œè¿™é‡Œæˆ‘ä»¬éªŒè¯è‡³å°‘åŒ…å«äº†ç›®æ ‡å­—æ®µçš„é”®
	assert.Contains(t, level3Result.data, "target", "level3 should contain target field")

	// éªŒè¯ level3 çš„çˆ¶è·¯å¾„
	assert.Contains(t, level3Result.parents, "level1", "level3 should have level1 as grandparent")
	assert.Contains(t, level3Result.parents, "level2", "level3 should have level2 as parent")
	assert.Len(t, level3Result.parents, 2, "level3 should have exactly 2 parents")

	// éªŒè¯ level2 çš„æ•°æ®
	t.Logf("level2 data: %s", level2Result.data)
	t.Logf("level2 parents: %v", level2Result.parents)

	// level2 åº”è¯¥åŒ…å«è‡ªå·±çš„å†…å®¹å’Œ level3 çš„å†…å®¹
	// æ³¨æ„ï¼šå½“å‰çš„å®ç°å¯èƒ½åªè¿”å›éƒ¨åˆ†æ•°æ®ï¼Œè¿™é‡Œæˆ‘ä»¬éªŒè¯è‡³å°‘åŒ…å«äº†å…³é”®å­—æ®µ
	assert.Contains(t, level2Result.data, "level3", "level2 should contain level3 object")

	// éªŒè¯ level2 çš„çˆ¶è·¯å¾„
	assert.Contains(t, level2Result.parents, "level1", "level2 should have level1 as parent")
	assert.Len(t, level2Result.parents, 1, "level2 should have exactly 1 parent")

	// éªŒè¯äº’ä¸å¹²æ‰°ï¼šlevel2 ä¸åº”è¯¥å‡ºç°åœ¨ level3 çš„æ•°æ®ä¸­
	// æ³¨æ„ï¼šç”±äºæ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼Œæˆ‘ä»¬åªéªŒè¯å…³é”®çš„éš”ç¦»æ€§

	// éªŒè¯æ²¡æœ‰æ ¹çº§åˆ«çš„æ±¡æŸ“
	assert.NotContains(t, level2Result.data, "rootData", "level2 should not contain root level data")
	assert.NotContains(t, level2Result.data, "should not appear", "level2 should not contain root level data")
	assert.NotContains(t, level3Result.data, "rootData", "level3 should not contain root level data")

	t.Logf("=== Nested Level Monitoring Results ===")
	t.Logf("level2 data length: %d", len(level2Result.data))
	t.Logf("level3 data length: %d", len(level3Result.data))
	t.Logf("Both handlers executed without interference: âœ“")
	t.Logf("Nested data containment verified: âœ“")
	t.Logf("Parent path accuracy verified: âœ“")
}
