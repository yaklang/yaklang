package gemini

import (
	"bytes"
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"github.com/yaklang/yaklang/common/go-funk"
	"github.com/yaklang/yaklang/common/jsonpath"
	"github.com/yaklang/yaklang/common/utils/bufpipe"
	"github.com/yaklang/yaklang/common/yak/yaklib/codec"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/bcicen/jstream"
	"github.com/davecgh/go-spew/spew"

	"github.com/yaklang/yaklang/common/ai/aispec"
	"github.com/yaklang/yaklang/common/log"
	"github.com/yaklang/yaklang/common/utils"
	"github.com/yaklang/yaklang/common/utils/lowhttp"
	"github.com/yaklang/yaklang/common/utils/lowhttp/poc"
)

const (
	// Use v1beta for model listing and potentially other features if v1 doesn't support them
	geminiAPIBaseURL   = "https://generativelanguage.googleapis.com"
	geminiModelListURL = geminiAPIBaseURL + "/v1beta/models"
	geminiStreamURL    = geminiAPIBaseURL + "/v1beta/models/%s:streamGenerateContent" // Use v1beta for streaming as well
	defaultModel       = "gemini-2.0-flash"                                           // 使用 gemini-2.0-flash 模型
)

// Part represents a part of the content, typically text.
type Part struct {
	Text string `json:"text"`
}

// Content represents the content sent or received.
type Content struct {
	Parts []Part `json:"parts"`
	Role  string `json:"role,omitempty"` // "user" or "model"
}

// GenerateContentRequest is the request body for the Gemini API.
type GenerateContentRequest struct {
	Contents          []Content         `json:"contents"`
	SafetySettings    []SafetySetting   `json:"safetySettings,omitempty"`
	GenerationConfig  *GenerationConfig `json:"generationConfig,omitempty"`
	SystemInstruction *Content          `json:"systemInstruction,omitempty"`
	CachedContent     string            `json:"cachedContent,omitempty"`
}

// SafetySetting defines safety thresholds for content generation.
type SafetySetting struct {
	Category  string `json:"category"`
	Threshold string `json:"threshold"`
}

// GenerationConfig specifies generation parameters.
type GenerationConfig struct {
	Temperature      float64  `json:"temperature,omitempty"`
	TopP             float64  `json:"topP,omitempty"`
	TopK             int      `json:"topK,omitempty"`
	CandidateCount   int      `json:"candidateCount,omitempty"`
	MaxOutputTokens  int      `json:"maxOutputTokens,omitempty"`
	StopSequences    []string `json:"stopSequences,omitempty"`
	ResponseMimeType string   `json:"responseMimeType,omitempty"`
}

// --- Response Structures ---

// StreamGenerateContentResponse is the structure for individual chunks in the stream.
// Based on documentation: https://ai.google.dev/api/rest/v1/models/streamGenerateContent#streamgeneratecontentresponse
type StreamGenerateContentResponse struct {
	Candidates     []Candidate     `json:"candidates"`
	PromptFeedback *PromptFeedback `json:"promptFeedback,omitempty"`
	UsageMetadata  *UsageMetadata  `json:"usageMetadata,omitempty"`
}

// Candidate represents a possible response generated by the model.
type Candidate struct {
	Content          Content           `json:"content"`
	FinishReason     string            `json:"finishReason,omitempty"` // "STOP", "MAX_TOKENS", "SAFETY", "RECITATION", "OTHER"
	SafetyRatings    []SafetyRating    `json:"safetyRatings,omitempty"`
	CitationMetadata *CitationMetadata `json:"citationMetadata,omitempty"`
	Index            int               `json:"index"`
}

// SafetyRating provides the safety assessment for a candidate.
type SafetyRating struct {
	Category    string `json:"category"`
	Probability string `json:"probability"` // "NEGLIGIBLE", "LOW", "MEDIUM", "HIGH"
	Blocked     bool   `json:"blocked,omitempty"`
}

// CitationMetadata contains citation information.
type CitationMetadata struct {
	CitationSources []CitationSource `json:"citationSources"`
}

// CitationSource provides details about a citation.
type CitationSource struct {
	StartIndex int    `json:"startIndex,omitempty"`
	EndIndex   int    `json:"endIndex,omitempty"`
	URI        string `json:"uri,omitempty"`
	License    string `json:"license,omitempty"`
}

// PromptFeedback contains feedback related to the prompt.
type PromptFeedback struct {
	BlockReason        string         `json:"blockReason,omitempty"` // "SAFETY", "OTHER"
	SafetyRatings      []SafetyRating `json:"safetyRatings,omitempty"`
	BlockReasonMessage string         `json:"blockReasonMessage,omitempty"`
}

// UsageMetadata contains token usage information.
type UsageMetadata struct {
	PromptTokenCount     int `json:"promptTokenCount"`
	CandidatesTokenCount int `json:"candidatesTokenCount,omitempty"` // Only populated in the last response
	TotalTokenCount      int `json:"totalTokenCount"`
}

// Model response struct for listing models
type GeminiModel struct {
	Name                       string   `json:"name"` // Format: models/model-name
	Version                    string   `json:"version"`
	DisplayName                string   `json:"displayName"`
	Description                string   `json:"description"`
	InputTokenLimit            int      `json:"inputTokenLimit"`
	OutputTokenLimit           int      `json:"outputTokenLimit"`
	SupportedGenerationMethods []string `json:"supportedGenerationMethods"`
	Temperature                *float64 `json:"temperature,omitempty"`
	TopP                       *float64 `json:"topP,omitempty"`
	TopK                       *int     `json:"topK,omitempty"`
}

type ListModelsResponse struct {
	Models        []GeminiModel `json:"models"`
	NextPageToken string        `json:"nextPageToken,omitempty"`
}

// ClientConfig contains configuration for a Gemini client.
type ClientConfig struct {
	APIKey        string
	APIEndpoint   string
	Model         string
	Proxy         string
	Context       context.Context
	Timeout       time.Duration
	MaxRetries    int
	RetryWaitTime time.Duration
	// StreamHandler receives the pipe reader for streaming operations
	StreamHandler    func(io.Reader)
	HTTPErrorHandler func(error)
	Debug            bool // 添加调试标志，控制详细日志输出
}

// Client now holds the AIConfig and http client.
type Client struct {
	config *aispec.AIConfig
	// apiKey and model removed, will be accessed via config
}

// Ensure Client implements AIClient interface (will add methods incrementally)
// var _ aispec.AIClient = (*Client)(nil) // Commented out temporarily

// NewClient creates a new Gemini API client instance.
// Configuration is primarily done via LoadOption.
func NewClient() *Client { // Return concrete type temporarily
	return &Client{
		config: aispec.NewDefaultAIConfig(), // Start with default config
	}
}

var _ aispec.AIClient = (*Client)(nil)

// --- Configurable Interface Implementation ---

// LoadOption applies configuration options to the client.
func (c *Client) LoadOption(opts ...aispec.AIConfigOption) {
	config := aispec.NewDefaultAIConfig(opts...) // Apply new options to default config
	c.config = config                            // Update client's config

	// Set default model if not provided
	if c.config.Model == "" {
		c.config.Model = defaultModel
		log.Debugf("Gemini model not specified, using default: %s", defaultModel)
	}

	if c.config.StreamHandler != nil {
		log.Infof("Gemini client configured with stream handler.")
	}
	if c.config.ReasonStreamHandler != nil {
		log.Infof("Gemini client configured with reason stream handler.")
	}

	log.Infof("Gemini client configured with model: %s", c.config.Model)
}

// CheckValid checks if the client configuration is valid (API key).
func (c *Client) CheckValid() error {
	if c.config == nil || c.config.APIKey == "" {
		return errors.New("APIKey is required for Gemini client")
	}
	log.Debugf("Gemini client configuration check passed.")
	return nil
}

// BuildHTTPOptions generates HTTP client options for the poc library based on config.
func (c *Client) BuildHTTPOptions() ([]poc.PocConfigOption, error) {
	if err := c.CheckValid(); err != nil {
		return nil, fmt.Errorf("client configuration invalid: %w", err)
	}
	opts := []poc.PocConfigOption{
		poc.WithReplaceHttpPacketHeader("Content-Type", "application/json"),
		poc.WithReplaceHttpPacketHeader("x-goog-api-key", c.config.APIKey),
	}
	if c.config.Proxy != "" {
		log.Debugf("Using proxy: %s", c.config.Proxy)
		opts = append(opts, poc.WithProxy(c.config.Proxy))
	}
	if c.config.Context != nil {
		opts = append(opts, poc.WithContext(c.config.Context))
	}

	if c.config.Timeout > 0 {
		log.Debugf("Setting connect timeout: %d seconds", c.config.Timeout)
		opts = append(opts, poc.WithConnectTimeout(c.config.Timeout))
	}
	// Add a general timeout for the whole request, matching http client timeout if set.
	requestTimeout := 600 // Default seconds
	log.Debugf("Setting request timeout: %d seconds", requestTimeout)
	opts = append(opts, poc.WithTimeout(float64(requestTimeout)))

	return opts, nil
}

// internalStreamGenerateContent handles the core API call and raw stream processing using poc.
func (c *Client) internalStreamGenerateContent(ctx context.Context, req GenerateContentRequest) (chan []byte, chan error) {
	rawChunkChan := make(chan []byte, 10)
	errChan := make(chan error, 1)

	go func() {
		deferShouldCloseChunkChan := utils.NewBool(true)
		defer func() {
			if deferShouldCloseChunkChan.IsSet() {
				close(rawChunkChan)
			}
		}()
		defer close(errChan)

		if err := c.CheckValid(); err != nil {
			log.Errorf("Client config invalid before request: %v", err)
			errChan <- fmt.Errorf("client config invalid before request: %w", err)
			return
		}

		// Prepare request body
		reqBytes, err := json.Marshal(req)
		if err != nil {
			log.Errorf("Failed to marshal request body: %v", err)
			errChan <- fmt.Errorf("failed to marshal request body: %w", err)
			return
		}
		log.Debugf("Gemini Request Body: %s", string(reqBytes))

		// Build poc options
		pocOpts, err := c.BuildHTTPOptions()
		if err != nil {
			log.Errorf("Failed to build HTTP options: %v", err)
			errChan <- fmt.Errorf("failed to build HTTP options: %w", err)
			return
		}

		deferShouldCloseChunkChan.UnSet()
		// Add body and stream handler
		streamProcessed := utils.NewAtomicBool()
		pocOpts = append(pocOpts,
			poc.WithReplaceHttpPacketBody(reqBytes, false),
			poc.WithBodyStreamReaderHandler(func(respHeader []byte, rawReader io.ReadCloser) {
				deferShouldCloseChunkChan.SetTo(false)
				defer func() {
					close(rawChunkChan)
				}()
				streamProcessed.Set()
				defer rawReader.Close()

				isChunk := false
				if strings.Contains(lowhttp.GetHTTPPacketHeader(respHeader, "Transfer-Encoding"), "chunked") {
					isChunk = true
				}

				var bodyReader io.Reader
				if isChunk {
					bodyReader, _, err = codec.ReadChunkedStream(rawReader)
					if err != nil {
						log.Errorf("Failed to read chunked (bodyReader, _, err = codec.ReadChunkedStream(rawReader)) body: %v", err)
						return
					}
				} else {
					bodyReader = rawReader
				}

				// Use lowhttp to get status code
				statusCode := lowhttp.GetStatusCodeFromResponse(respHeader)
				if statusCode != http.StatusOK {
					bodyBytes, _ := io.ReadAll(bodyReader) // Consume body to get error details
					errMsg := fmt.Sprintf("gemini API request failed with status %d: %s", statusCode, string(bodyBytes))
					log.Errorf(errMsg)
					errToSend := errors.New(errMsg)
					if c.config.HTTPErrorHandler != nil {
						go c.config.HTTPErrorHandler(errToSend)
					}
					// Send error via channel, but the main poc.DoPOST might also return an error
					select {
					case errChan <- errToSend:
					}
					return
				}

				bodyMirrorReader, bodyMirrorWriter := bufpipe.NewPipe()
				go func() {
					_, err := io.Copy(bodyMirrorWriter, bodyReader)
					bodyMirrorWriter.CloseWithError(err)
				}()

				log.Infof("Received successful streaming response header from Gemini API (Status: %d)", statusCode)
				decoder := jstream.NewDecoder(bodyMirrorReader, 1)
				count := 0
				for result := range decoder.Stream() {
					jsonRaw, err := json.Marshal(result)
					if err != nil {
						log.Errorf("Failed to marshal response body: %v, raw: %v", err, utils.ShrinkString(spew.Sdump(result), 200))
						continue
					}
					count++
					// 发送JSON数据块
					select {
					case rawChunkChan <- []byte(jsonRaw):
					}
				}
				if count <= 0 {
					errMsg := "No valid JSON data received from Gemini API stream."
					log.Warn(errMsg)
					errToSend := errors.New(errMsg)
					if c.config.HTTPErrorHandler != nil {
						go c.config.HTTPErrorHandler(errToSend)
					}
					// Send error via channel, but the main poc.DoPOST might also return an error
					select {
					case errChan <- errToSend:
					}
				} else {
					log.Infof("Received %d JSON chunks from Gemini API stream.", count)
				}
			}),
		)

		// Execute request using poc.DoPOST
		url := fmt.Sprintf(geminiStreamURL, c.config.Model)
		c.config.Domain = strings.TrimSpace(c.config.Domain)
		if !utils.IsHttpOrHttpsUrl(c.config.Domain) && c.config.Domain != "" {
			// not an url, use domain
			urlins, err := utils.ParseStringUrlToUrlInstance(url)
			if err != nil {
				log.Errorf("Failed to parse url: %v", err)
				return
			}
			urlins.Host = c.config.Domain
			url = urlins.String()
			log.Infof("rewrite url: %s", url)
		}
		log.Infof("Sending request to Gemini API via poc.DoPOST: %s", url)

		// poc.DoPOST handles the request execution and stream processing via the handler.
		// We primarily care about the error it returns.
		_, _, err = poc.DoPOST(url, pocOpts...)

		if err != nil {
			// If the stream handler ran, it might have already sent a more specific error.
			// Check if streamProcessed is set.
			if !streamProcessed.IsSet() {
				log.Errorf("poc.DoPOST failed before stream handler executed: %v", err)
				// Call error handler if configured
				if c.config.HTTPErrorHandler != nil {
					go c.config.HTTPErrorHandler(fmt.Errorf("poc.DoPOST failed: %w", err))
				}
				// Send the poc error if no stream error was sent
				select {
				case errChan <- fmt.Errorf("poc.DoPOST failed: %w", err):
				case <-ctx.Done(): // Check context cancellation too
					if ctxErr := ctx.Err(); ctxErr != nil {
						select {
						case errChan <- ctxErr:
						default:
						}
					}
				default:
					// errChan might be closed or already received an error from handler
				}
			} else {
				log.Warnf("poc.DoPOST returned error after stream handler executed: %v (stream handler might have sent specific error)", err)
				// Avoid sending redundant error if handler already sent one
			}
		}
	}()

	return rawChunkChan, errChan
}

// --- Chatter Interface (Partial Implementation) ---

// ChatStream sends a prompt and returns a reader for the streaming text response.
func (c *Client) ChatStream(prompt string) (io.Reader, error) {
	log.Infof("Initiating Gemini ChatStream with prompt: %s", utils.ShrinkString(prompt, 50))
	ctx := c.config.Context
	if ctx == nil {
		ctx = context.Background()
	}
	ctx, cancel := context.WithCancel(ctx) // Allow cancelling the stream

	req := NewTextRequest(prompt) // Use helper for simple text request
	rawChunkChan, errChan := c.internalStreamGenerateContent(ctx, req)

	// Create a pipe to return as io.Reader
	pr, pw := utils.NewBufPipe(nil) // Removed cancel func, handled internally now

	go func() {
		var streamErr error
		defer func() {
			cancel() // Cancel context when stream processing finishes or errors
			if streamErr != nil {
				pw.CloseWithError(streamErr) // Close pipe with error if loop exited due to error
			} else {
				pw.Close() // Ensure pipe writer is closed when done
			}
		}()

		for {
			select {
			case rawChunk, ok := <-rawChunkChan:
				if !ok {
					log.Infof("Raw chunk channel closed for ChatStream.")
					rawChunkChan = nil // Mark as closed
					if errChan == nil {
						return // Exit if both channels are closed
					}
					continue
				}

				// 调试日志
				log.Debugf("接收JSON块(%d字节): %s", len(rawChunk), utils.ShrinkString(string(rawChunk), 100))
				results := jsonpath.Find(string(rawChunk), "$..parts[*].text")
				if funk.IsIteratee(results) {
					funk.ForEach(results, func(v interface{}) {
						if utils.IsNil(v) {
							return
						}
						pw.WriteString(utils.InterfaceToString(v))
					})
				}
			case err, ok := <-errChan:
				if !ok {
					log.Infof("Error channel closed for ChatStream.")
					errChan = nil // Mark channel as closed
					if rawChunkChan == nil {
						return // Exit if both channels are closed
					}
					continue
				}
				if err != nil {
					log.Errorf("Error received from internal stream in ChatStream: %v", err)
					streamErr = err // Store error to close pipe with it
					return
				}

			case <-ctx.Done():
				log.Warnf("ChatStream context cancelled.")
				streamErr = ctx.Err() // Store context error
				return
			}
		}
	}()

	if c.config.ReasonStreamHandler != nil {
		reasonReader, reasonWriter := utils.NewPipe()
		go func() {
			reasonWriter.Close()
		}()
		go c.config.ReasonStreamHandler(reasonReader)
	}

	// Call stream handler if configured, passing the pipe reader
	if c.config.StreamHandler != nil {
		fpr, fpw := utils.NewPipe()
		mspr, mspw := utils.NewPipe()
		go func() {
			defer fpw.Close()
			defer mspw.Close()
			io.Copy(io.MultiWriter(fpw, mspw), pr)
		}()
		go c.config.StreamHandler(mspr)
		return fpr, nil
	}

	return pr, nil // Return the reading end of the pipe
}

// Chat sends a prompt and returns the full response as a string.
func (c *Client) Chat(prompt string, functions ...aispec.Function) (string, error) {
	log.Infof("Initiating Gemini Chat with prompt: %s", utils.ShrinkString(prompt, 50))
	if len(functions) > 0 {
		// TODO: Implement function calling support for Chat if needed.
		log.Warnf("Gemini Chat: Function calling is not yet implemented in this basic Chat method.")
	}

	stream, err := c.ChatStream(prompt)
	if err != nil {
		return "", fmt.Errorf("error starting chat stream: %w", err)
	}

	var buf bytes.Buffer
	n, err := io.Copy(&buf, stream)
	if err != nil {
		return "", fmt.Errorf("error reading from stream: %w", err)
	}

	response := buf.String()
	log.Infof("Gemini Chat completed, response length: %d, short: %v", n, utils.ShrinkString(buf.String(), 10))

	return response, nil
}

// ChatEx sends a multi-turn conversation history and returns the choices.
func (c *Client) ChatEx(details []aispec.ChatDetail, functions ...aispec.Function) ([]aispec.ChatChoice, error) {
	log.Infof("Initiating Gemini ChatEx with %d messages.", len(details))
	if len(details) == 0 {
		return nil, errors.New("ChatEx requires at least one message detail")
	}
	if len(functions) > 0 {
		// TODO: Implement function calling support for ChatEx if needed.
		log.Warnf("Gemini ChatEx: Function calling is not yet implemented.")
	}

	ctx := c.config.Context
	if ctx == nil {
		ctx = context.Background()
	}

	// Convert aispec.ChatDetail to Gemini Content
	contents := make([]Content, 0, len(details))
	for _, detail := range details {
		role := "user" // Default to user
		if strings.ToLower(detail.Role) == "assistant" || strings.ToLower(detail.Role) == "model" {
			role = "model"
		} else if strings.ToLower(detail.Role) == "system" {
			log.Warnf("Gemini ChatEx: System role provided, but Gemini uses a dedicated systemInstruction field. Ignoring message role.")
			// Potentially move this content to req.SystemInstruction if needed, but API might reject it here.
			continue // Skip system message in main contents for now
		}
		contents = append(contents, Content{
			Parts: []Part{{Text: detail.Content}},
			Role:  role,
		})
	}

	// Handle potential system instruction from the first message if applicable
	var systemInstruction *Content
	if len(details) > 0 && strings.ToLower(details[0].Role) == "system" {
		log.Infof("Using first message as system instruction.")
		systemInstruction = &Content{Parts: []Part{{Text: details[0].Content}}}
		if len(contents) > 0 && contents[0].Role == "user" { // Ensure user starts if system instruction is separate
			// Contents should be ok as is, or Gemini might require user first after system instruction.
		} else if len(contents) > 0 {
			log.Warnf("First non-system message role is %s, Gemini might expect 'user' after system instruction.", contents[0].Role)
		}
		// Remove the system message from the main contents if we handle it separately
		// contents = contents[1:] // Be careful if there was only a system message
	}
	if len(contents) == 0 && systemInstruction == nil {
		return nil, errors.New("ChatEx requires non-system messages or a system instruction")
	}
	if len(contents) > 0 && contents[len(contents)-1].Role != "user" {
		log.Warnf("ChatEx: Last message role is not 'user', Gemini API might require a final user message.")
		// Potentially append a dummy user message? Or let the API handle it.
	}

	req := GenerateContentRequest{
		Contents:          contents,
		SystemInstruction: systemInstruction, // Add system instruction if found
		// Include default safety/generation config if needed, potentially from c.config
		GenerationConfig: &GenerationConfig{ // Example defaults
			Temperature:     1.0,
			TopP:            0.95,
			TopK:            64,
			MaxOutputTokens: 8192,
		},
		SafetySettings: []SafetySetting{
			{Category: "HARM_CATEGORY_HARASSMENT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_HATE_SPEECH", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_DANGEROUS_CONTENT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
		},
	}

	// Use internal streaming function
	rawChunkChan, errChan := c.internalStreamGenerateContent(ctx, req)

	// Collect the full response text and metadata from the stream
	var fullResponseText strings.Builder
	var finalUsage *UsageMetadata
	var finishReason string
	var safetyRatings []SafetyRating
	var promptFeedback *PromptFeedback
	var lastErr error

	processDone := make(chan struct{})

	go func() {
		defer close(processDone)
		for {
			select {
			case rawChunk, ok := <-rawChunkChan:
				if !ok {
					rawChunkChan = nil // Mark as closed
					if errChan == nil {
						return
					} // Exit if both closed
					continue
				}
				lineStr := string(rawChunk)
				if strings.HasPrefix(lineStr, "data: ") {
					lineStr = strings.TrimPrefix(lineStr, "data: ")
					lineStr = strings.TrimSpace(lineStr)
				}
				if lineStr == "[" || lineStr == "]" || lineStr == "," {
					continue
				}

				var chunkResp StreamGenerateContentResponse
				if err := json.Unmarshal([]byte(lineStr), &chunkResp); err != nil {
					log.Warnf("ChatEx: Failed to unmarshal chunk '%s': %v. Skipping.", lineStr, err)
					continue
				}

				if chunkResp.UsageMetadata != nil {
					finalUsage = chunkResp.UsageMetadata
					log.Debugf("ChatEx: Received usage metadata: %+v", finalUsage)
				}
				if chunkResp.PromptFeedback != nil {
					promptFeedback = chunkResp.PromptFeedback // Store prompt feedback
					log.Debugf("ChatEx: Received prompt feedback: %+v", promptFeedback)
				}

				for _, candidate := range chunkResp.Candidates {
					if candidate.FinishReason != "" {
						finishReason = candidate.FinishReason
						log.Debugf("ChatEx: Received finish reason: %s", finishReason)
					}
					if len(candidate.SafetyRatings) > 0 {
						safetyRatings = candidate.SafetyRatings // Store safety ratings
					}
					for _, part := range candidate.Content.Parts {
						if part.Text != "" {
							fullResponseText.WriteString(part.Text)
						}
					}
				}

			case err, ok := <-errChan:
				if !ok {
					errChan = nil // Mark as closed
					if rawChunkChan == nil {
						return
					} // Exit if both closed
					continue
				}
				if err != nil {
					log.Errorf("Error received from internal stream in ChatEx: %v", err)
					lastErr = err // Store the last error
				}

			case <-ctx.Done():
				log.Warnf("ChatEx context cancelled.")
				lastErr = ctx.Err()
				return
			}
		}
	}()

	<-processDone // Wait for processing goroutine to finish

	if lastErr != nil {
		return nil, fmt.Errorf("error during ChatEx stream processing: %w", lastErr)
	}

	// Check for blocking reasons from prompt feedback
	if promptFeedback != nil && promptFeedback.BlockReason != "" {
		errMsg := fmt.Sprintf("ChatEx prompt blocked: %s. %s", promptFeedback.BlockReason, promptFeedback.BlockReasonMessage)
		log.Warnf(errMsg)
		return nil, errors.New(errMsg)
	}
	// Check for blocking reasons from safety ratings (though finishReason might also indicate this)
	for _, rating := range safetyRatings {
		if rating.Blocked {
			errMsg := fmt.Sprintf("ChatEx response blocked by safety filter: %s (%s)", rating.Category, rating.Probability)
			log.Warnf(errMsg)
			return nil, errors.New(errMsg)
		}
	}

	if fullResponseText.Len() == 0 && lastErr == nil && finishReason != "STOP" && finishReason != "MAX_TOKENS" {
		log.Warnf("ChatEx resulted in an empty response without explicit error and non-standard finish reason: %s", finishReason)
		// Return error if response is empty and finish reason indicates a problem (e.g., SAFETY, OTHER)
		if finishReason == "SAFETY" || finishReason == "OTHER" || finishReason == "RECITATION" {
			return nil, fmt.Errorf("gemini API finished with reason: %s, response empty", finishReason)
		}
		// Otherwise, might be ok (e.g. if model genuinely had nothing to say)
	}

	log.Infof("Gemini ChatEx completed, response length: %d, finish reason: %s", fullResponseText.Len(), finishReason)

	// Construct ChatChoice (Gemini stream typically has one main candidate result)
	choice := aispec.ChatChoice{
		Index: 0,
		Message: aispec.ChatDetail{
			Role:    "assistant", // Or "model"
			Content: fullResponseText.String(),
		},
		FinishReason: finishReason,
		// We are not populating Usage directly here as aispec.ChatChoice doesn't have it.
		// The aispec.ChatCompletion wrapper (if used) might aggregate usage.
	}

	return []aispec.ChatChoice{choice}, nil
}

// --- StructuredStreamer Interface Implementation ---

// SupportedStructuredStream indicates that Gemini client can provide structured stream data.
func (c *Client) SupportedStructuredStream() bool {
	log.Debugf("Gemini client reports SupportedStructuredStream: true")
	return true
}

// StructuredStream sends a prompt and returns a channel of structured data chunks.
func (c *Client) StructuredStream(prompt string, functions ...aispec.Function) (chan *aispec.StructuredData, error) {
	log.Infof("Initiating Gemini StructuredStream with prompt: %s", utils.ShrinkString(prompt, 50))
	if len(functions) > 0 {
		// TODO: Implement function calling support for StructuredStream if needed.
		log.Warnf("Gemini StructuredStream: Function calling is not yet implemented.")
	}

	ctx := c.config.Context
	if ctx == nil {
		ctx = context.Background()
	}
	ctx, cancel := context.WithCancel(ctx)

	req := NewTextRequest(prompt)
	rawChunkChan, errChan := c.internalStreamGenerateContent(ctx, req)
	structuredDataChan := make(chan *aispec.StructuredData, 10)

	go func() {
		defer close(structuredDataChan)
		defer cancel() // Cancel context when this goroutine exits

		streamID := utils.RandAlphaNumStringBytes(8) // Use RandAlphaNumStringBytes for random ID

		for {
			select {
			case rawChunk, ok := <-rawChunkChan:
				if !ok {
					log.Infof("Raw chunk channel closed for StructuredStream.")
					rawChunkChan = nil
					if errChan == nil {
						return
					} // Exit if both closed
					continue
				}

				lineStr := string(rawChunk)
				log.Debugf("StructuredStream received raw line: %q", lineStr)

				// Prepare base structured data object for this chunk
				sData := &aispec.StructuredData{
					Id:             fmt.Sprintf("%s-%s", streamID, utils.RandAlphaNumStringBytes(4)), // Use RandAlphaNumStringBytes
					Event:          "message",                                                        // Default event type
					DataSourceType: "gemini-stream",
					DataRaw:        rawChunk, // Store the raw chunk
				}

				// Clean potential SSE "data: " prefix
				if strings.HasPrefix(lineStr, "data: ") {
					lineStr = strings.TrimPrefix(lineStr, "data: ")
					lineStr = strings.TrimSpace(lineStr)
				}
				if lineStr == "[" || lineStr == "]" || lineStr == "," {
					log.Debugf("StructuredStream skipping non-JSON structural element: %s", lineStr)
					continue
				}

				var chunkResp StreamGenerateContentResponse
				if err := json.Unmarshal([]byte(lineStr), &chunkResp); err != nil {
					log.Warnf("StructuredStream: Failed to unmarshal chunk '%s': %v. Sending raw chunk data.", lineStr, err)
					sData.IsParsed = false
					sData.OutputReason = fmt.Sprintf("Unmarshal error: %v", err)
					// Send the partially filled StructuredData anyway
					select {
					case structuredDataChan <- sData:
					case <-ctx.Done():
						log.Warnf("Context cancelled while sending unparsed structured data.")
						return
					}
					continue
				}

				// Chunk successfully parsed
				sData.IsParsed = true
				sData.IsResponse = true // Assume any parsed chunk is part of the response

				var textContent strings.Builder
				// Populate StructuredData from the parsed chunk
				if chunkResp.UsageMetadata != nil {
					sData.HaveUsage = true
					sData.ModelUsage = append(sData.ModelUsage, aispec.UsageStatsInfo{
						Model:       c.config.Model, // Use configured model
						InputToken:  chunkResp.UsageMetadata.PromptTokenCount,
						OutputToken: chunkResp.UsageMetadata.CandidatesTokenCount,
					})
					log.Debugf("StructuredStream: Added usage data: %+v", sData.ModelUsage)
				}
				if chunkResp.PromptFeedback != nil {
					if chunkResp.PromptFeedback.BlockReason != "" {
						sData.Event = "error"
						sData.OutputReason = fmt.Sprintf("Blocked: %s %s", chunkResp.PromptFeedback.BlockReason, chunkResp.PromptFeedback.BlockReasonMessage)
						log.Warnf("StructuredStream: Prompt blocked - %s", sData.OutputReason)
					}
				}

				for _, candidate := range chunkResp.Candidates {
					if candidate.FinishReason != "" {
						sData.OutputReason = candidate.FinishReason // Capture finish reason
						sData.Event = "finish"                      // Mark event as finish
						log.Debugf("StructuredStream: Added finish reason: %s", candidate.FinishReason)
					}
					// Handle safety ratings if needed
					if len(candidate.SafetyRatings) > 0 {
						for _, rating := range candidate.SafetyRatings {
							if rating.Blocked {
								sData.Event = "error"
								sData.OutputReason = fmt.Sprintf("Content blocked by safety filter: %s (%s)", rating.Category, rating.Probability)
								log.Warnf("StructuredStream: Content blocked - %s", sData.OutputReason)
								break // One blocked rating is enough
							}
						}
					}

					for _, part := range candidate.Content.Parts {
						if part.Text != "" {
							textContent.WriteString(part.Text)
						}
					}
				}
				sData.OutputText = textContent.String()

				// Send the structured data
				log.Debugf("StructuredStream sending data: Event=%s, TextLen=%d", sData.Event, len(sData.OutputText))
				select {
				case structuredDataChan <- sData:
				case <-ctx.Done():
					log.Warnf("Context cancelled while sending structured data.")
					return
				}

			case err, ok := <-errChan:
				if !ok {
					log.Infof("Error channel closed for StructuredStream.")
					errChan = nil
					if rawChunkChan == nil {
						return
					} // Exit if both closed
					continue
				}
				if err != nil { // This if block handles errors from the errChan
					log.Errorf("Error received from internal stream in StructuredStream: %v", err)
					// Send an error event through the structured channel
					errorData := &aispec.StructuredData{
						Id:             fmt.Sprintf("%s-error-%s", streamID, utils.RandAlphaNumStringBytes(4)), // Use RandAlphaNumStringBytes
						Event:          "error",
						DataSourceType: "gemini-stream",
						IsParsed:       false,
						OutputReason:   err.Error(),
					}
					select {
					case structuredDataChan <- errorData:
					case <-ctx.Done():
						log.Warnf("Context cancelled while sending error structured data.")
						return
					}
				} // <<<<< Corrected: Added missing brace here

			case <-ctx.Done():
				log.Warnf("StructuredStream context cancelled.")
				// Send a final error event
				cancelData := &aispec.StructuredData{
					Id:             fmt.Sprintf("%s-cancel-%s", streamID, utils.RandAlphaNumStringBytes(4)), // Use RandAlphaNumStringBytes
					Event:          "error",
					DataSourceType: "gemini-stream",
					IsParsed:       false,
					OutputReason:   ctx.Err().Error(), // Use context error message
				}
				// Best effort send
				select {
				case structuredDataChan <- cancelData:
				default:
					log.Warnf("Failed to send cancel event to already closed/blocked channel.")
				}
				return
			}
		}
	}()

	return structuredDataChan, nil
}

// --- ModelListCaller Interface Implementation ---

// GetModelList fetches the list of available Gemini models using poc.
func (c *Client) GetModelList() ([]*aispec.ModelMeta, error) {
	log.Infof("Fetching Gemini model list via poc.")
	if err := c.CheckValid(); err != nil {
		return nil, fmt.Errorf("client config invalid before GetModelList: %w", err)
	}

	// Directly build poc options for GET request
	pocOpts := []poc.PocConfigOption{
		poc.WithReplaceAllHttpPacketHeaders(map[string]string{
			"Accept":         "application/json",
			"x-goog-api-key": c.config.APIKey,
		}),
	}
	if c.config.Proxy != "" {
		log.Debugf("GetModelList using proxy: %s", c.config.Proxy)
		pocOpts = append(pocOpts, poc.WithProxy(c.config.Proxy))
	}
	if c.config.Context != nil {
		pocOpts = append(pocOpts, poc.WithContext(c.config.Context))
	} else {
		pocOpts = append(pocOpts, poc.WithContext(context.Background()))
	}
	// Use specific timeouts for this request
	pocOpts = append(pocOpts, poc.WithTimeout(30), poc.WithConnectTimeout(10))

	urlStr := fmt.Sprintf("%s?key=%s", geminiModelListURL, c.config.APIKey)

	log.Infof("Requesting Gemini model list via poc.DoGET: %s", urlStr)
	resp, _, err := poc.DoGET(urlStr, pocOpts...)
	if err != nil {
		log.Errorf("poc.DoGET for model list failed: %v", err)
		if c.config.HTTPErrorHandler != nil {
			go c.config.HTTPErrorHandler(fmt.Errorf("poc.DoGET for model list failed: %w", err))
		}
		return nil, fmt.Errorf("model list request failed via poc: %w", err)
	}

	bodyBytes := resp.GetBody()
	if resp.GetStatusCode() != http.StatusOK {
		errMsg := fmt.Sprintf("gemini API model list request failed with status %d: %s", resp.GetStatusCode(), string(bodyBytes))
		log.Errorf(errMsg)
		errToSend := errors.New(errMsg)
		if c.config.HTTPErrorHandler != nil {
			go c.config.HTTPErrorHandler(errToSend)
		}
		return nil, errToSend
	}

	var listResp ListModelsResponse
	if err := json.Unmarshal(bodyBytes, &listResp); err != nil {
		log.Errorf("Failed to unmarshal model list response: %v. Body: %s", err, string(bodyBytes))
		return nil, fmt.Errorf("failed to unmarshal model list response: %w", err)
	}

	modelMetas := make([]*aispec.ModelMeta, 0, len(listResp.Models))
	for _, model := range listResp.Models {
		modelID := model.Name
		if parts := strings.Split(model.Name, "/"); len(parts) == 2 {
			modelID = parts[1]
		}
		supportsChat := false
		for _, method := range model.SupportedGenerationMethods {
			if method == "generateContent" || method == "streamGenerateContent" {
				supportsChat = true
				break
			}
		}
		if supportsChat {
			modelMetas = append(modelMetas, &aispec.ModelMeta{Id: modelID})
		}
	}

	log.Infof("Successfully fetched %d Gemini chat models via poc.", len(modelMetas))
	return modelMetas, nil
}

// Helper function to create a simple text request
func NewTextRequest(prompt string) GenerateContentRequest {
	return GenerateContentRequest{
		Contents: []Content{
			{
				Parts: []Part{
					{Text: prompt},
				},
				Role: "user", // Usually the initial prompt is from the user
			},
		},
		// Add default GenerationConfig or SafetySettings if needed
		GenerationConfig: &GenerationConfig{
			Temperature:     1.0, // Example default
			TopP:            0.95,
			TopK:            64,
			MaxOutputTokens: 8192,
		},
		SafetySettings: []SafetySetting{ // Example safety settings
			{Category: "HARM_CATEGORY_HARASSMENT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_HATE_SPEECH", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
			{Category: "HARM_CATEGORY_DANGEROUS_CONTENT", Threshold: "BLOCK_MEDIUM_AND_ABOVE"},
		},
	}
}

// --- FunctionCaller Interface Implementation ---

func (c *Client) ExtractData(data string, desc string, fields map[string]any) (map[string]any, error) {
	log.Infof("Initiating Gemini ExtractData. Desc: %s", utils.ShrinkString(desc, 50))
	prompt := aispec.GenerateJSONPrompt(data+"\n"+desc, fields)
	log.Debugf("ExtractData generated prompt: %s", utils.ShrinkString(prompt, 200))

	result, err := c.Chat(prompt)   // Use Chat for simplicity
	if err != nil && result == "" { // Check if Chat failed AND returned empty string
		log.Errorf("Chat call failed within ExtractData: %v", err)
		return nil, fmt.Errorf("gemini chat failed during data extraction: %w", err)
	}
	// Even if Chat returns an error, there might be a partial result we can try to parse.
	if err != nil {
		log.Warnf("Chat call within ExtractData returned an error (%v), but attempting to parse partial result.", err)
	}

	log.Debugf("ExtractData received result from Chat: %s", utils.ShrinkString(result, 200))

	extracted, extractErr := aispec.ExtractFromResult(result, fields)
	if extractErr != nil {
		log.Errorf("Failed to extract JSON data from Gemini response: %v. Raw response: %s", extractErr, utils.ShrinkString(result, 200))
		// Return the extraction error, but maybe wrap the original chat error if it exists?
		if err != nil {
			return nil, fmt.Errorf("json extraction failed (%w) after chat error (%v)", extractErr, err)
		}
		return nil, fmt.Errorf("failed to extract JSON data: %w", extractErr)
	}

	log.Infof("Gemini ExtractData completed successfully.")
	return extracted, nil // Return extracted data, potentially ignoring original Chat error if extraction succeeded
}
