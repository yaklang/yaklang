package aireact

import (
	"bytes"
	"context"
	"fmt"
	"sort"
	"strconv"
	"strings"

	"github.com/google/uuid"
	"github.com/yaklang/yaklang/common/ai/aid/aicommon"
	"github.com/yaklang/yaklang/common/ai/aid/aitool"
	"github.com/yaklang/yaklang/common/ai/rag"
	"github.com/yaklang/yaklang/common/consts"
	"github.com/yaklang/yaklang/common/log"
	"github.com/yaklang/yaklang/common/schema"
	"github.com/yaklang/yaklang/common/utils"
	"github.com/yaklang/yaklang/common/utils/memedit"
	"github.com/yaklang/yaklang/common/yakgrpc/yakit"
)

func (r *ReAct) EnhanceKnowledgeAnswer(ctx context.Context, userQuery string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}

	currentTask := r.GetCurrentTask()
	enhanceID := uuid.NewString()
	config := r.config

	ekm := config.EnhanceKnowledgeManager

	if ekm == nil {
		log.Errorf("enhanceKnowledgeManager is not configured, but ai choice knowledge enhance answer action, check config! use temp rag knowledge manager")
		ekm = rag.NewRagEnhanceKnowledgeManager()
		ekm.SetEmitter(r.Emitter)
	}

	enhanceData, err := ekm.FetchKnowledge(ctx, userQuery)
	if err != nil {
		return "", utils.Errorf("enhanceKnowledgeManager.FetchKnowledge(%s) failed: %v", userQuery, err)
	}

	// Collect all knowledge items for summary artifact
	var knowledgeList []aicommon.EnhanceKnowledge
	for enhanceDatum := range enhanceData {
		r.EmitKnowledge(enhanceID, enhanceDatum)
		ekm.AppendKnowledge(currentTask.GetId(), enhanceDatum)
		knowledgeList = append(knowledgeList, enhanceDatum)
	}
	knowledgeCount := len(knowledgeList)

	// Save all knowledge to a single artifact file
	if knowledgeCount > 0 {
		r.EmitKnowledgeReferenceArtifact(knowledgeList, userQuery)
	}

	var queryBuf bytes.Buffer
	queryBuf.WriteString(userQuery)

	enhance := r.DumpCurrentEnhanceData()

	// å¦‚æœçŸ¥è¯†æ¡ç›®è¿‡å¤šï¼ˆè¶…è¿‡ 5 æ¡ï¼‰ï¼Œä½¿ç”¨ AI æ™ºèƒ½å‹ç¼©
	// å‚è€ƒ loop_yaklangcode ä¸­çš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯
	if enhance != "" && knowledgeCount > 5 {
		log.Infof("EnhanceKnowledgeAnswer: %d knowledge items found, attempting AI compression", knowledgeCount)
		compressedEnhance := r.compressKnowledgeResults(ctx, enhance, userQuery, 15)
		if len(compressedEnhance) < len(enhance) {
			log.Infof("EnhanceKnowledgeAnswer: compressed from %d to %d chars", len(enhance), len(compressedEnhance))
			enhance = compressedEnhance
		}
	}

	if enhance != "" {
		enhancePayload, err := utils.RenderTemplate(`<|ENHANCE_DATA_{{ .Nonce }}|>
{{ .EnhanceData }}
<|ENHANCE_DATA_{{ .Nonce }}|>
`, map[string]interface{}{
			"Nonce":       nonce(),
			"EnhanceData": enhance,
		})
		if err != nil {
			log.Warnf("enhanceKnowledgeAnswer.DumpCurrentEnhanceData() failed: %v", err)
		}
		if enhancePayload != "" {
			queryBuf.WriteString("\n\n")
			queryBuf.WriteString(enhancePayload)
		}
	}

	// Build reference material content with original query and knowledge data
	referenceMaterial := ""
	if enhance != "" {
		referenceMaterial, _ = utils.RenderTemplate(`<|ORIGINAL_QUERY|>
{{ .OriginalQuery }}
<|ORIGINAL_QUERY_END|>

<|KNOWLEDGE_ENHANCED_DATA|>
{{ .EnhanceData }}
<|KNOWLEDGE_ENHANCED_DATA_END|>

çŸ¥è¯†æ¡ç›®æ•°é‡: {{ .KnowledgeCount }} (å·²é€šè¿‡ AI æ™ºèƒ½ç­›é€‰)
`, map[string]any{
			"OriginalQuery":  userQuery,
			"EnhanceData":    enhance,
			"KnowledgeCount": knowledgeCount,
		})
	}

	// Pass reference material to DirectlyAnswer for emission with stream
	var opts []any
	if referenceMaterial != "" {
		opts = append(opts, WithReferenceMaterial(referenceMaterial, 1))
	}

	finalResult, err := r.DirectlyAnswer(ctx, queryBuf.String(), nil, opts...)
	// Note: DirectlyAnswer already emits the result via stream
	// EmitTextArtifact only saves to file for reference, doesn't show duplicate UI
	if finalResult != "" {
		r.EmitTextArtifact("enhance_directly_answer", finalResult)
	}
	return finalResult, err
}

func (r *ReAct) EnhanceKnowledgeGetRandomN(ctx context.Context, n int, collections ...string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}
	_ = ctx // é¢„ç•™ ctx ä¾›åç»­ä½¿ç”¨

	if n <= 0 {
		n = 10
	}

	db := consts.GetGormProfileDatabase()
	var allEntries []*schema.KnowledgeBaseEntry

	// éå†æ¯ä¸ªçŸ¥è¯†åº“è·å–éšæœºæ¡ç›®
	for _, collectionName := range collections {
		// è·å–çŸ¥è¯†åº“ä¿¡æ¯
		kb, err := yakit.GetKnowledgeBaseByName(db, collectionName)
		if err != nil {
			log.Warnf("failed to get knowledge base %s: %v", collectionName, err)
			continue
		}

		// ä½¿ç”¨éšæœºæ’åºè·å–æ¡ç›®
		var entries []*schema.KnowledgeBaseEntry
		err = db.Model(&schema.KnowledgeBaseEntry{}).
			Where("knowledge_base_id = ?", kb.ID).
			Order("RANDOM()").
			Limit(n).
			Find(&entries).Error
		if err != nil {
			log.Warnf("failed to get random entries from knowledge base %s: %v", collectionName, err)
			continue
		}

		allEntries = append(allEntries, entries...)
	}

	if len(allEntries) == 0 {
		return "", nil
	}

	// æ ¼å¼åŒ–è¾“å‡º
	var result bytes.Buffer
	result.WriteString(fmt.Sprintf("=== çŸ¥è¯†åº“æ ·æœ¬æ•°æ® (å…± %d æ¡) ===\n\n", len(allEntries)))

	for i, entry := range allEntries {
		result.WriteString(fmt.Sprintf("ã€æ¡ç›® %dã€‘\n", i+1))
		result.WriteString(fmt.Sprintf("æ ‡é¢˜: %s\n", entry.KnowledgeTitle))
		if entry.Summary != "" {
			result.WriteString(fmt.Sprintf("æ‘˜è¦: %s\n", entry.Summary))
		}
		if len(entry.Keywords) > 0 {
			result.WriteString(fmt.Sprintf("å…³é”®è¯: %s\n", strings.Join(entry.Keywords, ", ")))
		}
		if entry.KnowledgeType != "" {
			result.WriteString(fmt.Sprintf("ç±»å‹: %s\n", entry.KnowledgeType))
		}
		if entry.KnowledgeDetails != "" {
			result.WriteString(fmt.Sprintf("è¯¦ç»†å†…å®¹: %s\n", entry.KnowledgeDetails))
		}
		result.WriteString("\n")
	}

	return result.String(), nil
}

func (r *ReAct) EnhanceKnowledgeGetter(ctx context.Context, userQuery string, collections ...string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}

	currentTask := r.GetCurrentTask()
	enhanceID := uuid.NewString()
	config := r.config

	ekm := config.EnhanceKnowledgeManager
	if ekm == nil {
		log.Errorf("enhanceKnowledgeManager is not configured, but ai choice knowledge enhance answer action, check config! use temp rag knowledge manager")
		ekm = rag.NewRagEnhanceKnowledgeManager()
		ekm.SetEmitter(r.Emitter)
	}

	enhanceData, err := ekm.FetchKnowledgeWithCollections(ctx, collections, userQuery)
	if err != nil {
		return "", utils.Errorf("enhanceKnowledgeManager.FetchKnowledge(%s) failed: %v", userQuery, err)
	}

	for enhanceDatum := range enhanceData {
		r.EmitKnowledge(enhanceID, enhanceDatum)
		ekm.AppendKnowledge(currentTask.GetId(), enhanceDatum)
	}

	var queryBuf bytes.Buffer
	queryBuf.WriteString(userQuery)

	enhance := r.DumpCurrentEnhanceData()
	if enhance != "" {
		enhancePayload, err := utils.RenderTemplate(`<|ENHANCE_DATA_{{ .Nonce }}|>
{{ .EnhanceData }}
<|ENHANCE_DATA_{{ .Nonce }}|>
`, map[string]interface{}{
			"Nonce":       nonce(),
			"EnhanceData": enhance,
		})
		if err != nil {
			log.Warnf("enhanceKnowledgeAnswer.DumpCurrentEnhanceData() failed: %v", err)
		}
		if enhancePayload != "" {
			queryBuf.WriteString("\n\n")
			queryBuf.WriteString(enhancePayload)
		}
	}

	return enhance, nil
}

// compressKnowledgeResults ä½¿ç”¨ AI æ™ºèƒ½å‹ç¼©çŸ¥è¯†æœç´¢ç»“æœ
// å‚è€ƒ loop_yaklangcode å’Œ aireducer ä¸­çš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯
// å°†é•¿å†…å®¹å¸¦è¡Œå·å±•ç¤ºï¼Œè®© AI ç­›é€‰å‡ºä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µ
// å¯¹äºè¶…å¤§å†…å®¹ï¼ˆ>20KBï¼‰ï¼Œä½¿ç”¨åˆ†ç‰‡ + overlap æŠ€æœ¯åˆ†æ‰¹å¤„ç†
func (r *ReAct) compressKnowledgeResults(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int) string {
	if len(knowledgeContent) == 0 {
		return knowledgeContent
	}

	// å¦‚æœå†…å®¹ä¸å¤Ÿé•¿ï¼Œä¸éœ€è¦å‹ç¼©
	if len(knowledgeContent) < 3000 {
		log.Infof("compressKnowledgeResults: content too short (%d chars), skip compression", len(knowledgeContent))
		return knowledgeContent
	}

	// è®¾ç½®é»˜è®¤å‚æ•°
	if maxRanges <= 0 {
		maxRanges = 15
	}

	// å¯¹äºè¶…å¤§å†…å®¹ï¼ˆ>20KBï¼‰ï¼Œä½¿ç”¨åˆ†ç‰‡å¤„ç†
	const maxChunkSize = 20 * 1024 // 20KB per chunk
	const overlapLines = 20        // 20 è¡Œ overlap
	const maxChunks = 10           // æœ€å¤š 10 ä¸ªåˆ†ç‰‡

	if len(knowledgeContent) > maxChunkSize {
		log.Infof("compressKnowledgeResults: content too large (%d bytes), using chunked processing", len(knowledgeContent))
		return r.compressKnowledgeResultsChunked(ctx, knowledgeContent, userQuery, maxRanges, maxChunkSize, overlapLines, maxChunks)
	}

	// å¯¹äºè¾ƒå°çš„å†…å®¹ï¼Œç›´æ¥å¤„ç†
	return r.compressKnowledgeResultsSingle(ctx, knowledgeContent, userQuery, maxRanges)
}

// compressKnowledgeResultsChunked ä½¿ç”¨åˆ†ç‰‡æ–¹å¼å¤„ç†è¶…å¤§å†…å®¹
// å€Ÿé‰´ aireducer çš„è®¾è®¡ï¼šå…ˆç»™æ•´ä¸ªå†…å®¹æ·»åŠ è¡Œå·ï¼Œç„¶åæŒ‰å¤§å°åˆ†ç‰‡
// ä½¿ç”¨è¡Œå· overlap ç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯
func (r *ReAct) compressKnowledgeResultsChunked(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int, chunkSize int, overlapLines int, maxChunks int) string {
	// æ­¥éª¤1: å…ˆæŒ‰è¡Œåˆ†å‰²åŸå§‹å†…å®¹
	originalLines := strings.Split(knowledgeContent, "\n")
	totalLines := len(originalLines)

	log.Infof("compressKnowledgeResultsChunked: processing %d bytes, %d lines, chunkSize=%d, overlapLines=%d, maxChunks=%d",
		len(knowledgeContent), totalLines, chunkSize, overlapLines, maxChunks)

	// æ­¥éª¤2: è®¡ç®—æ¯ä¸ª chunk åº”è¯¥åŒ…å«å¤šå°‘è¡Œ
	// ä¼°ç®—å¹³å‡æ¯è¡Œé•¿åº¦ï¼ˆè€ƒè™‘è¡Œå·å‰ç¼€çº¦ 10 å­—ç¬¦ï¼‰
	avgLineLen := len(knowledgeContent)/totalLines + 10
	linesPerChunk := chunkSize / avgLineLen
	if linesPerChunk < 50 {
		linesPerChunk = 50
	}

	// è°ƒæ•´ä»¥ç¡®ä¿ä¸è¶…è¿‡ maxChunks
	effectiveLinesPerChunk := linesPerChunk - overlapLines
	if effectiveLinesPerChunk <= 0 {
		effectiveLinesPerChunk = linesPerChunk / 2
	}
	estimatedChunks := (totalLines + effectiveLinesPerChunk - 1) / effectiveLinesPerChunk
	if estimatedChunks > maxChunks {
		effectiveLinesPerChunk = (totalLines + maxChunks - 1) / maxChunks
		linesPerChunk = effectiveLinesPerChunk + overlapLines
		log.Infof("compressKnowledgeResultsChunked: adjusted linesPerChunk to %d to limit chunks to %d", linesPerChunk, maxChunks)
	}

	// æ­¥éª¤3: åˆ†ç‰‡å¤„ç†
	type ChunkResult struct {
		ChunkIndex int
		StartLine  int
		EndLine    int
		Ranges     []RankedRange
	}
	var allChunkResults []ChunkResult

	chunkIndex := 0
	for startLineIdx := 0; startLineIdx < totalLines && chunkIndex < maxChunks; chunkIndex++ {
		// è®¡ç®—å½“å‰ chunk çš„è¡ŒèŒƒå›´ï¼ˆ1-based è¡Œå·ï¼‰
		startLine := startLineIdx + 1
		endLineIdx := startLineIdx + linesPerChunk
		if endLineIdx > totalLines {
			endLineIdx = totalLines
		}
		endLine := endLineIdx

		// æå–å½“å‰ chunk çš„è¡Œ
		chunkLines := originalLines[startLineIdx:endLineIdx]

		// æ„å»ºå¸¦è¡Œå·çš„ chunk å†…å®¹
		var chunkBuilder strings.Builder
		for i, line := range chunkLines {
			lineNum := startLineIdx + i + 1
			chunkBuilder.WriteString(fmt.Sprintf("%d | %s\n", lineNum, line))
		}
		chunkContent := chunkBuilder.String()

		// æ·»åŠ  overlap ä¸Šä¸‹æ–‡ï¼ˆä»å‰é¢å– overlapLines è¡Œï¼‰
		var chunkWithOverlap string
		if startLineIdx > 0 && overlapLines > 0 {
			overlapStartIdx := startLineIdx - overlapLines
			if overlapStartIdx < 0 {
				overlapStartIdx = 0
			}
			overlapLinesContent := originalLines[overlapStartIdx:startLineIdx]
			var overlapBuilder strings.Builder
			overlapBuilder.WriteString("--- [ä¸Šä¸‹æ–‡å¼€å§‹] ---\n")
			for i, line := range overlapLinesContent {
				lineNum := overlapStartIdx + i + 1
				overlapBuilder.WriteString(fmt.Sprintf("%d | %s\n", lineNum, line))
			}
			overlapBuilder.WriteString("--- [ä¸Šä¸‹æ–‡ç»“æŸ] ---\n\n")
			chunkWithOverlap = overlapBuilder.String() + chunkContent
		} else {
			chunkWithOverlap = chunkContent
		}

		// æ‰“å° chunk å†…å®¹æ‘˜è¦æ—¥å¿—
		chunkPreview := utils.ShrinkString(chunkContent, 300)
		log.Infof("compressKnowledgeResultsChunked: chunk %d/%d (lines %d-%d, %d lines, size=%d bytes):\n%s",
			chunkIndex+1, maxChunks, startLine, endLine, len(chunkLines), len(chunkContent), chunkPreview)

		// å¯¹å½“å‰ chunk è¿›è¡Œ AI ç­›é€‰
		chunkRanges := r.compressKnowledgeChunk(ctx, chunkWithOverlap, "", userQuery, maxRanges/2+1, startLine, endLine)

		if len(chunkRanges) > 0 {
			allChunkResults = append(allChunkResults, ChunkResult{
				ChunkIndex: chunkIndex,
				StartLine:  startLine,
				EndLine:    endLine,
				Ranges:     chunkRanges,
			})
			log.Infof("compressKnowledgeResultsChunked: chunk %d extracted %d ranges", chunkIndex+1, len(chunkRanges))
		} else {
			log.Infof("compressKnowledgeResultsChunked: chunk %d extracted 0 ranges", chunkIndex+1)
		}

		// ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª chunkï¼ˆå‡å» overlap è¡Œæ•°ï¼‰
		startLineIdx = endLineIdx - overlapLines
		if startLineIdx < 0 {
			startLineIdx = 0
		}
		// ç¡®ä¿å‘å‰æ¨è¿›
		if startLineIdx <= (endLineIdx - linesPerChunk) {
			startLineIdx = endLineIdx
		}
	}

	log.Infof("compressKnowledgeResultsChunked: processed %d chunks total", chunkIndex)

	// åˆå¹¶æ‰€æœ‰ chunk çš„ç»“æœ
	var allRanges []RankedRange
	for _, cr := range allChunkResults {
		allRanges = append(allRanges, cr.Ranges...)
	}

	if len(allRanges) == 0 {
		log.Warnf("compressKnowledgeResultsChunked: no valid ranges extracted from any chunk")
		// è¿”å›æˆªæ–­çš„åŸå§‹å†…å®¹
		if len(knowledgeContent) > 50000 {
			return knowledgeContent[:50000] + "\n\n[... å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ ...]"
		}
		return knowledgeContent
	}

	// æŒ‰ rank æ’åº
	sort.Slice(allRanges, func(i, j int) bool {
		return allRanges[i].Rank < allRanges[j].Rank
	})

	// é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
	if len(allRanges) > maxRanges {
		allRanges = allRanges[:maxRanges]
	}

	// å»é‡ï¼ˆåŸºäºè¡ŒèŒƒå›´é‡å ï¼‰
	allRanges = deduplicateRanges(allRanges)

	// ä»åŸå§‹å†…å®¹ä¸­æå–æœ€ç»ˆç»“æœ
	resultEditor := memedit.NewMemEditor(knowledgeContent)
	var result strings.Builder
	result.WriteString(fmt.Sprintf("ã€AI æ™ºèƒ½ç­›é€‰ã€‘ä» %d å­—èŠ‚å†…å®¹ä¸­æå–çš„ %d ä¸ªæœ€ç›¸å…³çŸ¥è¯†ç‰‡æ®µï¼š\n\n", len(knowledgeContent), len(allRanges)))

	totalExtracted := 0
	maxTotalLines := 200

	for i, item := range allRanges {
		text := resultEditor.GetTextFromPositionInt(item.StartLine, 1, item.EndLine, 1)
		if text == "" {
			continue
		}

		lineCount := strings.Count(text, "\n") + 1
		if totalExtracted+lineCount > maxTotalLines {
			result.WriteString(fmt.Sprintf("\n[... å·²è¾¾åˆ° %d è¡Œé™åˆ¶ï¼Œå‰©ä½™ %d ä¸ªç‰‡æ®µæœªå±•ç¤º ...]\n", maxTotalLines, len(allRanges)-i))
			break
		}

		result.WriteString(fmt.Sprintf("=== [%d] ç›¸å…³æ€§æ’åº: %d (è¡Œ %d-%d) ===\n", i+1, item.Rank, item.StartLine, item.EndLine))
		if item.Reason != "" {
			result.WriteString(fmt.Sprintf("ç›¸å…³æ€§è¯´æ˜: %s\n", item.Reason))
		}
		result.WriteString(text)
		result.WriteString("\n\n")

		totalExtracted += lineCount
	}

	finalResult := result.String()

	log.Infof("compressKnowledgeResultsChunked: compressed from %d chars to %d chars, %d ranges from %d chunks",
		len(knowledgeContent), len(finalResult), len(allRanges), len(allChunkResults))

	return finalResult
}

// RankedRange è¡¨ç¤ºä¸€ä¸ªå¸¦æ’åçš„è¡ŒèŒƒå›´
type RankedRange struct {
	Range     string
	StartLine int
	EndLine   int
	Rank      int
	Reason    string
	Text      string
}

// deduplicateRanges å»é™¤é‡å çš„èŒƒå›´
func deduplicateRanges(ranges []RankedRange) []RankedRange {
	if len(ranges) <= 1 {
		return ranges
	}

	var result []RankedRange
	for _, r := range ranges {
		overlaps := false
		for _, existing := range result {
			// æ£€æŸ¥æ˜¯å¦é‡å 
			if r.StartLine <= existing.EndLine && r.EndLine >= existing.StartLine {
				overlaps = true
				break
			}
		}
		if !overlaps {
			result = append(result, r)
		}
	}
	return result
}

// compressKnowledgeChunk å¯¹å•ä¸ª chunk è¿›è¡Œ AI ç­›é€‰
func (r *ReAct) compressKnowledgeChunk(ctx context.Context, chunkContentWithLineNum string, overlapContext string, userQuery string, maxRanges int, chunkStartLine int, chunkEndLine int) []RankedRange {
	dNonce := utils.RandStringBytes(4)
	minLines := 3
	maxLines := 20

	var overlapSection string
	if overlapContext != "" {
		overlapSection = fmt.Sprintf(`<|OVERLAP_CONTEXT_{{ .nonce }}|>
%s
<|OVERLAP_CONTEXT_END_{{ .nonce }}|>

`, overlapContext)
	}

	promptTemplate := `<|USER_QUERY_{{ .nonce }}|>
{{ .userQuery }}
<|USER_QUERY_END_{{ .nonce }}|>

` + overlapSection + `<|KNOWLEDGE_CHUNK_{{ .nonce }}|>
å½“å‰å¤„ç†åˆ†ç‰‡: è¡Œ {{ .chunkStart }} - {{ .chunkEnd }}
{{ .samples }}
<|KNOWLEDGE_CHUNK_END_{{ .nonce }}|>

<|INSTRUCT_{{ .nonce }}|>
ã€æ™ºèƒ½çŸ¥è¯†ç­›é€‰ã€‘è¯·ä»å½“å‰åˆ†ç‰‡ä¸­æå–ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
ä»ä¸Šè¿°å¸¦è¡Œå·çš„çŸ¥è¯†å†…å®¹ä¸­ï¼Œæå–ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„ç‰‡æ®µã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
1. æœ€å¤šæå– %d ä¸ªç‰‡æ®µ
2. æ¯ä¸ªç‰‡æ®µ %d-%d è¡Œ
3. ä½¿ç”¨åŸå§‹è¡Œå·ï¼ˆç¬¬ä¸€åˆ—æ•°å­—ï¼‰
4. æŒ‰ç›¸å…³æ€§æ’åºï¼ˆ1æœ€ç›¸å…³ï¼‰

ã€è¯„åˆ¤æ ‡å‡†ã€‘
- rank 1-3: ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜
- rank 4-7: ç›¸å…³èƒŒæ™¯/æŠ€æœ¯ç»†èŠ‚
- rank 8+: è¡¥å……æ€§ä¿¡æ¯

è¯·è¾“å‡º ranges æ•°ç»„ã€‚
<|INSTRUCT_END_{{ .nonce }}|>
`

	materials, err := utils.RenderTemplate(fmt.Sprintf(promptTemplate, maxRanges, minLines, maxLines), map[string]any{
		"nonce":      dNonce,
		"samples":    chunkContentWithLineNum,
		"userQuery":  userQuery,
		"chunkStart": chunkStartLine,
		"chunkEnd":   chunkEndLine,
	})

	if err != nil {
		log.Errorf("compressKnowledgeChunk: template render failed: %v", err)
		return nil
	}

	forgeResult, err := aicommon.InvokeLiteForge(
		materials,
		aicommon.WithContext(ctx),
		aicommon.WithLiteForgeOutputSchemaFromAIToolOptions(
			aitool.WithStructArrayParam(
				"ranges",
				[]aitool.PropertyOption{
					aitool.WithParam_Description("æŒ‰ç›¸å…³æ€§æ’åºçš„çŸ¥è¯†ç‰‡æ®µèŒƒå›´æ•°ç»„"),
				},
				nil,
				aitool.WithStringParam("range", aitool.WithParam_Description("åŸå§‹è¡ŒèŒƒå›´ï¼Œæ ¼å¼: start-end")),
				aitool.WithIntegerParam("rank", aitool.WithParam_Description("ç›¸å…³æ€§æ’åºï¼Œ1æœ€ç›¸å…³")),
				aitool.WithStringParam("relevance_reason", aitool.WithParam_Description("ç›¸å…³æ€§è¯´æ˜")),
			),
		),
	)

	if err != nil {
		log.Errorf("compressKnowledgeChunk: LiteForge failed: %v", err)
		return nil
	}

	if forgeResult == nil {
		return nil
	}

	rangeItems := forgeResult.GetInvokeParamsArray("ranges")
	var results []RankedRange

	for _, item := range rangeItems {
		rangeStr := item.GetString("range")
		rank := item.GetInt("rank")
		reason := item.GetString("relevance_reason")

		if rangeStr == "" {
			continue
		}

		parts := strings.Split(rangeStr, "-")
		if len(parts) != 2 {
			continue
		}

		startLine, err1 := strconv.Atoi(strings.TrimSpace(parts[0]))
		endLine, err2 := strconv.Atoi(strings.TrimSpace(parts[1]))

		if err1 != nil || err2 != nil || startLine <= 0 || endLine < startLine {
			continue
		}

		results = append(results, RankedRange{
			Range:     rangeStr,
			StartLine: startLine,
			EndLine:   endLine,
			Rank:      int(rank),
			Reason:    reason,
		})
	}

	return results
}

// compressKnowledgeResultsSingle å¯¹è¾ƒå°çš„å†…å®¹ç›´æ¥è¿›è¡Œå‹ç¼©ï¼ˆä¸åˆ†ç‰‡ï¼‰
func (r *ReAct) compressKnowledgeResultsSingle(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int) string {
	resultEditor := memedit.NewMemEditor(knowledgeContent)
	dNonce := utils.RandStringBytes(4)

	minLines := 5
	maxLines := 30

	promptTemplate := `<|USER_QUERY_{{ .nonce }}|>
{{ .userQuery }}
<|USER_QUERY_END_{{ .nonce }}|>

<|KNOWLEDGE_RESULTS_{{ .nonce }}|>
{{ .samples }}
<|KNOWLEDGE_RESULTS_END_{{ .nonce }}|>

<|INSTRUCT_{{ .nonce }}|>
ã€æ™ºèƒ½çŸ¥è¯†ç­›é€‰ä¸æ’åºã€‘

è¯·ä¸¥æ ¼æ ¹æ®ç”¨æˆ·é—®é¢˜ä»ä¸Šè¿°çŸ¥è¯†æœç´¢ç»“æœä¸­æå–æœ€æœ‰ä»·å€¼çš„çŸ¥è¯†ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§æ’åºï¼š

ã€æ ¸å¿ƒåŸåˆ™ã€‘
- å¿…é¡»ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³
- è¿‡æ»¤æ‰æ‰€æœ‰æ— å…³çš„çŸ¥è¯†ç‰‡æ®µ
- ä¼˜å…ˆé€‰æ‹©èƒ½ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†
- ä¿ç•™å®Œæ•´çš„çŸ¥è¯†æ¡ç›®ï¼Œé¿å…æˆªæ–­

ã€æå–è¦æ±‚ã€‘
1. æœ€å¤šæå– %d ä¸ªçŸ¥è¯†ç‰‡æ®µ
2. æ¯ä¸ªç‰‡æ®µ %d-%d è¡Œï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´
3. æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºï¼ˆrank: 1æœ€ç›¸å…³ï¼Œæ•°å­—è¶Šå¤§è¶Šä¸ç›¸å…³ï¼‰
4. ä¸¥æ ¼è¿‡æ»¤ä¸ç”¨æˆ·é—®é¢˜æ— å…³çš„çŸ¥è¯†

ã€ç›¸å…³æ€§è¯„åˆ¤æ ‡å‡†ã€‘ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
ğŸ”¥ æœ€é«˜ç›¸å…³ (rank 1-3)ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†
- åŒ…å«ç”¨æˆ·é—®é¢˜ä¸­æåˆ°çš„å…³é”®å®ä½“/æ¦‚å¿µ
- æä¾›å…·ä½“è§£å†³æ–¹æ¡ˆæˆ–æ“ä½œæ­¥éª¤

â­ é«˜åº¦ç›¸å…³ (rank 4-7)ï¼š
- ä¸ç”¨æˆ·é—®é¢˜é¢†åŸŸç›¸å…³çš„çŸ¥è¯†
- æä¾›èƒŒæ™¯ä¿¡æ¯æˆ–ç›¸å…³æ¦‚å¿µè§£é‡Š
- åŒ…å«ç›¸å…³çš„æŠ€æœ¯ç»†èŠ‚æˆ–é…ç½®

ğŸ“ ä¸€èˆ¬ç›¸å…³ (rank 8-15)ï¼š
- å¯èƒ½å¯¹ç†è§£é—®é¢˜æœ‰å¸®åŠ©çš„çŸ¥è¯†
- æä¾›è¡¥å……æ€§ä¿¡æ¯
- ç›¸å…³ä½†ä¸ç›´æ¥å›ç­”é—®é¢˜

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
{
  "range": "start-end", 
  "rank": æ•°å­—(1-15),
  "relevance_reason": "ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§è¯´æ˜"
}

ã€ä¸¥æ ¼è¦æ±‚ã€‘
- æ€»å†…å®¹æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
- é¿å…é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„çŸ¥è¯†ç‰‡æ®µ
- ä¼˜å…ˆé€‰æ‹©ä¿¡æ¯å¯†åº¦é«˜çš„çŸ¥è¯†
- ç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½å¯¹å›ç­”ç”¨æˆ·é—®é¢˜æœ‰ä»·å€¼

è¯·æŒ‰ç›¸å…³æ€§æ’åºè¾“å‡ºrangesæ•°ç»„ã€‚
<|INSTRUCT_END_{{ .nonce }}|>
`

	materials, err := utils.RenderTemplate(fmt.Sprintf(promptTemplate, maxRanges, minLines, maxLines), map[string]any{
		"nonce":     dNonce,
		"samples":   utils.PrefixLinesWithLineNumbers(knowledgeContent),
		"userQuery": userQuery,
	})

	if err != nil {
		log.Errorf("compressKnowledgeResultsSingle: template render failed: %v", err)
		return knowledgeContent
	}

	forgeResult, err := aicommon.InvokeLiteForge(
		materials,
		aicommon.WithContext(ctx),
		aicommon.WithLiteForgeOutputSchemaFromAIToolOptions(
			aitool.WithStructArrayParam(
				"ranges",
				[]aitool.PropertyOption{
					aitool.WithParam_Description("æŒ‰ç›¸å…³æ€§æ’åºçš„çŸ¥è¯†ç‰‡æ®µèŒƒå›´æ•°ç»„"),
				},
				nil,
				aitool.WithStringParam("range", aitool.WithParam_Description("è¡ŒèŒƒå›´ï¼Œæ ¼å¼: start-endï¼Œä¾‹å¦‚ 18-45")),
				aitool.WithIntegerParam("rank", aitool.WithParam_Description("ç›¸å…³æ€§æ’åºï¼Œ1æœ€ç›¸å…³ï¼Œæ•°å­—è¶Šå¤§è¶Šä¸ç›¸å…³")),
				aitool.WithStringParam("relevance_reason", aitool.WithParam_Description("ä¸ç”¨æˆ·é—®é¢˜çš„ç›¸å…³æ€§è¯´æ˜")),
			),
		),
	)

	if err != nil {
		log.Errorf("compressKnowledgeResultsSingle: LiteForge failed: %v", err)
		return knowledgeContent
	}

	if forgeResult == nil {
		log.Warnf("compressKnowledgeResultsSingle: forge result is nil")
		return knowledgeContent
	}

	rangeItems := forgeResult.GetInvokeParamsArray("ranges")

	if len(rangeItems) == 0 {
		log.Warnf("compressKnowledgeResultsSingle: no ranges extracted")
		return knowledgeContent
	}

	var rankedRanges []RankedRange
	totalLines := 0
	maxTotalLines := 150

	for _, item := range rangeItems {
		rangeStr := item.GetString("range")
		rank := item.GetInt("rank")
		reason := item.GetString("relevance_reason")

		if rangeStr == "" {
			continue
		}

		parts := strings.Split(rangeStr, "-")
		if len(parts) != 2 {
			log.Warnf("compressKnowledgeResultsSingle: invalid range format: %s", rangeStr)
			continue
		}

		startLine, err1 := strconv.Atoi(strings.TrimSpace(parts[0]))
		endLine, err2 := strconv.Atoi(strings.TrimSpace(parts[1]))

		if err1 != nil || err2 != nil {
			log.Errorf("compressKnowledgeResultsSingle: parse range failed: %s, errors: %v, %v", rangeStr, err1, err2)
			continue
		}

		if startLine <= 0 || endLine < startLine {
			log.Warnf("compressKnowledgeResultsSingle: invalid range values: %s (start=%d, end=%d)", rangeStr, startLine, endLine)
			continue
		}

		text := resultEditor.GetTextFromPositionInt(startLine, 1, endLine, 1)
		if text == "" {
			log.Warnf("compressKnowledgeResultsSingle: empty text for range: %s", rangeStr)
			continue
		}

		lineCount := strings.Count(text, "\n") + 1
		if totalLines+lineCount > maxTotalLines {
			log.Warnf("compressKnowledgeResultsSingle: would exceed %d lines limit, stopping at range: %s", maxTotalLines, rangeStr)
			break
		}

		rankedRanges = append(rankedRanges, RankedRange{
			Range:     rangeStr,
			StartLine: startLine,
			EndLine:   endLine,
			Rank:      int(rank),
			Reason:    reason,
			Text:      text,
		})

		totalLines += lineCount
	}

	if len(rankedRanges) == 0 {
		log.Warnf("compressKnowledgeResultsSingle: no valid ranges extracted")
		return knowledgeContent
	}

	sort.Slice(rankedRanges, func(i, j int) bool {
		return rankedRanges[i].Rank < rankedRanges[j].Rank
	})

	var result strings.Builder
	result.WriteString("ã€AI æ™ºèƒ½ç­›é€‰ã€‘æŒ‰ç›¸å…³æ€§æ’åºçš„çŸ¥è¯†ç‰‡æ®µï¼š\n\n")

	for i, item := range rankedRanges {
		result.WriteString(fmt.Sprintf("=== [%d] ç›¸å…³æ€§æ’åº: %d ===\n", i+1, item.Rank))
		if item.Reason != "" {
			result.WriteString(fmt.Sprintf("ç›¸å…³æ€§è¯´æ˜: %s\n", item.Reason))
		}
		result.WriteString(item.Text)
		result.WriteString("\n\n")
	}

	finalResult := result.String()

	log.Infof("compressKnowledgeResultsSingle: compressed from %d chars to %d chars, %d ranges extracted",
		len(knowledgeContent), len(finalResult), len(rankedRanges))

	return finalResult
}
