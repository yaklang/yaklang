package aireact

import (
	"bytes"
	"context"
	"fmt"
	"io"
	"sort"
	"strconv"
	"strings"

	"github.com/google/uuid"
	"github.com/yaklang/yaklang/common/ai/aid/aicommon"
	"github.com/yaklang/yaklang/common/ai/aid/aitool"
	"github.com/yaklang/yaklang/common/ai/rag"
	"github.com/yaklang/yaklang/common/ai/rag/vectorstore"
	"github.com/yaklang/yaklang/common/aiforge"
	"github.com/yaklang/yaklang/common/consts"
	"github.com/yaklang/yaklang/common/log"
	"github.com/yaklang/yaklang/common/schema"
	"github.com/yaklang/yaklang/common/utils"
	"github.com/yaklang/yaklang/common/utils/memedit"
	"github.com/yaklang/yaklang/common/yakgrpc/yakit"
	"github.com/yaklang/yaklang/common/yakgrpc/ypb"
)

func (r *ReAct) EnhanceKnowledgeAnswer(ctx context.Context, userQuery string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}

	currentTask := r.GetCurrentTask()
	enhanceID := uuid.NewString()
	config := r.config

	ekm := config.EnhanceKnowledgeManager

	if ekm == nil {
		log.Errorf("enhanceKnowledgeManager is not configured, but ai choice knowledge enhance answer action, check config! use temp rag knowledge manager")
		ekm = rag.NewRagEnhanceKnowledgeManager()
		ekm.SetEmitter(r.Emitter)
	}

	enhanceData, err := ekm.FetchKnowledge(ctx, userQuery)
	if err != nil {
		return "", utils.Errorf("enhanceKnowledgeManager.FetchKnowledge(%s) failed: %v", userQuery, err)
	}

	// Collect all knowledge items for summary artifact
	var knowledgeList []aicommon.EnhanceKnowledge
	for enhanceDatum := range enhanceData {
		r.EmitKnowledge(enhanceID, enhanceDatum)
		ekm.AppendKnowledge(currentTask.GetId(), enhanceDatum)
		knowledgeList = append(knowledgeList, enhanceDatum)
	}
	knowledgeCount := len(knowledgeList)

	// Save all knowledge to a single artifact file
	if knowledgeCount > 0 {
		r.EmitKnowledgeReferenceArtifact(knowledgeList, userQuery)
	}

	var queryBuf bytes.Buffer
	queryBuf.WriteString(userQuery)

	enhance := r.DumpCurrentEnhanceData()

	// å¦‚æœçŸ¥è¯†æ¡ç›®è¿‡å¤šï¼ˆè¶…è¿‡ 5 æ¡ï¼‰ï¼Œä½¿ç”¨ AI æ™ºèƒ½å‹ç¼©
	// å‚è€ƒ loop_yaklangcode ä¸­çš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯
	if enhance != "" && knowledgeCount > 5 {
		log.Infof("EnhanceKnowledgeAnswer: %d knowledge items found, attempting AI compression", knowledgeCount)
		compressedEnhance := r.compressKnowledgeResults(ctx, enhance, userQuery, 15)
		if len(compressedEnhance) < len(enhance) {
			log.Infof("EnhanceKnowledgeAnswer: compressed from %d to %d chars", len(enhance), len(compressedEnhance))
			enhance = compressedEnhance
		}
	}

	if enhance != "" {
		enhancePayload, err := utils.RenderTemplate(`<|ENHANCE_DATA_{{ .Nonce }}|>
{{ .EnhanceData }}
<|ENHANCE_DATA_{{ .Nonce }}|>
`, map[string]interface{}{
			"Nonce":       nonce(),
			"EnhanceData": enhance,
		})
		if err != nil {
			log.Warnf("enhanceKnowledgeAnswer.DumpCurrentEnhanceData() failed: %v", err)
		}
		if enhancePayload != "" {
			queryBuf.WriteString("\n\n")
			queryBuf.WriteString(enhancePayload)
		}
	}

	// Build reference material content with original query and knowledge data
	referenceMaterial := ""
	if enhance != "" {
		referenceMaterial, _ = utils.RenderTemplate(`<|ORIGINAL_QUERY|>
{{ .OriginalQuery }}
<|ORIGINAL_QUERY_END|>

<|KNOWLEDGE_ENHANCED_DATA|>
{{ .EnhanceData }}
<|KNOWLEDGE_ENHANCED_DATA_END|>

çŸ¥è¯†æ¡ç›®æ•°é‡: {{ .KnowledgeCount }} (å·²é€šè¿‡ AI æ™ºèƒ½ç­›é€‰)
`, map[string]any{
			"OriginalQuery":  userQuery,
			"EnhanceData":    enhance,
			"KnowledgeCount": knowledgeCount,
		})
	}

	// Pass reference material to DirectlyAnswer for emission with stream
	var opts []any
	if referenceMaterial != "" {
		opts = append(opts, WithReferenceMaterial(referenceMaterial, 1))
	}

	finalResult, err := r.DirectlyAnswer(ctx, queryBuf.String(), nil, opts...)
	// Note: DirectlyAnswer already emits the result via stream
	// EmitTextArtifact only saves to file for reference, doesn't show duplicate UI
	if finalResult != "" {
		r.EmitTextArtifact("enhance_directly_answer", finalResult)
	}
	return finalResult, err
}

func (r *ReAct) EnhanceKnowledgeGetRandomN(ctx context.Context, n int, collections ...string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}
	_ = ctx // é¢„ç•™ ctx ä¾›åç»­ä½¿ç”¨

	if n <= 0 {
		n = 10
	}

	db := consts.GetGormProfileDatabase()
	var allEntries []*schema.KnowledgeBaseEntry

	// éå†æ¯ä¸ªçŸ¥è¯†åº“è·å–éšæœºæ¡ç›®
	for _, collectionName := range collections {
		// è·å–çŸ¥è¯†åº“ä¿¡æ¯
		kb, err := yakit.GetKnowledgeBaseByName(db, collectionName)
		if err != nil {
			log.Warnf("failed to get knowledge base %s: %v", collectionName, err)
			continue
		}

		// ä½¿ç”¨éšæœºæ’åºè·å–æ¡ç›®
		var entries []*schema.KnowledgeBaseEntry
		err = db.Model(&schema.KnowledgeBaseEntry{}).
			Where("knowledge_base_id = ?", kb.ID).
			Order("RANDOM()").
			Limit(n).
			Find(&entries).Error
		if err != nil {
			log.Warnf("failed to get random entries from knowledge base %s: %v", collectionName, err)
			continue
		}

		allEntries = append(allEntries, entries...)
	}

	if len(allEntries) == 0 {
		return "", nil
	}

	// æ ¼å¼åŒ–è¾“å‡º
	var result bytes.Buffer
	result.WriteString(fmt.Sprintf("=== çŸ¥è¯†åº“æ ·æœ¬æ•°æ® (å…± %d æ¡) ===\n\n", len(allEntries)))

	for i, entry := range allEntries {
		result.WriteString(fmt.Sprintf("ã€æ¡ç›® %dã€‘\n", i+1))
		result.WriteString(fmt.Sprintf("æ ‡é¢˜: %s\n", entry.KnowledgeTitle))
		if entry.Summary != "" {
			result.WriteString(fmt.Sprintf("æ‘˜è¦: %s\n", entry.Summary))
		}
		if len(entry.Keywords) > 0 {
			result.WriteString(fmt.Sprintf("å…³é”®è¯: %s\n", strings.Join(entry.Keywords, ", ")))
		}
		if entry.KnowledgeType != "" {
			result.WriteString(fmt.Sprintf("ç±»å‹: %s\n", entry.KnowledgeType))
		}
		if entry.KnowledgeDetails != "" {
			result.WriteString(fmt.Sprintf("è¯¦ç»†å†…å®¹: %s\n", entry.KnowledgeDetails))
		}
		result.WriteString("\n")
	}

	return result.String(), nil
}

func (r *ReAct) EnhanceKnowledgeGetter(ctx context.Context, userQuery string, collections ...string) (string, error) {
	// é»˜è®¤ä½¿ç”¨å®Œæ•´å¢å¼ºæµç¨‹
	return r.EnhanceKnowledgeGetterEx(ctx, userQuery, nil, collections...)
}

// EnhanceKnowledgeGetterEx æ”¯æŒå¤šç§ EnhancePlan çš„çŸ¥è¯†å¢å¼ºè·å–å™¨
// enhancePlans å‚æ•°å¯é€‰ï¼Œæ”¯æŒï¼š
//   - nil æˆ–ç©ºåˆ‡ç‰‡ï¼šä½¿ç”¨é»˜è®¤å®Œæ•´å¢å¼ºæµç¨‹ï¼ˆhypothetical_answer, generalize_query, split_query, exact_keyword_searchï¼‰
//   - []string{"exact_keyword_search"}: ä»…ä½¿ç”¨ç²¾å‡†å…³é”®è¯æœç´¢ï¼ˆè·³è¿‡å…³é”®è¯ç”Ÿæˆï¼Œé€‚ç”¨äº keyword æœç´¢æ¨¡å¼ï¼‰
//   - []string{"hypothetical_answer"}: ä»…ä½¿ç”¨ HyDE å‡è®¾å›ç­”
//   - []string{"split_query"}: ä»…ä½¿ç”¨æ‹†åˆ†æŸ¥è¯¢
//   - []string{"generalize_query"}: ä»…ä½¿ç”¨æ³›åŒ–æŸ¥è¯¢
//   - å¯ç»„åˆä½¿ç”¨: []string{"hypothetical_answer", "generalize_query"}
func (r *ReAct) EnhanceKnowledgeGetterEx(ctx context.Context, userQuery string, enhancePlans []string, collections ...string) (string, error) {
	if utils.IsNil(ctx) {
		ctx = r.config.GetContext()
	}

	currentTask := r.GetCurrentTask()
	enhanceID := uuid.NewString()

	// æ„å»º RAG æŸ¥è¯¢é€‰é¡¹
	ragOpts := []rag.RAGSystemConfigOption{
		rag.WithRAGCtx(ctx),
		rag.WithEveryQueryResultCallback(func(data *rag.ScoredResult) {
			r.EmitKnowledge(enhanceID, data)
			if currentTask != nil && r.config.EnhanceKnowledgeManager != nil {
				r.config.EnhanceKnowledgeManager.AppendKnowledge(currentTask.GetId(), data)
			}
		}),
	}

	// è®¾ç½®é›†åˆåç§°é™åˆ¶
	if len(collections) > 0 {
		ragOpts = append(ragOpts, rag.WithRAGCollectionNames(collections...))
	}

	// è®¾ç½® EnhancePlan
	if len(enhancePlans) > 0 {
		ragOpts = append(ragOpts, rag.WithRAGEnhance(enhancePlans...))
	}
	// å¦‚æœ enhancePlans ä¸ºç©ºï¼Œä½¿ç”¨ RAG é»˜è®¤çš„å®Œæ•´å¢å¼ºæµç¨‹

	// é…ç½®æ—¥å¿—è¾“å‡º
	if r.Emitter != nil {
		ragOpts = append(ragOpts, rag.WithRAGLogReaderWithInfo(func(reader io.Reader, info *vectorstore.SubQueryLogInfo, referenceMaterialCallback func(content string)) {
			var event *schema.AiOutputEvent
			var err error
			event, err = r.Emitter.EmitDefaultStreamEvent(
				"enhance-query",
				reader,
				"",
				func() {
					if info.ResultBuffer != nil && info.ResultBuffer.Len() > 0 {
						streamId := ""
						if event != nil {
							streamId = event.GetContentJSONPath(`$.event_writer_id`)
						}
						if streamId != "" {
							_, emitErr := r.Emitter.EmitTextReferenceMaterial(streamId, info.ResultBuffer.String())
							if emitErr != nil {
								log.Warnf("failed to emit reference material: %v", emitErr)
							}
						}
					}
				},
			)
			if err != nil {
				log.Warnf("failed to emit enhance-query stream event: %v", err)
				return
			}
		}))
	}

	// æ‰§è¡Œ RAG æŸ¥è¯¢ï¼Œè¿”å›çš„ channel åŒ…å«æŸ¥è¯¢ç»“æœ
	resultCh, err := rag.QueryYakitProfile(userQuery, ragOpts...)
	if err != nil {
		return "", utils.Errorf("RAG QueryYakitProfile(%s) failed: %v", userQuery, err)
	}

	// æ¶ˆè´¹ç»“æœ channelï¼Œç­‰å¾…æŸ¥è¯¢å®Œæˆ
	// channel å…³é—­æ—¶è¡¨ç¤ºæŸ¥è¯¢å®Œæˆ
	for range resultCh {
		// ç»“æœå·²é€šè¿‡ WithEveryQueryResultCallback å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯ç­‰å¾… channel å…³é—­
	}

	// è·å–å¢å¼ºæ•°æ®
	enhance := r.DumpCurrentEnhanceData()
	if enhance != "" {
		enhancePayload, err := utils.RenderTemplate(`<|ENHANCE_DATA_{{ .Nonce }}|>
{{ .EnhanceData }}
<|ENHANCE_DATA_{{ .Nonce }}|>
`, map[string]interface{}{
			"Nonce":       nonce(),
			"EnhanceData": enhance,
		})
		if err != nil {
			log.Warnf("enhanceKnowledgeAnswer.DumpCurrentEnhanceData() failed: %v", err)
		}
		if enhancePayload != "" {
			enhance = enhancePayload
		}
	}

	return enhance, nil
}

// compressKnowledgeResults ä½¿ç”¨ AI æ™ºèƒ½å‹ç¼©çŸ¥è¯†æœç´¢ç»“æœ
// å‚è€ƒ loop_yaklangcode å’Œ aireducer ä¸­çš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯
// å°†é•¿å†…å®¹å¸¦è¡Œå·å±•ç¤ºï¼Œè®© AI ç­›é€‰å‡ºä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ç‰‡æ®µ
// å¯¹äºè¶…å¤§å†…å®¹ï¼ˆ>20KBï¼‰ï¼Œä½¿ç”¨åˆ†ç‰‡ + overlap æŠ€æœ¯åˆ†æ‰¹å¤„ç†
func (r *ReAct) compressKnowledgeResults(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int) string {
	if len(knowledgeContent) == 0 {
		return knowledgeContent
	}

	// å¦‚æœå†…å®¹ä¸å¤Ÿé•¿ï¼Œä¸éœ€è¦å‹ç¼©
	if len(knowledgeContent) < 3000 {
		log.Infof("compressKnowledgeResults: content too short (%d chars), skip compression", len(knowledgeContent))
		return knowledgeContent
	}

	// è®¾ç½®é»˜è®¤å‚æ•°
	if maxRanges <= 0 {
		maxRanges = 15
	}

	// å¯¹äºè¶…å¤§å†…å®¹ï¼ˆ>40KBï¼‰ï¼Œä½¿ç”¨åˆ†ç‰‡å¤„ç†
	const maxChunkSize = 40 * 1024 // 40KB per chunk
	const overlapLines = 20        // 20 è¡Œ overlap
	const maxChunks = 10           // æœ€å¤š 10 ä¸ªåˆ†ç‰‡

	if len(knowledgeContent) > maxChunkSize {
		log.Infof("compressKnowledgeResults: content too large (%d bytes), using chunked processing", len(knowledgeContent))
		return r.compressKnowledgeResultsChunked(ctx, knowledgeContent, userQuery, maxRanges, maxChunkSize, overlapLines, maxChunks)
	}

	// å¯¹äºè¾ƒå°çš„å†…å®¹ï¼Œç›´æ¥å¤„ç†
	return r.compressKnowledgeResultsSingle(ctx, knowledgeContent, userQuery, maxRanges)
}

// compressKnowledgeResultsChunked ä½¿ç”¨åˆ†ç‰‡æ–¹å¼å¤„ç†è¶…å¤§å†…å®¹
// å€Ÿé‰´ aireducer çš„è®¾è®¡ï¼šå…ˆç»™æ•´ä¸ªå†…å®¹æ·»åŠ è¡Œå·ï¼Œç„¶åæŒ‰å¤§å°åˆ†ç‰‡
// ä½¿ç”¨è¡Œå· overlap ç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯
func (r *ReAct) compressKnowledgeResultsChunked(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int, chunkSize int, overlapLines int, maxChunks int) string {
	// æ­¥éª¤1: å…ˆæŒ‰è¡Œåˆ†å‰²åŸå§‹å†…å®¹
	originalLines := strings.Split(knowledgeContent, "\n")
	totalLines := len(originalLines)

	log.Infof("compressKnowledgeResultsChunked: processing %d bytes, %d lines, chunkSize=%d, overlapLines=%d, maxChunks=%d",
		len(knowledgeContent), totalLines, chunkSize, overlapLines, maxChunks)

	// æ­¥éª¤2: è®¡ç®—æ¯ä¸ª chunk åº”è¯¥åŒ…å«å¤šå°‘è¡Œ
	// ä¼°ç®—å¹³å‡æ¯è¡Œé•¿åº¦ï¼ˆè€ƒè™‘è¡Œå·å‰ç¼€çº¦ 10 å­—ç¬¦ï¼‰
	avgLineLen := len(knowledgeContent)/totalLines + 10
	linesPerChunk := chunkSize / avgLineLen
	if linesPerChunk < 50 {
		linesPerChunk = 50
	}

	// è°ƒæ•´ä»¥ç¡®ä¿ä¸è¶…è¿‡ maxChunks
	effectiveLinesPerChunk := linesPerChunk - overlapLines
	if effectiveLinesPerChunk <= 0 {
		effectiveLinesPerChunk = linesPerChunk / 2
	}
	estimatedChunks := (totalLines + effectiveLinesPerChunk - 1) / effectiveLinesPerChunk
	if estimatedChunks > maxChunks {
		effectiveLinesPerChunk = (totalLines + maxChunks - 1) / maxChunks
		linesPerChunk = effectiveLinesPerChunk + overlapLines
		log.Infof("compressKnowledgeResultsChunked: adjusted linesPerChunk to %d to limit chunks to %d", linesPerChunk, maxChunks)
	}

	// æ­¥éª¤3: åˆ†ç‰‡å¤„ç†
	type ChunkResult struct {
		ChunkIndex int
		StartLine  int
		EndLine    int
		Ranges     []RankedRange
	}
	var allChunkResults []ChunkResult

	chunkIndex := 0
	for startLineIdx := 0; startLineIdx < totalLines && chunkIndex < maxChunks; chunkIndex++ {
		// è®¡ç®—å½“å‰ chunk çš„è¡ŒèŒƒå›´ï¼ˆ1-based è¡Œå·ï¼‰
		startLine := startLineIdx + 1
		endLineIdx := startLineIdx + linesPerChunk
		if endLineIdx > totalLines {
			endLineIdx = totalLines
		}
		endLine := endLineIdx

		// æå–å½“å‰ chunk çš„è¡Œ
		chunkLines := originalLines[startLineIdx:endLineIdx]

		// æ„å»ºå¸¦è¡Œå·çš„ chunk å†…å®¹
		var chunkBuilder strings.Builder
		for i, line := range chunkLines {
			lineNum := startLineIdx + i + 1
			chunkBuilder.WriteString(fmt.Sprintf("%d | %s\n", lineNum, line))
		}
		chunkContent := chunkBuilder.String()

		// æ·»åŠ  overlap ä¸Šä¸‹æ–‡ï¼ˆä»å‰é¢å– overlapLines è¡Œï¼‰
		var chunkWithOverlap string
		if startLineIdx > 0 && overlapLines > 0 {
			overlapStartIdx := startLineIdx - overlapLines
			if overlapStartIdx < 0 {
				overlapStartIdx = 0
			}
			overlapLinesContent := originalLines[overlapStartIdx:startLineIdx]
			var overlapBuilder strings.Builder
			overlapBuilder.WriteString("--- [ä¸Šä¸‹æ–‡å¼€å§‹] ---\n")
			for i, line := range overlapLinesContent {
				lineNum := overlapStartIdx + i + 1
				overlapBuilder.WriteString(fmt.Sprintf("%d | %s\n", lineNum, line))
			}
			overlapBuilder.WriteString("--- [ä¸Šä¸‹æ–‡ç»“æŸ] ---\n\n")
			chunkWithOverlap = overlapBuilder.String() + chunkContent
		} else {
			chunkWithOverlap = chunkContent
		}

		// æ‰“å° chunk å†…å®¹æ‘˜è¦æ—¥å¿—
		chunkPreview := utils.ShrinkString(chunkContent, 300)
		log.Infof("compressKnowledgeResultsChunked: chunk %d/%d (lines %d-%d, %d lines, size=%d bytes):\n%s",
			chunkIndex+1, maxChunks, startLine, endLine, len(chunkLines), len(chunkContent), chunkPreview)

		// å¯¹å½“å‰ chunk è¿›è¡Œ AI ç­›é€‰
		chunkRanges := r.compressKnowledgeChunk(ctx, chunkWithOverlap, "", userQuery, maxRanges/2+1, startLine, endLine)

		if len(chunkRanges) > 0 {
			allChunkResults = append(allChunkResults, ChunkResult{
				ChunkIndex: chunkIndex,
				StartLine:  startLine,
				EndLine:    endLine,
				Ranges:     chunkRanges,
			})
			log.Infof("compressKnowledgeResultsChunked: chunk %d extracted %d ranges", chunkIndex+1, len(chunkRanges))
		} else {
			log.Infof("compressKnowledgeResultsChunked: chunk %d extracted 0 ranges", chunkIndex+1)
		}

		// ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª chunkï¼ˆå‡å» overlap è¡Œæ•°ï¼‰
		startLineIdx = endLineIdx - overlapLines
		if startLineIdx < 0 {
			startLineIdx = 0
		}
		// ç¡®ä¿å‘å‰æ¨è¿›
		if startLineIdx <= (endLineIdx - linesPerChunk) {
			startLineIdx = endLineIdx
		}
	}

	log.Infof("compressKnowledgeResultsChunked: processed %d chunks total", chunkIndex)

	// åˆå¹¶æ‰€æœ‰ chunk çš„ç»“æœ
	var allRanges []RankedRange
	for _, cr := range allChunkResults {
		allRanges = append(allRanges, cr.Ranges...)
	}

	if len(allRanges) == 0 {
		log.Warnf("compressKnowledgeResultsChunked: no valid ranges extracted from any chunk")
		// è¿”å›æˆªæ–­çš„åŸå§‹å†…å®¹
		if len(knowledgeContent) > 50000 {
			return knowledgeContent[:50000] + "\n\n[... å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­ ...]"
		}
		return knowledgeContent
	}

	// æŒ‰ score ä»é«˜åˆ°ä½æ’åºï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰
	sort.Slice(allRanges, func(i, j int) bool {
		return allRanges[i].Score > allRanges[j].Score
	})

	// é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
	if len(allRanges) > maxRanges {
		allRanges = allRanges[:maxRanges]
	}

	// å»é‡ï¼ˆåŸºäºè¡ŒèŒƒå›´é‡å ï¼‰
	allRanges = deduplicateRanges(allRanges)

	// ä»åŸå§‹å†…å®¹ä¸­æå–æœ€ç»ˆç»“æœ
	resultEditor := memedit.NewMemEditor(knowledgeContent)
	var result strings.Builder
	result.WriteString(fmt.Sprintf("ã€AI æ™ºèƒ½ç­›é€‰ã€‘ä» %d å­—èŠ‚å†…å®¹ä¸­æå–çš„ %d ä¸ªæœ€ç›¸å…³çŸ¥è¯†ç‰‡æ®µï¼š\n\n", len(knowledgeContent), len(allRanges)))

	totalExtractedBytes := 0
	maxTotalBytes := 10 * 1024 // 10KB

	for i, item := range allRanges {
		text := resultEditor.GetTextFromPositionInt(item.StartLine, 1, item.EndLine, 1)
		if text == "" {
			continue
		}

		textBytes := len(text)
		if totalExtractedBytes+textBytes > maxTotalBytes {
			result.WriteString(fmt.Sprintf("\n[... å·²è¾¾åˆ° %d å­—èŠ‚é™åˆ¶ï¼Œå‰©ä½™ %d ä¸ªç‰‡æ®µæœªå±•ç¤º ...]\n", maxTotalBytes, len(allRanges)-i))
			break
		}

		result.WriteString(fmt.Sprintf("=== [%d] Score: %.2f (è¡Œ %d-%d) ===\n", i+1, item.Score, item.StartLine, item.EndLine))
		result.WriteString(text)
		result.WriteString("\n\n")

		totalExtractedBytes += textBytes
	}

	finalResult := result.String()

	log.Infof("compressKnowledgeResultsChunked: compressed from %d chars to %d chars (%d bytes), %d ranges from %d chunks",
		len(knowledgeContent), len(finalResult), totalExtractedBytes, len(allRanges), len(allChunkResults))

	return finalResult
}

// RankedRange è¡¨ç¤ºä¸€ä¸ªå¸¦è¯„åˆ†çš„è¡ŒèŒƒå›´
type RankedRange struct {
	Range     string
	StartLine int
	EndLine   int
	Score     float64 // ç›¸å…³æ€§è¯„åˆ†ï¼Œ0.0-1.0ï¼Œè¶Šé«˜è¶Šç›¸å…³
	Text      string
}

// deduplicateRanges å»é™¤é‡å çš„èŒƒå›´
func deduplicateRanges(ranges []RankedRange) []RankedRange {
	if len(ranges) <= 1 {
		return ranges
	}

	var result []RankedRange
	for _, r := range ranges {
		overlaps := false
		for _, existing := range result {
			// æ£€æŸ¥æ˜¯å¦é‡å 
			if r.StartLine <= existing.EndLine && r.EndLine >= existing.StartLine {
				overlaps = true
				break
			}
		}
		if !overlaps {
			result = append(result, r)
		}
	}
	return result
}

// compressKnowledgeChunk å¯¹å•ä¸ª chunk è¿›è¡Œ AI ç­›é€‰
func (r *ReAct) compressKnowledgeChunk(ctx context.Context, chunkContentWithLineNum string, overlapContext string, userQuery string, maxRanges int, chunkStartLine int, chunkEndLine int) []RankedRange {
	dNonce := utils.RandStringBytes(4)
	minLines := 3
	maxLines := 20

	var overlapSection string
	if overlapContext != "" {
		overlapSection = fmt.Sprintf(`<|OVERLAP_CONTEXT_{{ .nonce }}|>
%s
<|OVERLAP_CONTEXT_END_{{ .nonce }}|>

`, overlapContext)
	}

	promptTemplate := `<|USER_QUERY_{{ .nonce }}|>
{{ .userQuery }}
<|USER_QUERY_END_{{ .nonce }}|>

` + overlapSection + `<|KNOWLEDGE_CHUNK_{{ .nonce }}|>
å½“å‰å¤„ç†åˆ†ç‰‡: è¡Œ {{ .chunkStart }} - {{ .chunkEnd }}
{{ .samples }}
<|KNOWLEDGE_CHUNK_END_{{ .nonce }}|>

<|INSTRUCT_{{ .nonce }}|>
ã€æ™ºèƒ½çŸ¥è¯†ç­›é€‰ã€‘è¯·ä»å½“å‰åˆ†ç‰‡ä¸­æå–ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„çŸ¥è¯†ç‰‡æ®µã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
ä»ä¸Šè¿°å¸¦è¡Œå·çš„çŸ¥è¯†å†…å®¹ä¸­ï¼Œæå–ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³çš„ç‰‡æ®µã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
1. æœ€å¤šæå– %d ä¸ªç‰‡æ®µ
2. æ¯ä¸ªç‰‡æ®µ %d-%d è¡Œ
3. ä½¿ç”¨åŸå§‹è¡Œå·ï¼ˆç¬¬ä¸€åˆ—æ•°å­—ï¼‰
4. ç»™å‡º 0.0-1.0 çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆscoreï¼‰ï¼Œè¶Šé«˜è¶Šç›¸å…³

ã€è¯„åˆ†æ ‡å‡†ã€‘
- 0.8-1.0: ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒå†…å®¹
- 0.6-0.8: ç›¸å…³èƒŒæ™¯/æŠ€æœ¯ç»†èŠ‚
- 0.4-0.6: è¡¥å……æ€§ä¿¡æ¯
- 0.0-0.4: å¼±ç›¸å…³æˆ–æ— å…³å†…å®¹ï¼ˆä¸å»ºè®®è¾“å‡ºï¼‰

è¯·è¾“å‡º ranges æ•°ç»„ã€‚
<|INSTRUCT_END_{{ .nonce }}|>
`

	materials, err := utils.RenderTemplate(fmt.Sprintf(promptTemplate, maxRanges, minLines, maxLines), map[string]any{
		"nonce":      dNonce,
		"samples":    chunkContentWithLineNum,
		"userQuery":  userQuery,
		"chunkStart": chunkStartLine,
		"chunkEnd":   chunkEndLine,
	})

	if err != nil {
		log.Errorf("compressKnowledgeChunk: template render failed: %v", err)
		return nil
	}

	// Create pipe for streaming output
	pr, pw := utils.NewPipe()

	// Get task index for emit
	var taskIndex string
	if r.GetCurrentTask() != nil {
		taskIndex = r.GetCurrentTask().GetIndex()
	}

	// Start streaming output with unified nodeId
	r.Emitter.EmitDefaultStreamEvent(
		"knowledge-compress",
		pr,
		taskIndex,
	)

	// Create LiteForge instance
	liteForgeIns, err := aiforge.NewLiteForge(
		"knowledge-compress",
		aiforge.WithLiteForge_Emitter(r.Emitter),
		aiforge.WithLiteForge_OutputSchema(
			aitool.WithStructArrayParam(
				"ranges",
				[]aitool.PropertyOption{
					aitool.WithParam_Description("æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºçš„çŸ¥è¯†ç‰‡æ®µèŒƒå›´æ•°ç»„"),
				},
				nil,
				aitool.WithStringParam("range", aitool.WithParam_Description("åŸå§‹è¡ŒèŒƒå›´ï¼Œæ ¼å¼: start-end")),
				aitool.WithNumberParam("score", aitool.WithParam_Description("ç›¸å…³æ€§è¯„åˆ†ï¼Œ0.0-1.0ï¼Œè¶Šé«˜è¶Šç›¸å…³")),
			),
		),
		aiforge.WithExtendLiteForge_AIOption(
			aicommon.WithContext(ctx),
		),
	)
	if err != nil {
		log.Errorf("compressKnowledgeChunk: NewLiteForge failed: %v", err)
		pw.Close()
		return nil
	}

	forgeResult, err := liteForgeIns.Execute(ctx, []*ypb.ExecParamItem{
		{Key: "query", Value: materials},
	})

	if err != nil {
		log.Errorf("compressKnowledgeChunk: LiteForge.Execute failed: %v", err)
		pw.Close()
		return nil
	}

	if forgeResult == nil || forgeResult.Action == nil {
		pw.Close()
		return nil
	}

	rangeItems := forgeResult.Action.GetInvokeParamsArray("ranges")
	var results []RankedRange

	for _, item := range rangeItems {
		rangeStr := item.GetString("range")
		score := item.GetFloat("score")

		if rangeStr == "" {
			continue
		}

		// Filter out low score items (< 0.4)
		if score < 0.4 {
			continue
		}

		parts := strings.Split(rangeStr, "-")
		if len(parts) != 2 {
			continue
		}

		startLine, err1 := strconv.Atoi(strings.TrimSpace(parts[0]))
		endLine, err2 := strconv.Atoi(strings.TrimSpace(parts[1]))

		if err1 != nil || err2 != nil || startLine <= 0 || endLine < startLine {
			continue
		}

		// Write to stream: ç‰‡æ®µï¼š[Score: 0.x] startLine-endLine
		pw.WriteString(fmt.Sprintf("ç‰‡æ®µï¼š[Score: %.2f] %d-%d\n", score, startLine, endLine))

		results = append(results, RankedRange{
			Range:     rangeStr,
			StartLine: startLine,
			EndLine:   endLine,
			Score:     score,
		})
	}

	pw.Close()
	return results
}

// compressKnowledgeResultsSingle å¯¹è¾ƒå°çš„å†…å®¹ç›´æ¥è¿›è¡Œå‹ç¼©ï¼ˆä¸åˆ†ç‰‡ï¼‰
func (r *ReAct) compressKnowledgeResultsSingle(ctx context.Context, knowledgeContent string, userQuery string, maxRanges int) string {
	resultEditor := memedit.NewMemEditor(knowledgeContent)
	dNonce := utils.RandStringBytes(4)

	minLines := 5
	maxLines := 30

	promptTemplate := `<|USER_QUERY_{{ .nonce }}|>
{{ .userQuery }}
<|USER_QUERY_END_{{ .nonce }}|>

<|KNOWLEDGE_RESULTS_{{ .nonce }}|>
{{ .samples }}
<|KNOWLEDGE_RESULTS_END_{{ .nonce }}|>

<|INSTRUCT_{{ .nonce }}|>
ã€æ™ºèƒ½çŸ¥è¯†ç­›é€‰ä¸æ’åºã€‘

è¯·ä¸¥æ ¼æ ¹æ®ç”¨æˆ·é—®é¢˜ä»ä¸Šè¿°çŸ¥è¯†æœç´¢ç»“æœä¸­æå–æœ€æœ‰ä»·å€¼çš„çŸ¥è¯†ç‰‡æ®µï¼ŒæŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºï¼š

ã€æ ¸å¿ƒåŸåˆ™ã€‘
- å¿…é¡»ä¸ç”¨æˆ·é—®é¢˜ç›´æ¥ç›¸å…³
- è¿‡æ»¤æ‰æ‰€æœ‰æ— å…³çš„çŸ¥è¯†ç‰‡æ®µ
- ä¼˜å…ˆé€‰æ‹©èƒ½ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†
- ä¿ç•™å®Œæ•´çš„çŸ¥è¯†æ¡ç›®ï¼Œé¿å…æˆªæ–­

ã€æå–è¦æ±‚ã€‘
1. æœ€å¤šæå– %d ä¸ªçŸ¥è¯†ç‰‡æ®µ
2. æ¯ä¸ªç‰‡æ®µ %d-%d è¡Œï¼Œç¡®ä¿ä¸Šä¸‹æ–‡å®Œæ•´
3. ç»™å‡º 0.0-1.0 çš„ç›¸å…³æ€§è¯„åˆ†ï¼ˆscoreï¼‰ï¼Œè¶Šé«˜è¶Šç›¸å…³
4. ä¸¥æ ¼è¿‡æ»¤ä¸ç”¨æˆ·é—®é¢˜æ— å…³çš„çŸ¥è¯†

ã€è¯„åˆ†æ ‡å‡†ã€‘
ğŸ”¥ é«˜åº¦ç›¸å…³ (0.8-1.0)ï¼š
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„çŸ¥è¯†
- åŒ…å«ç”¨æˆ·é—®é¢˜ä¸­æåˆ°çš„å…³é”®å®ä½“/æ¦‚å¿µ
- æä¾›å…·ä½“è§£å†³æ–¹æ¡ˆæˆ–æ“ä½œæ­¥éª¤

â­ è¾ƒé«˜ç›¸å…³ (0.6-0.8)ï¼š
- ä¸ç”¨æˆ·é—®é¢˜é¢†åŸŸç›¸å…³çš„çŸ¥è¯†
- æä¾›èƒŒæ™¯ä¿¡æ¯æˆ–ç›¸å…³æ¦‚å¿µè§£é‡Š
- åŒ…å«ç›¸å…³çš„æŠ€æœ¯ç»†èŠ‚æˆ–é…ç½®

ğŸ“ ä¸€èˆ¬ç›¸å…³ (0.4-0.6)ï¼š
- å¯èƒ½å¯¹ç†è§£é—®é¢˜æœ‰å¸®åŠ©çš„çŸ¥è¯†
- æä¾›è¡¥å……æ€§ä¿¡æ¯
- ç›¸å…³ä½†ä¸ç›´æ¥å›ç­”é—®é¢˜

âŒ å¼±ç›¸å…³ (0.0-0.4)ï¼šä¸å»ºè®®è¾“å‡º

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
{
  "range": "start-end", 
  "score": 0.0-1.0çš„å°æ•°
}

ã€ä¸¥æ ¼è¦æ±‚ã€‘
- æ€»å†…å®¹æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…
- é¿å…é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„çŸ¥è¯†ç‰‡æ®µ
- ä¼˜å…ˆé€‰æ‹©ä¿¡æ¯å¯†åº¦é«˜çš„çŸ¥è¯†
- ç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½å¯¹å›ç­”ç”¨æˆ·é—®é¢˜æœ‰ä»·å€¼

è¯·æŒ‰ç›¸å…³æ€§è¯„åˆ†ä»é«˜åˆ°ä½è¾“å‡ºrangesæ•°ç»„ã€‚
<|INSTRUCT_END_{{ .nonce }}|>
`

	materials, err := utils.RenderTemplate(fmt.Sprintf(promptTemplate, maxRanges, minLines, maxLines), map[string]any{
		"nonce":     dNonce,
		"samples":   utils.PrefixLinesWithLineNumbers(knowledgeContent),
		"userQuery": userQuery,
	})

	if err != nil {
		log.Errorf("compressKnowledgeResultsSingle: template render failed: %v", err)
		return knowledgeContent
	}

	// Create pipe for streaming output
	pr, pw := utils.NewPipe()

	// Get task index for emit
	var taskIndex string
	if r.GetCurrentTask() != nil {
		taskIndex = r.GetCurrentTask().GetIndex()
	}

	// Start streaming output with unified nodeId
	r.Emitter.EmitDefaultStreamEvent(
		"knowledge-compress",
		pr,
		taskIndex,
	)

	// Create LiteForge instance
	liteForgeIns, err := aiforge.NewLiteForge(
		"knowledge-compress",
		aiforge.WithLiteForge_Emitter(r.Emitter),
		aiforge.WithLiteForge_OutputSchema(
			aitool.WithStructArrayParam(
				"ranges",
				[]aitool.PropertyOption{
					aitool.WithParam_Description("æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºçš„çŸ¥è¯†ç‰‡æ®µèŒƒå›´æ•°ç»„"),
				},
				nil,
				aitool.WithStringParam("range", aitool.WithParam_Description("è¡ŒèŒƒå›´ï¼Œæ ¼å¼: start-endï¼Œä¾‹å¦‚ 18-45")),
				aitool.WithNumberParam("score", aitool.WithParam_Description("ç›¸å…³æ€§è¯„åˆ†ï¼Œ0.0-1.0ï¼Œè¶Šé«˜è¶Šç›¸å…³")),
			),
		),
		aiforge.WithExtendLiteForge_AIOption(
			aicommon.WithContext(ctx),
		),
	)
	if err != nil {
		log.Errorf("compressKnowledgeResultsSingle: NewLiteForge failed: %v", err)
		pw.Close()
		return knowledgeContent
	}

	forgeResult, err := liteForgeIns.Execute(ctx, []*ypb.ExecParamItem{
		{Key: "query", Value: materials},
	})

	if err != nil {
		log.Errorf("compressKnowledgeResultsSingle: LiteForge.Execute failed: %v", err)
		pw.Close()
		return knowledgeContent
	}

	if forgeResult == nil || forgeResult.Action == nil {
		log.Warnf("compressKnowledgeResultsSingle: forge result is nil")
		pw.Close()
		return knowledgeContent
	}

	rangeItems := forgeResult.Action.GetInvokeParamsArray("ranges")

	if len(rangeItems) == 0 {
		log.Warnf("compressKnowledgeResultsSingle: no ranges extracted")
		pw.Close()
		return knowledgeContent
	}

	var rankedRanges []RankedRange
	totalBytes := 0
	maxTotalBytes := 10 * 1024 // 10KB

	for _, item := range rangeItems {
		rangeStr := item.GetString("range")
		score := item.GetFloat("score")

		if rangeStr == "" {
			continue
		}

		// Filter out low score items (< 0.4)
		if score < 0.4 {
			continue
		}

		parts := strings.Split(rangeStr, "-")
		if len(parts) != 2 {
			log.Warnf("compressKnowledgeResultsSingle: invalid range format: %s", rangeStr)
			continue
		}

		startLine, err1 := strconv.Atoi(strings.TrimSpace(parts[0]))
		endLine, err2 := strconv.Atoi(strings.TrimSpace(parts[1]))

		if err1 != nil || err2 != nil {
			log.Errorf("compressKnowledgeResultsSingle: parse range failed: %s, errors: %v, %v", rangeStr, err1, err2)
			continue
		}

		if startLine <= 0 || endLine < startLine {
			log.Warnf("compressKnowledgeResultsSingle: invalid range values: %s (start=%d, end=%d)", rangeStr, startLine, endLine)
			continue
		}

		text := resultEditor.GetTextFromPositionInt(startLine, 1, endLine, 1)
		if text == "" {
			log.Warnf("compressKnowledgeResultsSingle: empty text for range: %s", rangeStr)
			continue
		}

		// Write to stream: ç‰‡æ®µï¼š[Score: 0.x] startLine-endLine
		pw.WriteString(fmt.Sprintf("ç‰‡æ®µï¼š[Score: %.2f] %d-%d\n", score, startLine, endLine))

		rankedRanges = append(rankedRanges, RankedRange{
			Range:     rangeStr,
			StartLine: startLine,
			EndLine:   endLine,
			Score:     score,
			Text:      text,
		})

		totalBytes += len(text)
	}

	pw.Close()

	if len(rankedRanges) == 0 {
		log.Warnf("compressKnowledgeResultsSingle: no valid ranges extracted")
		return knowledgeContent
	}

	// Sort by score descending (higher score = more relevant)
	sort.Slice(rankedRanges, func(i, j int) bool {
		return rankedRanges[i].Score > rankedRanges[j].Score
	})

	var result strings.Builder
	result.WriteString("ã€AI æ™ºèƒ½ç­›é€‰ã€‘æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åºçš„çŸ¥è¯†ç‰‡æ®µï¼š\n\n")

	currentBytes := 0
	for i, item := range rankedRanges {
		if currentBytes+len(item.Text) > maxTotalBytes {
			log.Infof("compressKnowledgeResultsSingle: reached %d bytes limit, stopping at %d ranges", maxTotalBytes, i)
			break
		}
		result.WriteString(fmt.Sprintf("=== [%d] Score: %.2f ===\n", i+1, item.Score))
		result.WriteString(item.Text)
		result.WriteString("\n\n")
		currentBytes += len(item.Text)
	}

	finalResult := result.String()

	log.Infof("compressKnowledgeResultsSingle: compressed from %d chars to %d chars (%d bytes), %d ranges extracted",
		len(knowledgeContent), len(finalResult), currentBytes, len(rankedRanges))

	return finalResult
}
