yakit.AutoInitYakit()

targetUrl = cli.String("targetUrl", cli.setHelp("爬虫目标url"),cli.setVerboseName("目标"),cli.setRequired(true))
wsAddress = cli.String("wsAddress", cli.setHelp("chrome headless运行时的ws地址"),cli.setVerboseName("浏览器ws地址"),cli.setCliGroup("浏览器"))
exePath = cli.String("exePath", cli.setHelp("chrome浏览器可执行程序的路径"),cli.setVerboseName("浏览器执行程序路径"),cli.setCliGroup("浏览器"))
proxy = cli.String("proxy", cli.setHelp("代理地址"),cli.setVerboseName("代理地址"),cli.setCliGroup("浏览器"))
proxyUsername = cli.String("proxyUsername", cli.setHelp("代理用户名"),cli.setVerboseName("代理用户名"),cli.setCliGroup("浏览器"))
proxyPassword = cli.String("proxyPassword", cli.setHelp("代理密码"),cli.setVerboseName("代理密码"),cli.setCliGroup("浏览器"))
pageTimeout = cli.Int("pageTimeout", cli.setDefault(30),cli.setHelp("单页面爬虫的最大操作时间，单位秒"),cli.setVerboseName("单页面超时时间"),cli.setCliGroup("爬虫参数"))
fullTimeout = cli.Int("fullTimeout", cli.setDefault(1800),cli.setHelp("整个爬虫运行的超时时间 单位秒"),cli.setVerboseName("全局超时时间"),cli.setCliGroup("爬虫参数"))
formFill = cli.String("formFill", cli.setDefault("username:admin;password:admin"),cli.setHelp("key和value用英文冒号隔开，不同组数据用英文分号隔开"),cli.setVerboseName("表单填写"),cli.setCliGroup("爬虫参数"))
fileUpload = cli.String("fileUpload", cli.setDefault("default:/opt/defaultFile.txt;"),cli.setHelp("key（关键词）和value（文件路径）用英文冒号隔开，不同组数据用英文分号隔开\nkey为default时value为默认上传文件路径"),cli.setVerboseName("文件输入"),cli.setCliGroup("爬虫参数"))
header = cli.String("header", cli.setHelp("header名和header值用英文冒号隔开，不同组数据用英文分号隔开"),cli.setCliGroup("爬虫参数"))
cookie = cli.String("cookie", cli.setHelp("cookie名和cookie值用英文冒号隔开，不同组数据用英文分号隔开"),cli.setCliGroup("爬虫参数"))
scanRange = cli.StringSlice("scanRange", cli.setMultipleSelect(false),cli.setSelectOption("AllDomainScan", "AllDomainScan"),cli.setSelectOption("SubMenuScan", "SubMenuScan"),cli.setVerboseName("扫描范围"),cli.setCliGroup("爬虫参数"),cli.setDefault("AllDomainScan"))
scanRepeat = cli.StringSlice("scanRepeat", cli.setMultipleSelect(false),cli.setSelectOption("ExtremeRepeatLevel", "ExtremeRepeatLevel"),cli.setSelectOption("HighRepeatLevel", "HighRepeatLevel"),cli.setSelectOption("MediumRepeatLevel", "MediumRepeatLevel"),cli.setSelectOption("LowRepeatLevel", "LowRepeatLevel"),cli.setSelectOption("UnLimitRepeat", "UnLimitRepeat"),cli.setVerboseName("url去重级别"),cli.setCliGroup("爬虫参数"),cli.setDefault("LowRepeatLevel"))
maxUrl = cli.Int("maxUrl", cli.setVerboseName("最大url数量"),cli.setCliGroup("爬虫参数"),cli.setDefault(0))
maxDepth = cli.Int("maxDepth", cli.setVerboseName("最大爬虫深度"),cli.setCliGroup("爬虫参数"),cli.setDefault(0))
ignoreQuery = cli.String("ignoreQuery", cli.setHelp("url去重检测时忽略的query-name 以英文逗号隔开"),cli.setVerboseName("忽略参数名"),cli.setCliGroup("爬虫参数"))
extraWaitLoad = cli.Int("extraWaitLoad", cli.setHelp("页面加载的额外等待时间 单位毫秒"),cli.setVerboseName("额外等待时间"),cli.setCliGroup("爬虫参数"),cli.setDefault(0))

blacklist = cli.String("blacklist", cli.setHelp("url黑名单，以英文逗号隔开"),cli.setVerboseName("url黑名单"),cli.setCliGroup("爬虫参数"))
whitelist = cli.String("whitelist", cli.setHelp("url白名单，以英文逗号隔开"),cli.setVerboseName("url白名单"),cli.setCliGroup("爬虫参数"))
sensitiveWords = cli.String("sensitiveWords", cli.setHelp("当设置敏感词时，对应待操作元素innerHTml中存在该词汇则不会进行操作\n不同词之间用英文逗号隔开"),cli.setVerboseName("敏感词"),cli.setCliGroup("爬虫参数"))
leakless = cli.StringSlice("leakless", cli.setMultipleSelect(false),cli.setSelectOption("default", "default"),cli.setSelectOption("true", "true"),cli.setSelectOption("false", "false"),cli.setHelp("浏览器自动进程关闭进行在windows下会报病毒 默认在windows下会关闭\n当关闭时 如果强制关闭爬虫进程时chrome.exe会存在后台 过多时需要手动进行关闭"),cli.setVerboseName("浏览器进程自动关闭"),cli.setCliGroup("其他参数"),cli.setDefault("default"))
concurrent = cli.Int("concurrent", cli.setDefault(3),cli.setVerboseName("浏览器同时打开页面数量"),cli.setCliGroup("爬虫参数"))
rawHeaders = cli.Text("rawHeaders", cli.setHelp("数据包中原始headers文本块"),cli.setCliGroup("爬虫参数"))
rawCookie = cli.Text("rawCookie", cli.setHelp("数据包中原始cookie文本块"),cli.setCliGroup("爬虫参数"))


//用于替换mitm插件参数，动态爬虫接收参数
//开始替换内容标志width_param_list
//待替换内容，勿动！
width_param_list=cli.LineDict("width_param_list",cli.setDefault("captchaWidth\nimgW\nimgWidth\nw\nwidth\nwidthPixels\nwSize"),cli.setVerboseName("图片宽度参数"),cli.setRequired(true))
//结束替换内容标志width_param_list

//开始替换内容标志height_param_list
//待替换内容，勿动！
height_param_list=cli.LineDict("height_param_list",cli.setDefault("captchaHeight\nimgH\nimgHeight\nh\nheight\nheightPixels\nhSize"),cli.setVerboseName("图片高度参数"),cli.setRequired(true))
//结束替换内容标志height_param_list

//开始替换内容标志font_param_list
//待替换内容，勿动！
font_param_list=cli.LineDict("font_param_list",cli.setDefault("font_size\nsize"),cli.setVerboseName("字体大小参数"),cli.setRequired(true))
//结束替换内容标志font_param_list

//开始替换内容标志diff_percent
//待替换内容，勿动！
diff_percent=cli.Float("diff_percent",cli.setDefault(0.3),cli.setVerboseName("响应差值阈值"),cli.setHelp("大于等于该值则告警"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志diff_percent

//开始替换内容标志length_list
//待替换内容，勿动！
length_list=cli.StringSlice("length_list",cli.setDefault("1,10,100,1000,10000"),cli.setMultipleSelect(true),cli.setVerboseName("参数 fuzz 范围"),cli.setHelp("对图片、字体参数的 fuzz 范围"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志length_list

//开始替换内容标志img_type_list
//待替换内容，勿动！
img_type_list=cli.StringSlice("img_type_list",cli.setDefault("image"),cli.setMultipleSelect(true),cli.setVerboseName("响应 Content-Type 类型"),cli.setHelp("只对响应报文中 Content-Type 以该参数开头的报文进行检测"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志img_type_list

//开始替换内容标志param_codec_type
//待替换内容，勿动！
param_codec_type=cli.StringSlice("param_codec_type",cli.setDefault("1"),cli.setVerboseName("参数codec模式"),cli.setHelp("只对请求报文参数生效，按照自定义的codec脚本对可 fuzz 参数进行测试"),cli.setSelectOption("不检测编码", "1"),cli.setSelectOption("检测Url编码", "2"),cli.setSelectOption("检测Base64编码", "3"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志param_codec_type

//开始替换内容标志recognition_patten
//待替换内容，勿动！
recognition_patten=cli.StringSlice("recognition_patten", cli.setVerboseName("识别模式"),cli.setMultipleSelect(false),cli.setSelectOption("字典模式", "1"),cli.setSelectOption("机器学习模式", "2"),cli.setSelectOption("字典模式优先-机器学习辅助", "3"),cli.setSelectOption("字典模式-机器学习并行", "4"),cli.setSelectOption("机器学习优先-字典模式辅助", "5"),cli.setDefault("1"),cli.setHelp("检测识别模式"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志recognition_patten

//开始替换内容标志task_type
//待替换内容，勿动！
task_type=cli.StringSlice("task_type", cli.setVerboseName("任务类型"),cli.setMultipleSelect(false),cli.setSelectOption("即时", "即时"),cli.setSelectOption("定时", "定时"),cli.setSelectOption("周期", "周期"),cli.setDefault("即时"),cli.setHelp("任务类型，mitm都为即时任务"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志task_type

//定时任务延时开始时间
timed_task_start_time=cli.String("timed_task_start_time", cli.setDefault("2024-09-01 10:00:00"),cli.setVerboseName("定时任务开始时间"),cli.setHelp("定时任务开始时间，严格按照模板值填写: 2024-09-01 10:00:00 表示北京时间24年9月1号上午10点"),cli.setCliGroup("插件额外参数"))

//周期任务间隔时间
periodic_tasks_interval_time=cli.Int("periodic_tasks_interval_time", cli.setVerboseName("周期任务间隔时间"),cli.setHelp("周期任务间隔时间，单位s"),cli.setCliGroup("插件额外参数"))


cli.check()



//mitm插件源码
plugin=`


// mitm参数输入 动态爬虫调用mitm插件时候，使用正则字符串替换 plugin 源码进行传参，需要给参数注释进行正则匹配

//开始替换内容标志width_param_list
//待替换内容，勿动！
width_param_list=cli.LineDict("width_param_list",cli.setDefault("captchaWidth\nimgW\nimgWidth\nw\nwidth\nwidthPixels\nwSize"),cli.setVerboseName("图片宽度参数"),cli.setRequired(true))
//结束替换内容标志width_param_list

//开始替换内容标志height_param_list
//待替换内容，勿动！
height_param_list=cli.LineDict("height_param_list",cli.setDefault("captchaHeight\nimgH\nimgHeight\nh\nheight\nheightPixels\nhSize"),cli.setVerboseName("图片高度参数"),cli.setRequired(true))
//结束替换内容标志height_param_list

//开始替换内容标志font_param_list
//待替换内容，勿动！
font_param_list=cli.LineDict("font_param_list",cli.setDefault("font_size\nsize"),cli.setVerboseName("字体大小参数"),cli.setRequired(true))
//结束替换内容标志font_param_list

//开始替换内容标志diff_percent
//待替换内容，勿动！
diff_percent=cli.Float("diff_percent",cli.setDefault(0.3),cli.setVerboseName("响应差值阈值"),cli.setHelp("大于等于该值则告警"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志diff_percent

//开始替换内容标志length_list
//待替换内容，勿动！
length_list=cli.StringSlice("length_list",cli.setDefault("1,10,100,1000,10000"),cli.setMultipleSelect(true),cli.setVerboseName("参数 fuzz 范围"),cli.setHelp("对图片、字体参数的 fuzz 范围"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志length_list

//开始替换内容标志img_type_list
//待替换内容，勿动！
img_type_list=cli.StringSlice("img_type_list",cli.setDefault("image"),cli.setMultipleSelect(true),cli.setVerboseName("响应 Content-Type 类型"),cli.setHelp("只对响应报文中 Content-Type 以该参数开头的报文进行检测"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志img_type_list

//开始替换内容标志param_codec_type
//待替换内容，勿动！
param_codec_type=cli.StringSlice("param_codec_type",cli.setDefault("1"),cli.setVerboseName("参数codec模式"),cli.setHelp("只对请求报文参数生效，按照自定义的codec脚本对可 fuzz 参数进行测试"),cli.setSelectOption("不检测编码", "1"),cli.setSelectOption("检测Url编码", "2"),cli.setSelectOption("检测Base64编码", "3"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志param_codec_type

//开始替换内容标志recognition_patten
//待替换内容，勿动！
recognition_patten=cli.StringSlice("recognition_patten", cli.setVerboseName("识别模式"),cli.setMultipleSelect(false),cli.setSelectOption("字典模式", "1"),cli.setSelectOption("机器学习模式", "2"),cli.setSelectOption("字典模式优先-机器学习辅助", "3"),cli.setSelectOption("字典模式-机器学习并行", "4"),cli.setSelectOption("机器学习优先-字典模式辅助", "5"),cli.setDefault("1"),cli.setHelp("检测识别模式"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志recognition_patten

//开始替换内容标志task_type
//待替换内容，勿动！
task_type=cli.StringSlice("task_type", cli.setVerboseName("任务类型"),cli.setMultipleSelect(false),cli.setSelectOption("即时", "即时"),cli.setDefault("即时"),cli.setHelp("任务类型，mitm都为即时任务"),cli.setCliGroup("插件额外参数"))
//结束替换内容标志task_type


// 定义全局变量
recognition_patten_dic={
    "1":"字典模式",
    "2":"机器学习模式",
    "3":"字典模式优先-机器学习辅助",
    "4":"字典模式-机器学习并行",
    "5":"机器学习优先-字典模式辅助",
}

param_codec_dic={
    "1":"不检测编码",
    "2":"Url编码",
    "3":"Base64编码",
}




// 定义用于打印的 base_info 变量
base_info_script_name="验证码 DoS 检测"
base_info_task_creater=""
base_info_task_type=task_type[0]
base_info_detect_mode="动态爬虫"
base_info_task_mode=recognition_patten_dic[recognition_patten[0]]
base_info_login=""
base_info_crediential=""
base_info_user_define_header=""
base_info_start_time=time.Now().String()
base_info_end_time=""


// 定义打印函数
base_info_output=func(){
    yakit_output(f"加载插件结束。配置的基本信息如下：任务名称为'${base_info_script_name}'、任务创建人为'${base_info_task_creater}'、任务类型为'${base_info_task_type}任务'、检测模式为'${base_info_detect_mode}模式'、任务模式为'${base_info_task_mode}'、登录入口信息为'${base_info_login}'、凭证信息为'${base_info_crediential}'、自定义请求头信息为'${base_info_user_define_header}'、任务开始时间为 ${base_info_start_time}、结束时间为${base_info_end_time}。")
}
base_info_output()


cli.check()


solution="1.实现验证码频率限制：限制每个用户请求验证码的频率，防止攻击者发送大量恶意请求。可以设置每个用户在一定时间内只能请求验证码的次数。\n2.监控异常请求：实施安全审计和监控机制，监视系统中的异常请求模式。及时检测到异常行为并采取相应措施来应对。\n3.定期更新验证码生成算法：定期更新验证码生成算法，确保验证码的安全性和抗攻击能力。"

description="验证码 DoS（拒绝服务）漏洞是一种安全漏洞，攻击者利用该漏洞可以通过恶意请求使验证码系统不可用或性能严重下降。这种攻击可能导致系统无法正常响应合法用户的请求，从而影响系统的可用性。"

risk_output_dic={}
detect_output_dic={}
flow_total=-1



invokeCodec = (name,arg)=>{
    if name=="不检测编码"{
        return arg
    }
    plugin = db.GetYakitPluginByName(name)~
    if plugin.Type!="codec"{
        return ""
    }
    code = plugin.Content
    file.TempFileName()
    name, err = file.TempFileName()
    if err != nil{
        return ""
    }
    defer os.Remove(name)
    file.Save(name, code)
    hanlde = import(name, "handle")~
    return hanlde(arg)
}


gen_dos_fuzz_freq=func(freq,length_value){
    freq_finall,param,payload=freq,"",invokeCodec(param_codec_dic[param_codec_type[0]],length_value)
    // 不获取报文中的参数，而是对设置的参数列表强制fuzz，无需检测原始参数编码类型

    for width_param in width_param_list{
        freq_finall=freq_finall.FuzzGetParams(width_param, payload).FuzzPostParams(width_param,payload )
        param=param+"|"+width_param
    }

    for height_param in height_param_list{
        freq_finall=freq_finall.FuzzGetParams(height_param, payload).FuzzPostParams(height_param,payload )
        param=param+"|"+height_param
    }

    for font_param in font_param_list{
        freq_finall=freq_finall.FuzzGetParams(font_param,payload).FuzzPostParams(font_param,payload )
        param=param+"|"+font_param
    }

    return freq_finall,param[1:]
}


dos_check=func(freq_finall,raw_content_length){

    // dos 可能时间增加，设置等待30s
    result=freq_finall.ExecFirst(fuzz.WithTimeOut(30))~
    fuzz_http_rsp_header=str.ParseBytesToHTTPResponse(result.ResponseRaw)~
    // 检测响应时间会有误报 考虑检测响应content-length ,默认 fuzz 后的报文长度和原始长度差值大于原始长度的 30% 时，便认为存在漏洞
    fuzz_content_length=int(fuzz_http_rsp_header.Header["Content-Length"][0])
    diff_value=fuzz_content_length-raw_content_length
    if diff_value>0 && (diff_value*1.0/raw_content_length)>=diff_percent{
        yakit_output(f"检测到 fuzz 后的响应差值大于预设值的阈值 ${diff_percent}")
        return true,result,fuzz_content_length
    }
    return false,result,fuzz_content_length
}


mitm_risk_output=func(url,req,rsp,vul_type,vul_name,vul_severity,param,payload,payload_success_flag,pocname){
    // 考虑去重策略 即时/定时 任务去重（因为任务本身也只跑一次），周期任务不去重
    if task_type[0]=="即时" || task_type[0]=="定时" {
        // 计算url和string后的body md5进行去重
        header_md5_check,body_md5_check=poc.Split(req)
        url_body_hash=codec.Md5(url+string(body_md5_check))
        if url_body_hash in risk_output_dic{
            //该条流已经告警输出，不再进行告警
            return
        }else{
            //记录到告警去重字典中
            risk_output_dic[url_body_hash]=1
        }

        //记录到检测去重字典中,借助库函数，拼接root_url和path
        detect_root_url=str.ParseStringUrlToWebsiteRootPath(url)
        detect_url_Instance,err=str.ParseStringUrlToUrlInstance(url)
        if err!=nil{
            die(err)
        }
        detect_path=detect_url_Instance.Path
        detect_url_path_hash=codec.Md5(detect_root_url+detect_path)
        if detect_url_path_hash in detect_output_dic{
            //该条流已经检测，后续 mirrorFilteredHTTPFlow 不再进行检测
        }else{
            //记录到检测去重字典中
            detect_output_dic[detect_url_path_hash]=1
        }
    }

    risk.NewRisk(
        url,
        risk.title(f"发现 ${url} 中存在 ${vul_name} 漏洞"),
        risk.type(vul_type),
        risk.severity(vul_severity),
        risk.request(string(req)),
        risk.response(string(rsp)),
        risk.payload(payload),
        risk.solution(solution),
        risk.description(description),
        risk.parameter(param),
        risk.details({
            "location":url,
            "pocname":pocname,
            "payload_success_flag":f"Content-Length: ${payload_success_flag}",
            "recognition_patten":recognition_patten_dic[recognition_patten[0]]
        })
        )
}


# mirrorHTTPFlow 会镜像所有的流量到这里，包括 .js / .css / .jpg 这类一般会被劫持程序过滤的请求
mirrorHTTPFlow = func(isHttps /*bool*/, url /*string*/, req /*[]byte*/, rsp /*[]byte*/, body /*[]byte*/) {
    // 手动对爬虫预检测流量进行过滤
    // if flow_total==-1{
    //     flow_total=flow_total+1
    //     return
    // }

    detect_root_url=str.ParseStringUrlToWebsiteRootPath(url)
    detect_url_Instance,err=str.ParseStringUrlToUrlInstance(url)
    if err!=nil{
        yakit_output("url 解析错误，脚本即将退出")
        die(err)
    }

    detect_path=detect_url_Instance.Path
    detect_url_path_hash=codec.Md5(detect_root_url+detect_path)
    // 考虑去重策略 即时/定时 任务去重（因为任务本身也只跑一次），周期任务不去重
    if task_type[0]=="即时" || task_type[0]=="定时"{
        if detect_url_path_hash in detect_output_dic{
            //该条流已经检测并告警，不再进行检测
            yakit_output("detect_output_dic 去重")
            return
        }

        // 计算url和string后的body md5进行去重
        header_md5_check,body_md5_check=poc.Split(req)
        url_body_hash=codec.Md5(url+string(body_md5_check))
        if url_body_hash in risk_output_dic{
            //该条流已经告警输出，不再进行检测
            yakit_output("risk_output_dic 去重")
            return
        }
    }

    //过滤出图片类型的响应
    http_rsp=str.ParseBytesToHTTPResponse(rsp)~
    if ("Content-Type" in http_rsp.Header) && (str.MatchAnyOfSubString(http_rsp.Header["Content-Type"], img_type_list...)) {
        yakit_output(f"检测到响应报文为图片类型，开始尝试对 ${url} 进行验证码 DoS 检测")
    }else{
        return
    }

    //生成freq
    raw_content_length,freq=int(http_rsp.Header["Content-Length"][0]),fuzz.HTTPRequest(req,fuzz.https(isHttps))~
    // 有参数场景下 容易 fuzz 无必要参数，考虑后续补充参数字典形式优化 pass
    // 无参数场景下 直接 fuzz 参数 只考虑 get/post 两种形式 ，且参数字典也可覆盖有参数场景
    for length_value in length_list{
        freq_finall,param=gen_dos_fuzz_freq(freq, length_value)
        dos_check_res,result,payload_success_flag=dos_check(freq_finall,raw_content_length)
        if dos_check_res{
            mitm_risk_output(result.Url, result.RequestRaw, result.ResponseRaw, "逻辑漏洞", "验证码DoS", "middle", param, invokeCodec(param_codec_dic[param_codec_type[0]],length_value), payload_success_flag, "mitm_verification_code_DoS_check")
            yakit_output(f"检测到 ${url} 中存在验证码 DoS 漏洞!")
            break
        }
    }
    yakit_output(f"完成对 ${url} 的验证码 DoS 检测")
}
`
// 第一步 处理被动流量插件，替换参数

re_rule_template=`//开始替换内容标志%s
//待替换内容，勿动！
(.*?)
//结束替换内容标志%s`

// 该方法只适用于cli参数返回为 (适配param_object参数类型 []string /float64 可自定义扩展替换类型)
replace_plugin_handle=func(plugin,replace_param_name,param_object){
    re_rule=re_rule_template % [replace_param_name,replace_param_name]
    if typeof(param_object)==float64 && param_object!=nil{
        param_object_str=param_object
        // yakit.Info(f`设置 ${replace_param_name} 参数为 "${param_object_str}"`)
        plugin=re.ReplaceAll(plugin, re_rule /*type: string*/, f`${replace_param_name}=${param_object_str}`)
    }

    if typeof(param_object)==[]string && param_object!=nil{
        param_object_str=`","`.Join(param_object)
        // yakit.Info(f`设置 ${replace_param_name} 参数为 "${param_object_str}"`)
        plugin=re.ReplaceAll(plugin, re_rule /*type: string*/, f`${replace_param_name}=["${param_object_str}"]`)
    }

    return plugin
}

//依次替换
plugin=replace_plugin_handle(plugin, "width_param_list",width_param_list)
plugin=replace_plugin_handle(plugin, "heigth_param_list",height_param_list)
plugin=replace_plugin_handle(plugin, "font_param_list",font_param_list)
plugin=replace_plugin_handle(plugin, "diff_percent",diff_percent)
plugin=replace_plugin_handle(plugin, "length_list",length_list)
plugin=replace_plugin_handle(plugin, "img_type_list",img_type_list)
plugin=replace_plugin_handle(plugin, "param_codec_type",param_codec_type)
plugin=replace_plugin_handle(plugin, "recognition_patten",recognition_patten)
plugin=replace_plugin_handle(plugin, "task_type",task_type)

manager, err = hook.NewMixPluginCaller()
startMitm = func(port){
    println("创建manager")
    if err != nil {
        println("build mix plugin caller failed: %s", err)
        die(err)
    }

    manager.LoadPluginByName(context.Background(), uuid(), nil, plugin)

    println("启动mitm")
    mitm.Start(port, mitm.callback(
        func(isHttps,url,req,rsp){
            req,err = http.dump(req)
            if err!= nil{
                yakit.Info(err.Error())
                return
            }
            rsp,err = http.dump(rsp)
            if err!= nil{
                yakit.Info(err.Error())
                return
            }
            body,err = str.ExtractBodyFromHTTPResponseRaw(rsp)
            if err!= nil{
                yakit.Info(err.Error())
                return
            }

            defer func{
                err = recover()
                if err != nil {
                    yakit.Info("handle result met error: %s", err)
                }
            }
            try {
                println("调用插件")
                manager.MirrorHTTPFlowEx(false,isHttps,url,req,rsp,body)
            } catch err {
                yakit.Info(err.Error())
            }
        },
    ))
}

// 第二步 启动被动流量服务
randomPort = randn(10000, 60000)
go startMitm(randomPort)
sleep(5)
proxy = "http://127.0.0.1:" + f`${randomPort}`

func stringToDict(tempStr) {
    result = make(map[string]string, 0)
    items = tempStr.Split(";")
    for _, item := range items {
        if item.Contains(":") {
            kv := item.Split(":")
            result[kv[0]] = kv[1]
        }
    }
    return result
}

host = ""
if targetUrl.Contains("://") {
    host = targetUrl.Split("://")[1]
} else {
    host = targetUrl
}
host = host.Split("/")[0]

scanRangeMap = {
    "AllDomainScan": crawlerx.AllDomainScan,
    "SubMenuScan": crawlerx.SubMenuScan,
}

scanRepeatMap = {
    "UnLimitRepeat": crawlerx.UnLimitRepeat,
    "LowRepeatLevel": crawlerx.LowRepeatLevel,
    "MediumRepeatLevel": crawlerx.MediumRepeatLevel,
    "HighRepeatLevel": crawlerx.HighRepeatLevel,
    "ExtremeRepeatLevel": crawlerx.ExtremeRepeatLevel,
}

browserInfo = {
    "ws_address":"",
    // "ws_address":"ws://192.168.0.68:7317",
    "exe_path":"",
    "proxy_address":"",
    "proxy_username":"",
    "proxy_password":"",
}
if wsAddress != "" {
    browserInfo["ws_address"] = wsAddress
}
if exePath != "" {
    browserInfo["exe_path"] = exePath
}
if proxy != "" {
    browserInfo["proxy_address"] = proxy
    if proxyUsername != "" {
        browserInfo["proxy_username"] = proxyUsername
        browserInfo["proxy_password"] = proxyPassword
    }
}
browserInfoOpt = crawlerx.browserInfo(json.dumps(browserInfo))

pageTimeoutOpt = crawlerx.pageTimeout(pageTimeout)

fullTimeoutOpt = crawlerx.fullTimeout(fullTimeout)

concurrentOpt = crawlerx.concurrent(concurrent)

opts = [
    browserInfoOpt,
    pageTimeoutOpt,
    fullTimeoutOpt,
    concurrentOpt,
    crawlerx.sourceType("plugin"),
    crawlerx.fromPlugin("headless_crawlerx"),
    crawlerx.saveToDB(true),
    crawlerx.urlCheck(false),//关闭后不会对url存活进行检测
]

if formFill != "" {
    formFillInfo = stringToDict(formFill)
    formFillOpt = crawlerx.formFill(formFillInfo)
    opts = append(opts, formFillOpt)
}

if fileUpload != "" {
    fileUploadInfo = stringToDict(fileUpload)
    fileUploadOpt = crawlerx.fileInput(fileUploadInfo)
    opts = append(opts, fileUploadOpt)
}

if header != "" {
    headerInfo = stringToDict(header)
    headerOpt = crawlerx.headers(headerInfo)
    opts = append(opts, headerOpt)
}

if rawHeaders != "" {
    opts = append(opts, crawlerx.rawHeaders(rawHeaders))
}

if rawCookie != "" {
    opts = append(opts, crawlerx.rawCookie(host, rawCookie))
}

if cookie != "" {
    cookieInfo = stringToDict(cookie)
    cookieOpt = crawlerx.cookies(host, cookieInfo)
    opts = append(opts, cookieOpt)
}

scanRangeStr = scanRange[0]
if scanRangeStr != "" {
    scanRangeItem = scanRangeMap[scanRangeStr]
    scanRangeOpt = crawlerx.scanRangeLevel(scanRangeItem)
    opts = append(opts, scanRangeOpt)
}

scanRepeatStr = scanRepeat[0]
if scanRepeatStr != "" {
    scanRepeatItem = scanRepeatMap[scanRepeatStr]
    scanRepeatOpt = crawlerx.scanRepeatLevel(scanRepeatItem)
    opts = append(opts, scanRepeatOpt)
}

if maxUrl != 0 {
    opts = append(opts, crawlerx.maxUrl(maxUrl))
}

if maxDepth != 0 {
    opts = append(opts, crawlerx.maxDepth(maxDepth))
}

if extraWaitLoad != 0 {
    opts = append(opts, crawlerx.extraWaitLoadTime(extraWaitLoad))
}

if ignoreQuery != "" {
    opts = append(opts, crawlerx.ignoreQueryName(ignoreQuery.Split(",")...))
}

if blacklist != "" {
    opts = append(opts, crawlerx.blacklist(blacklist.Split(",")...))
}

if whitelist != "" {
    opts = append(opts, crawlerx.whitelist(whitelist.Split(",")...))
}

if sensitiveWords != "" {
    opts = append(opts, crawlerx.sensitiveWords(sensitiveWords.Split(",")))
}

leaklessStr = leakless[0]
if leaklessStr != "" {
    opts = append(opts, crawlerx.leakless(leaklessStr))
}

//todo适配周期/定时任务，对爬虫进行控制

if task_type[0]=="即时"{
    ch = crawlerx.StartCrawler(targetUrl, opts...)~
    for item = range ch{
        yakit.Info(item.Method() + " " + item.Url())
    }
}elif task_type[0]=="定时"{
    // 使用延时函数
    // 返回 UTC 时间
    timed_task_start_time_obj=time.Parse("2006-01-02 15:04:05", timed_task_start_time /*type: string*/)~
    // 计算到 CST 间隔，差值8小时
    sleep_time=time.Since(timed_task_start_time_obj).Seconds()+3600*8
    if sleep_time>0 {
        yakit.Info(f"定时任务时间不合法")
    }else{
        yakit.Info(f"定时任务将于 ${timed_task_start_time} 开始执行，请耐心等待")
        sleep(math.Abs(sleep_time))
        ch = crawlerx.StartCrawler(targetUrl, opts...)~
        for item = range ch{
            yakit.Info(item.Method() + " " + item.Url())
        }
    }
}elif task_type[0]=="周期"{
    // 使用延时函数 避免无休止执行 先执行10次
    for i in 10 {
        yakit.Info(f"开始第 ${i+1} 次周期任务")
        ch = crawlerx.StartCrawler(targetUrl, opts...)~
        for item = range ch{
            yakit.Info(item.Method() + " " + item.Url())
        }
        sleep(periodic_tasks_interval_time)
    }
}


