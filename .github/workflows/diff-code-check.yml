name: Diff-Code-Check
on:
  workflow_run:
    workflows: [ "Essential Tests" ]
    types:
      - completed
  pull_request:
    branches: [main]
    types: [ opened, synchronize, reopened ]
    paths:
      - "go.mod"
      - ".github/workflows/diff-code-check.yml"
      - ".github/workflows/security-comment-simple.yml"
      - ".github/actions/security-commenter/action.yml"
      - "common/ssa_bootstrapping/ci_rule/**"
      - "common/syntaxflow/sfbuildin/buildin/golang/**"
      - "scripts/ssa-risk-tools/**"

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  setup:
    runs-on: ubuntu-22.04
    if: ${{ (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success') || (github.event_name == 'pull_request') }}
    permissions:
      contents: read
      pull-requests: write
      issues: write  # TODO: Test removal - may be redundant with pull-requests: write
      actions: read
    steps:
      - name: Initialize workflow context
        id: init
        run: | 
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            HEAD_SHA="${{ github.event.workflow_run.head_sha }}"
            if [ "${{ github.event.workflow_run.pull_requests }}" != "[]" ] && [ "${{ github.event.workflow_run.pull_requests }}" != "null" ]; then
              PR_NUMBER="${{ github.event.workflow_run.pull_requests[0].number }}"
            else
              PR_NUMBER="0"
            fi
          elif [ "${{ github.event_name }}" == "pull_request" ]; then
            HEAD_SHA="${{ github.event.pull_request.head.sha }}"
            PR_NUMBER="${{ github.event.pull_request.number }}"
          else
            echo "::error::Unsupported event type: ${{ github.event_name }}"
            exit 1
          fi
          
          echo "HEAD_SHA=$HEAD_SHA" >> $GITHUB_ENV
          echo "PR_NUMBER=$PR_NUMBER" >> $GITHUB_ENV

      - name: Set Cache Key
        id: cache_key
        run: |
          CACHE_SHA="${{ env.HEAD_SHA }}"
          echo "sha=$CACHE_SHA" >> $GITHUB_OUTPUT
          echo "Cache key will be: go-$CACHE_SHA"

      - name: Security validation and PR checks
        id: validate
        run: |
          if [ "${{ env.PR_NUMBER }}" == "0" ] || [ "${{ env.PR_NUMBER }}" == "" ]; then
            echo "should_skip=false" >> $GITHUB_OUTPUT
            echo "skip_reason=none" >> $GITHUB_OUTPUT
            exit 0
          fi
          
            PR_RESPONSE=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/pulls/${{ env.PR_NUMBER }}")
            
            HEAD_REPO=$(echo "$PR_RESPONSE" | jq -r '.head.repo.full_name // "unknown"')
            BASE_REPO=$(echo "$PR_RESPONSE" | jq -r '.base.repo.full_name // "unknown"')
            PR_STATE=$(echo "$PR_RESPONSE" | jq -r '.state // "unknown"')
            PR_HEAD_SHA=$(echo "$PR_RESPONSE" | jq -r '.head.sha // "unknown"')
          
          if [ "$HEAD_REPO" != "$BASE_REPO" ] && [ "$HEAD_REPO" != "unknown" ] && [ "$BASE_REPO" != "unknown" ]; then
            echo "::error::ğŸš« SECURITY BLOCK: Fork PR detected (Base: $BASE_REPO, Head: $HEAD_REPO)"
            echo "::error::This workflow only runs for PRs from the same repository."
            exit 1
          fi
          
            if [ "$PR_STATE" != "open" ]; then
              echo "should_skip=true" >> $GITHUB_OUTPUT
              echo "skip_reason=pr_not_open" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            if [ "$PR_HEAD_SHA" != "${{ env.HEAD_SHA }}" ]; then
              echo "should_skip=true" >> $GITHUB_OUTPUT
              echo "skip_reason=sha_mismatch" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "should_skip=false" >> $GITHUB_OUTPUT
            echo "skip_reason=none" >> $GITHUB_OUTPUT

      - name: Skip Workflow (Validation Failed)
        if: steps.validate.outputs.should_skip == 'true'
        run: |
          echo "Skipped: ${{ steps.validate.outputs.skip_reason }}"

      - name: Restore cached YakLang project
        if: steps.validate.outputs.should_skip != 'true'
        uses: actions/cache@v3
        id: cache-project
        with:
          path: |
            ~/yakit-projects
            ~/go/pkg/mod
          key: go-${{ steps.cache_key.outputs.sha }}
          restore-keys: |
            go-

      - name: Clean problematic directories
        if: steps.validate.outputs.should_skip != 'true'
        run: |
          if [ -d "vendor" ]; then
            sudo chmod -R +w vendor/ 2>/dev/null || true
            sudo rm -rf vendor/ || rm -rf vendor/ || true
          fi
          if [ -d ".git/modules" ]; then
            sudo chmod -R +w .git/modules/ 2>/dev/null || true
          fi

      - name: Check out code into the Go module directory
        if: steps.validate.outputs.should_skip != 'true' && steps.cache-project.outputs.cache-hit != 'true'
        uses: actions/checkout@v3
        with:
          ref: ${{ env.HEAD_SHA }}
          fetch-depth: 0
          clean: false

      - name: Fetch Main And Reset Main
        if: steps.validate.outputs.should_skip != 'true' && steps.cache-project.outputs.cache-hit != 'true'
        run: |
          git fetch --all
          git checkout main
          git reset --hard origin/main

      - name: Set up Go 1.x
        if: steps.validate.outputs.should_skip != 'true' && steps.cache-project.outputs.cache-hit != 'true'
        uses: actions/setup-go@v5
        with:
          go-version-file: "./go.mod"
        id: go

      #      - name: Download From oos
      #        run: |
      #          wget https://aliyun-oss.yaklang.com/yak/latest/yak_linux_amd64
      #          chmod +x ./yak_linux_amd64

      - name: Init Module
        if: steps.validate.outputs.should_skip != 'true' && steps.cache-project.outputs.cache-hit != 'true'
        run: |
          go mod tidy && go work vendor
          
          LIBPCAP_PATH=$(go env GOPATH)/pkg/mod/$(go list -m github.com/yaklang/pcap | sed 's/ /@/')/libpcap
          
          if [ -d "$LIBPCAP_PATH" ]; then
            chmod -R +r "$LIBPCAP_PATH" 2>/dev/null || true
            mkdir -p ./vendor/github.com/yaklang/pcap/
            cp -r "$LIBPCAP_PATH" ./vendor/github.com/yaklang/pcap/
            chmod -R +w ./vendor/github.com/yaklang/pcap/libpcap 2>/dev/null || true
          fi
          
          tree ./vendor || ls -la ./vendor

      - name: Init Project
        if: steps.validate.outputs.should_skip != 'true' && steps.cache-project.outputs.cache-hit != 'true'
        env: 
          SKIP_SYNC_EMBED_RULE_IN_GITHUB: "true"
        run: |
          go build common/yak/cmd/yak.go 
          ./yak --help

      - name: Generate Prog
        if: steps.validate.outputs.should_skip != 'true'
        run: |
          git checkout ${{ env.HEAD_SHA }}
          ./yak sf-import --file common/ssa_bootstrapping/ci_rule/ --format raw
          MERGE_BASE=$(git merge-base main ${{ env.HEAD_SHA }})
          ./yak gitefs --start $MERGE_BASE --end ${{ env.HEAD_SHA }} --output ./fs.zip 

      - name: Upload fs.zip
        if: steps.validate.outputs.should_skip != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: fs.zip
          path: fs.zip

      - name: Check With SyntaxFlow
        if: steps.validate.outputs.should_skip != 'true'
        id: scan
        run: |
          ./yak code-scan -t ./fs.zip -l golang --rule-keyword golang --format irify -o risk --memory --log-level debug --exclude-file **/vendor/**,vendor/**,**/classes/**,**/target/**,**include/**,**caches/**,**cache/**,**tmp/**,**alipay/**,**includes/**,**temp/**,**zh_cn/**,**zh_en/**,**plugins/**,**PHPExcel/**,*.pb.go
          NUM=$(cat risk.json | jq .RiskNums)
          echo "risk_count=$NUM" >> $GITHUB_OUTPUT
          if [ $NUM == 0 ]; then
            echo "scan_result=success" >> $GITHUB_OUTPUT
          else
            echo "scan_result=failure" >> $GITHUB_OUTPUT
          fi
        # Get-Content risk.json -Encoding UTF8 | jq .RiskNums

      - name: Setup Python environment
        if: steps.validate.outputs.should_skip != 'true'
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install Python dependencies
        if: steps.validate.outputs.should_skip != 'true'
        run: |
          pip install --quiet --upgrade pip
          pip install --quiet pyyaml requests

      - name: Verify GitHub token permissions
        if: steps.validate.outputs.should_skip != 'true' && env.PR_NUMBER != '0'
        id: test_token
        run: |
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/${{ github.repository }}/pulls/${{ env.PR_NUMBER }}")
          
          if [ "$HTTP_CODE" = "200" ]; then
            echo "has_permissions=true" >> $GITHUB_OUTPUT
          else
            echo "has_permissions=false" >> $GITHUB_OUTPUT
          fi

      - name: Skip Security Comment (Insufficient Permissions)
        if: steps.validate.outputs.should_skip != 'true' && steps.scan.outputs.scan_result == 'failure' && steps.test_token.outputs.has_permissions == 'false'
        run: |
          echo "::warning::Insufficient token permissions, skipping security comments"
      
      - name: Generate security report
        if: steps.validate.outputs.should_skip != 'true' && steps.scan.outputs.scan_result == 'failure'
        run: |
          rm -rf results
          
          if [ ! -f "./risk.json" ]; then
            echo "::error::Risk file not found: ./risk.json"
            exit 1
          fi
          
          if [ ! -f "./scripts/ssa-risk-tools/extract-risks-deserializer.awk" ]; then
            echo "::error::AWK script not found: ./scripts/ssa-risk-tools/extract-risks-deserializer.awk"
            exit 1
          fi
          
          awk -f scripts/ssa-risk-tools/extract-risks-deserializer.awk ./risk.json
          
          if [ -d "results" ]; then
            HIGH_CRITICAL_COUNT=0
            if [ -f "results/scan_summary.txt" ]; then
              if ls results/risk_details_*.txt 1> /dev/null 2>&1; then
                COUNT_RESULT=$(grep -c "ä¸¥é‡ç¨‹åº¦: high\|ä¸¥é‡ç¨‹åº¦: critical" results/risk_details_*.txt 2>/dev/null || echo "0")
                if [[ "$COUNT_RESULT" =~ ^[0-9]+$ ]]; then
                  HIGH_CRITICAL_COUNT=$COUNT_RESULT
                else
                  HIGH_CRITICAL_COUNT=0
                fi
              else
                HIGH_CRITICAL_COUNT=0
              fi
            else
              HIGH_CRITICAL_COUNT=0
            fi
            echo "HIGH_CRITICAL_COUNT=$HIGH_CRITICAL_COUNT" >> $GITHUB_OUTPUT
          else
            echo "::error::Failed to generate results directory"
            exit 1
          fi

      - name: Clean up old security comments (No vulnerabilities found)
        if: steps.validate.outputs.should_skip != 'true' && steps.scan.outputs.scan_result == 'success' && steps.test_token.outputs.has_permissions == 'true' && env.PR_NUMBER != '' && env.PR_NUMBER != '0'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            try {
              // å®šä¹‰å®‰å…¨æ‰«æè¯„è®ºçš„å”¯ä¸€æ ‡è¯†ç¬¦
              const commentIdentifier = '<!-- security-scan-report:main -->';
              
              // æŸ¥è¯¢æ‰€æœ‰è¯„è®º
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: Number(process.env.PR_NUMBER),
                per_page: 100
              });
              
              let deletedCount = 0;
              
              // åˆ é™¤æ‰€æœ‰æœºå™¨äººè‡ªå·±çš„å®‰å…¨ç›¸å…³è¯„è®º
              for (const comment of comments.data) {
                const isBotComment = comment.user.type === 'Bot' || 
                                   comment.user.login === 'github-actions[bot]' ||
                                   comment.user.login.endsWith('[bot]') ||
                                   comment.body.includes('æ­¤è¯„è®ºç”±ä»£ç å®‰å…¨æ£€æŸ¥å·¥å…·è‡ªåŠ¨ç”Ÿæˆ') ||
                                   comment.body.includes('æ­¤æŠ¥å‘Šç”±ä»£ç å®‰å…¨æ‰«æå·¥å…·è‡ªåŠ¨ç”Ÿæˆ');
                
                if (isBotComment && (comment.body.includes(commentIdentifier) || comment.body.includes('<!-- security-line-comment:'))) {
                  await github.rest.issues.deleteComment({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    comment_id: comment.id
                  });
                  deletedCount++;
                  console.log(`Cleaned up old security comment: ${comment.id}`);
                }
              }
              
              if (deletedCount > 0) {
                console.log(`Cleaned up ${deletedCount} old security comments (no vulnerabilities found)`);
              } else {
                console.log('No old security comments found to clean up');
              }
              
            } catch (error) {
              console.error('Failed to clean up old security comments:', error);
              // ä¸æŠ›å‡ºé”™è¯¯ï¼Œé¿å…å½±å“ä¸»æµç¨‹
            }

      - name: Comment PR with security findings
        if: steps.validate.outputs.should_skip != 'true' && steps.scan.outputs.scan_result == 'failure' && steps.test_token.outputs.has_permissions == 'true' && env.PR_NUMBER != '' && env.PR_NUMBER != '0'
        uses: actions/github-script@v6
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            try {
              // ä»æ–‡ä»¶è¯»å–æŠ¥å‘Šå†…å®¹ï¼Œé¿å… JavaScript å…³é”®å­—å†²çª
              const path = require('path');
              const resultsDir = path.join(process.cwd(), 'results');
              
              let report = '';
              const summaryFile = path.join(resultsDir, 'scan_summary.txt');
              if (fs.existsSync(summaryFile)) {
                report = fs.readFileSync(summaryFile, 'utf8');
              } else {
                report = 'æŠ¥å‘Šæ–‡ä»¶æœªæ‰¾åˆ°';
              }
              
              // å®šä¹‰å®‰å…¨æ‰«æè¯„è®ºçš„å”¯ä¸€æ ‡è¯†ç¬¦
              const commentIdentifier = '<!-- security-scan-report:main -->';
              
              // ä½¿ç”¨æ›´é«˜æ•ˆçš„æŸ¥è¯¢æ–¹å¼ï¼šåªæŸ¥è¯¢æœºå™¨äººè‡ªå·±çš„è¯„è®º
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: Number(process.env.PR_NUMBER),
                per_page: 100
              });
              
              // æŸ¥æ‰¾ç°æœ‰çš„å®‰å…¨æ‰«æä¸»æŠ¥å‘Šè¯„è®ºï¼ˆåªæŸ¥æ‰¾æœºå™¨äººè‡ªå·±çš„è¯„è®ºï¼‰
              let existingMainComment = null;
              for (const comment of comments.data) {
                // åªå¤„ç†æœºå™¨äººè‡ªå·±çš„è¯„è®ºï¼Œé¿å…è¯¯æ“ä½œå…¶ä»–äººçš„è¯„è®º
                if (comment.user.type === 'Bot' && comment.body.includes(commentIdentifier)) {
                  existingMainComment = comment;
                  break;
                }
              }
              
              // åœ¨æŠ¥å‘Šå†…å®¹ä¸­æ·»åŠ æ ‡è¯†ç¬¦
              const reportWithIdentifier = commentIdentifier + '\n\n' + report;
              
              if (existingMainComment) {
                // æ›´æ–°ç°æœ‰ä¸»æŠ¥å‘Šè¯„è®º
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingMainComment.id,
                  body: reportWithIdentifier
                });
                console.log('Security report comment updated successfully');
              } else {
                // åˆ›å»ºæ–°çš„ä¸»æŠ¥å‘Šè¯„è®º
                await github.rest.issues.createComment({
                  issue_number: Number(process.env.PR_NUMBER),
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: reportWithIdentifier
                });
                console.log('Security report comment posted successfully to PR comments');
              }
              
              // ç„¶åå°è¯•åœ¨å…·ä½“çš„ä»£ç è¡Œä¸Šæ·»åŠ è¯„è®º
              // è¯»å– results ç›®å½•ä¸­çš„é£é™©è¯¦æƒ…æ–‡ä»¶
              
              try {
                if (fs.existsSync(resultsDir)) {
                  const files = fs.readdirSync(resultsDir);
                  const detailFiles = files.filter(file => file.startsWith('risk_details_') && file.endsWith('.txt'));
                  
                  for (const detailFile of detailFiles) {
                    const detailPath = path.join(resultsDir, detailFile);
                    const content = fs.readFileSync(detailPath, 'utf8');
                    
                    // è§£ææ–‡ä»¶å†…å®¹
                    const lines = content.split('\n');
                    let filePath = '';
                    let lineNum = '';
                    let severity = '';
                    let title = '';
                    let description = '';
                    let solution = '';
                    
                    // è§£ææ–‡ä»¶å†…å®¹ï¼Œå¤„ç†å¤šè¡Œå­—æ®µ
                    let inDescription = false;
                    let inSolution = false;
                    
                    console.log(`Processing risk detail file: ${detailFile}`);
                    
                    for (let i = 0; i < lines.length; i++) {
                      const line = lines[i];
                      
                      if (line.startsWith('æ–‡ä»¶è·¯å¾„:')) {
                        filePath = line.replace('æ–‡ä»¶è·¯å¾„:', '').trim();
                        console.log(`Found file path: ${filePath}`);
                      } else if (line.startsWith('è¡Œå·:')) {
                        lineNum = line.replace('è¡Œå·:', '').trim();
                        console.log(`Found line number: ${lineNum}`);
                      } else if (line.startsWith('ä¸¥é‡ç¨‹åº¦:')) {
                        severity = line.replace('ä¸¥é‡ç¨‹åº¦:', '').trim();
                        console.log(`Found severity: ${severity}`);
                      } else if (line.startsWith('ä¸­æ–‡æ ‡é¢˜:')) {
                        title = line.replace('ä¸­æ–‡æ ‡é¢˜:', '').trim();
                        console.log(`Found Chinese title: ${title}`);
                      } else if (line.startsWith('æ ‡é¢˜:') && !title) {
                        title = line.replace('æ ‡é¢˜:', '').trim();
                        console.log(`Found title: ${title}`);
                      } else if (line.startsWith('æè¿°:')) {
                        description = line.replace('æè¿°:', '').trim();
                        inDescription = true;
                        inSolution = false;
                        console.log(`Found description: ${description.substring(0, 50)}...`);
                      } else if (line.startsWith('è§£å†³æ–¹æ¡ˆ:')) {
                        solution = line.replace('è§£å†³æ–¹æ¡ˆ:', '').trim();
                        inDescription = false;
                        inSolution = true;
                        console.log(`Found solution: ${solution.substring(0, 50)}...`);
                      } else if (line.startsWith('=== æ‰«æç»Ÿè®¡ ===')) {
                        // é‡åˆ°ç»Ÿè®¡ä¿¡æ¯ï¼Œåœæ­¢è§£æ
                        inDescription = false;
                        inSolution = false;
                        break;
                      } else if (inDescription && line.trim() !== '') {
                        // ç´¯ç§¯æè¿°å†…å®¹
                        description += '\n' + line;
                      } else if (inSolution && line.trim() !== '') {
                        // ç´¯ç§¯è§£å†³æ–¹æ¡ˆå†…å®¹
                        solution += '\n' + line;
                      }
                    }
                    
                    console.log(`Parsed values - filePath: "${filePath}", lineNum: "${lineNum}", severity: "${severity}", title: "${title}"`);
                    
                     // å¦‚æœæ‰¾åˆ°äº†æ–‡ä»¶è·¯å¾„å’Œè¡Œå·ï¼Œä¸”ä¸¥é‡ç¨‹åº¦ä¸º high æˆ– criticalï¼Œåˆ›å»ºå¸¦ä»£ç å¼•ç”¨çš„è¯„è®º
                     console.log(`Checking conditions - filePath: ${!!filePath}, lineNum: ${!!lineNum}, severity: ${!!severity}, title: ${!!title}, severity value: "${severity}"`);
                     
                     if (filePath && lineNum && severity && title && (severity === 'high' || severity === 'critical')) {
                       console.log(`âœ… All conditions met, creating line-specific comment for ${filePath}:${lineNum}`);
                       try {
                         // æ„å»ºå¸¦ä»£ç å¼•ç”¨çš„è¯„è®ºå†…å®¹å’Œç²¾å‡†é“¾æ¥
                         const codeReference = `\`${filePath}:${lineNum}\``;
                         const lineCommentIdentifier = `<!-- security-line-comment:${filePath}:${lineNum} -->`;
                         
                         // ç”Ÿæˆç²¾å‡†çš„ GitHub ä»£ç é“¾æ¥
                         const lineLink = `https://github.com/${context.repo.owner}/${context.repo.repo}/blob/${context.sha}/${filePath}#L${lineNum}`;
                         
                         const lineComment = `${lineCommentIdentifier}\n\n## âš ï¸ å®‰å…¨é£é™©æ£€æµ‹\n\n**é—®é¢˜:** ${title}\n**ä¸¥é‡ç¨‹åº¦:** ${severity}\n**æ–‡ä»¶ä½ç½®:** ${codeReference}\n**ä»£ç é“¾æ¥:** [æŸ¥çœ‹ä»£ç ](${lineLink})\n\n**æè¿°:**\n${description}\n\n**å»ºè®®è§£å†³æ–¹æ¡ˆ:**\n${solution}\n\n---\n*æ­¤è¯„è®ºç”±ä»£ç å®‰å…¨æ£€æŸ¥å·¥å…·è‡ªåŠ¨ç”Ÿæˆ*`;
                         
                         // é«˜æ•ˆæŸ¥æ‰¾ç°æœ‰çš„è¡Œçº§è¯„è®ºï¼ˆåªæŸ¥æ‰¾æœºå™¨äººè‡ªå·±çš„è¯„è®ºï¼‰
                         let existingLineComment = null;
                         for (const comment of comments.data) {
                           if (comment.user.type === 'Bot' && comment.body.includes(lineCommentIdentifier)) {
                             existingLineComment = comment;
                             break;
                           }
                         }
                         
                         if (existingLineComment) {
                           // æ›´æ–°ç°æœ‰çš„è¡Œçº§è¯„è®º
                           await github.rest.issues.updateComment({
                             owner: context.repo.owner,
                             repo: context.repo.repo,
                             comment_id: existingLineComment.id,
                             body: lineComment
                           });
                           console.log(`Line-specific comment updated for ${filePath}:${lineNum} (severity: ${severity})`);
                         } else {
                           // åˆ›å»ºæ–°çš„è¡Œçº§è¯„è®º
                           await github.rest.issues.createComment({
                             issue_number: Number(process.env.PR_NUMBER),
                             owner: context.repo.owner,
                             repo: context.repo.repo,
                             body: lineComment
                           });
                           console.log(`Line-specific comment posted for ${filePath}:${lineNum} (severity: ${severity})`);
                         }
                       } catch (lineError) {
                         console.warn(`Failed to post line-specific comment for ${filePath}:${lineNum}:`, lineError.message);
                         // ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                       }
                     } else if (severity && severity !== 'high' && severity !== 'critical') {
                       console.log(`Skipping line-specific comment for ${filePath}:${lineNum} (severity: ${severity} - not high/critical)`);
                     } else {
                       console.log(`âŒ Conditions not met for line-specific comment - filePath: "${filePath}", lineNum: "${lineNum}", severity: "${severity}", title: "${title}"`);
                     }
                  }
                }
              } catch (resultsError) {
                console.warn('Failed to process results directory for line comments:', resultsError.message);
                // ä¸ä¸­æ–­ä¸»æµç¨‹
              }
              
              // é«˜æ•ˆæ¸…ç†ä¸å†ç›¸å…³çš„æ—§è¯„è®º
              try {
                // æ”¶é›†å½“å‰æ´»è·ƒçš„é£é™©æ–‡ä»¶è·¯å¾„å’Œè¡Œå·
                const activeRisks = new Set();
                if (fs.existsSync(resultsDir)) {
                  const files = fs.readdirSync(resultsDir);
                  const detailFiles = files.filter(file => file.startsWith('risk_details_') && file.endsWith('.txt'));
                  
                  for (const detailFile of detailFiles) {
                    const detailPath = path.join(resultsDir, detailFile);
                    const content = fs.readFileSync(detailPath, 'utf8');
                    
                    const lines = content.split('\n');
                    let filePath = '';
                    let lineNum = '';
                    let severity = '';
                    
                    for (const line of lines) {
                      if (line.startsWith('æ–‡ä»¶è·¯å¾„:')) {
                        filePath = line.replace('æ–‡ä»¶è·¯å¾„:', '').trim();
                      } else if (line.startsWith('è¡Œå·:')) {
                        lineNum = line.replace('è¡Œå·:', '').trim();
                      } else if (line.startsWith('ä¸¥é‡ç¨‹åº¦:')) {
                        severity = line.replace('ä¸¥é‡ç¨‹åº¦:', '').trim();
                      } else if (line.startsWith('=== æ‰«æç»Ÿè®¡ ===')) {
                        // é‡åˆ°ç»Ÿè®¡ä¿¡æ¯ï¼Œåœæ­¢è§£æ
                        break;
                      }
                    }
                    
                    // åªä¿ç•™ high å’Œ critical çš„é£é™©
                    if (filePath && lineNum && severity && (severity === 'high' || severity === 'critical')) {
                      activeRisks.add(`${filePath}:${lineNum}`);
                    }
                  }
                }
                
                // é«˜æ•ˆæ¸…ç†ï¼šåªå¤„ç†æœºå™¨äººè‡ªå·±çš„è¡Œçº§è¯„è®º
                let deletedCount = 0;
                for (const comment of comments.data) {
                  if (comment.user.type === 'Bot' && comment.body.includes('<!-- security-line-comment:')) {
                    // æå–è¯„è®ºä¸­çš„æ–‡ä»¶è·¯å¾„å’Œè¡Œå·
                    const match = comment.body.match(/<!-- security-line-comment:([^:]+:\d+) -->/);
                    if (match) {
                      const commentKey = match[1];
                      if (!activeRisks.has(commentKey)) {
                        // è¿™ä¸ªè¯„è®ºå¯¹åº”çš„é£é™©å·²ç»ä¸å­˜åœ¨ï¼Œåˆ é™¤è¯¥è¯„è®º
                        await github.rest.issues.deleteComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
                          comment_id: comment.id
                        });
                        deletedCount++;
                        console.log(`Cleaned up outdated comment for ${commentKey}`);
                      }
                    }
                  }
                }
                
                if (deletedCount > 0) {
                  console.log(`Cleaned up ${deletedCount} outdated security comments`);
                }
              } catch (cleanupError) {
                console.warn('Failed to cleanup outdated comments:', cleanupError.message);
                // ä¸ä¸­æ–­ä¸»æµç¨‹
              }
              
              console.log('Security comments posted successfully, but workflow will fail to trigger failure handling steps');
              // æ•…æ„é€€å‡ºï¼Œè®©å·¥ä½œæµå¤±è´¥ï¼Œä»¥ä¾¿æ‰§è¡Œåç»­çš„å¤±è´¥å¤„ç†æ­¥éª¤
              console.log('Intentionally failing workflow to trigger failure handling steps');
              process.exit(1);
              
            } catch (error) {
              console.error('Failed to post security comments:', error);
              throw error;
            }

      - name: Upload risk log
        if: steps.validate.outputs.should_skip != 'true' && failure()
        uses: actions/upload-artifact@v4
        with:
          name: risk.json
          path: risk.json

      - name: Workflows fail info
        if: steps.validate.outputs.should_skip != 'true' && failure()
        run: |
          ./yak ssa-risk --input ./risk.json --with-code --severity high

      - name: Workflows fail info - full
        if: steps.validate.outputs.should_skip != 'true' && failure()
        run: |
          ./yak ssa-risk --input ./risk.json --with-code 
      
      - name: Cleanup workspace after workflow
        if: always()
        run: |
          if [ -d "vendor" ]; then
            sudo chmod -R +w vendor/ 2>/dev/null || chmod -R +w vendor/ || true
            rm -rf vendor/ || true
          fi 
        # cat risk.json | jq -r '
        #   "=== Scan Report Summary ===",
        #   "Scan Time: \(.report_time)",
        #   "Program: \(.program_name)",
        #   "Language: \(.program_lang)",
        #   "Files Scanned: \(.file_count)",
        #   "Lines of Code: \(.code_line_count)",
        #   "Risks Found: \(.RiskNums)",
        #   "",
        #   "=== Risk Details ===",
        #   (.Risks[] | 
        #     "Risk ID: \(.hash)",
        #     "Title: \(.title_verbose)",
        #     "Severity: \(.severity)",
        #     "Location: \(.code_source_url):\(.line)",
        #     "Description: \(.description | split("\n")[0])",
        #     "Solution: \(.solution | split("\n")[0])",
        #     "Affected Code:",
        #     (.code_fragment | split("\n")[] | "  \(.)"),
        #     ""
        #   ),
        #   "=== Affected Files ===",
        #   (.File[] | 
        #     "File: \(.path)",
        #     "Lines: \(.line_count)",
        #     "Risk IDs: \(.risks | join(", "))",
        #     ""
        #   )'
        # exit 1